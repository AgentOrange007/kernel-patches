From 6c54ff2329e85f4174f6cfff4b57532e03576989 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 8 Oct 2019 04:15:23 +0800
Subject: [PATCH 1/6] bmq: Remove sched_cpu_llc_start_mask.

---
 kernel/sched/bmq.c | 16 +++++-----------
 1 file changed, 5 insertions(+), 11 deletions(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 034a5fa1859d..293934062664 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -124,7 +124,6 @@ enum {
 };
 
 DEFINE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_CHK_LEVEL], sched_cpu_affinity_chk_masks);
-DEFINE_PER_CPU(cpumask_t *, sched_cpu_llc_start_mask);
 DEFINE_PER_CPU(cpumask_t *, sched_cpu_affinity_chk_end_masks);
 
 #ifdef CONFIG_SCHED_SMT
@@ -2809,7 +2808,7 @@ static inline int take_other_rq_tasks(struct rq *rq, int cpu)
 	if (cpumask_empty(&sched_rq_pending_mask))
 		return 0;
 
-	affinity_mask = per_cpu(sched_cpu_llc_start_mask, cpu);
+	affinity_mask = &(per_cpu(sched_cpu_affinity_chk_masks, cpu)[0]);
 	end_mask = per_cpu(sched_cpu_affinity_chk_end_masks, cpu);
 	do {
 		int i;
@@ -5525,8 +5524,6 @@ static void sched_init_topology_cpumask_early(void)
 			cpumask_copy(tmp, cpu_possible_mask);
 			cpumask_clear_cpu(cpu, tmp);
 		}
-		per_cpu(sched_cpu_llc_start_mask, cpu) =
-			&(per_cpu(sched_cpu_affinity_chk_masks, cpu)[0]);
 		per_cpu(sched_cpu_affinity_chk_end_masks, cpu) =
 			&(per_cpu(sched_cpu_affinity_chk_masks, cpu)[1]);
 	}
@@ -5547,15 +5544,14 @@ static void sched_init_topology_cpumask(void)
 			printk(KERN_INFO "bmq: cpu #%d affinity check mask - smt 0x%08lx",
 			       cpu, (chk++)->bits[0]);
 		}
+		cpumask_complement(chk, topology_sibling_cpumask(cpu));
+#else
+		cpumask_clear_cpu(cpu, chk);
 #endif
 #ifdef CONFIG_SCHED_MC
-		cpumask_setall(chk);
-		cpumask_clear_cpu(cpu, chk);
-		if (cpumask_and(chk, chk, cpu_coregroup_mask(cpu))) {
-			per_cpu(sched_cpu_llc_start_mask, cpu) = chk;
+		if (cpumask_and(chk, chk, cpu_coregroup_mask(cpu)))
 			printk(KERN_INFO "bmq: cpu #%d affinity check mask - coregroup 0x%08lx",
 			       cpu, (chk++)->bits[0]);
-		}
 		cpumask_complement(chk, cpu_coregroup_mask(cpu));
 
 		/**
@@ -5567,8 +5563,6 @@ static void sched_init_topology_cpumask(void)
 		per_cpu(sd_llc_id, cpu) =
 			cpumask_first(topology_core_cpumask(cpu));
 
-		per_cpu(sched_cpu_llc_start_mask, cpu) = chk;
-
 		cpumask_setall(chk);
 		cpumask_clear_cpu(cpu, chk);
 #endif /* NOT CONFIG_SCHED_MC */
-- 
2.24.0.rc0


From 4451c57e9d6b3f1aae0b6d0785f36eafea8938be Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 8 Oct 2019 22:24:07 +0800
Subject: [PATCH 2/6] bmq: Rename as sched_cpu_affinity_masks and
 sched_cpu_affinity_end_mask.

---
 kernel/sched/bmq.c | 24 ++++++++++++------------
 1 file changed, 12 insertions(+), 12 deletions(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 293934062664..51b16c1d21fd 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -123,8 +123,8 @@ enum {
 	NR_CPU_AFFINITY_CHK_LEVEL
 };
 
-DEFINE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_CHK_LEVEL], sched_cpu_affinity_chk_masks);
-DEFINE_PER_CPU(cpumask_t *, sched_cpu_affinity_chk_end_masks);
+DEFINE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_CHK_LEVEL], sched_cpu_affinity_masks);
+DEFINE_PER_CPU(cpumask_t *, sched_cpu_affinity_end_mask);
 
 #ifdef CONFIG_SCHED_SMT
 DEFINE_STATIC_KEY_FALSE(sched_smt_present);
@@ -1328,7 +1328,7 @@ static int select_fallback_rq(int cpu, struct task_struct *p)
 
 static inline int __best_mask_cpu(int cpu, cpumask_t *cpumask)
 {
-	cpumask_t *mask = &(per_cpu(sched_cpu_affinity_chk_masks, cpu)[0]);
+	cpumask_t *mask = &(per_cpu(sched_cpu_affinity_masks, cpu)[0]);
 	while ((cpu = cpumask_any_and(cpumask, mask)) >= nr_cpu_ids)
 		mask++;
 	return cpu;
@@ -2808,8 +2808,8 @@ static inline int take_other_rq_tasks(struct rq *rq, int cpu)
 	if (cpumask_empty(&sched_rq_pending_mask))
 		return 0;
 
-	affinity_mask = &(per_cpu(sched_cpu_affinity_chk_masks, cpu)[0]);
-	end_mask = per_cpu(sched_cpu_affinity_chk_end_masks, cpu);
+	affinity_mask = &(per_cpu(sched_cpu_affinity_masks, cpu)[0]);
+	end_mask = per_cpu(sched_cpu_affinity_end_mask, cpu);
 	do {
 		int i;
 		for_each_cpu_and(i, &sched_rq_pending_mask, affinity_mask) {
@@ -5082,8 +5082,8 @@ int get_nohz_timer_target(void)
 	if (!idle_cpu(cpu) && housekeeping_cpu(cpu, HK_FLAG_TIMER))
 		return cpu;
 
-	for (mask = &(per_cpu(sched_cpu_affinity_chk_masks, cpu)[0]);
-	     mask < per_cpu(sched_cpu_affinity_chk_end_masks, cpu); mask++)
+	for (mask = &(per_cpu(sched_cpu_affinity_masks, cpu)[0]);
+	     mask < per_cpu(sched_cpu_affinity_end_mask, cpu); mask++)
 		for_each_cpu(i, mask)
 			if (!idle_cpu(i) && housekeeping_cpu(i, HK_FLAG_TIMER))
 				return i;
@@ -5520,12 +5520,12 @@ static void sched_init_topology_cpumask_early(void)
 
 	for_each_possible_cpu(cpu) {
 		for (level = 0; level < NR_CPU_AFFINITY_CHK_LEVEL; level++) {
-			tmp = &(per_cpu(sched_cpu_affinity_chk_masks, cpu)[level]);
+			tmp = &(per_cpu(sched_cpu_affinity_masks, cpu)[level]);
 			cpumask_copy(tmp, cpu_possible_mask);
 			cpumask_clear_cpu(cpu, tmp);
 		}
-		per_cpu(sched_cpu_affinity_chk_end_masks, cpu) =
-			&(per_cpu(sched_cpu_affinity_chk_masks, cpu)[1]);
+		per_cpu(sched_cpu_affinity_end_mask, cpu) =
+			&(per_cpu(sched_cpu_affinity_masks, cpu)[1]);
 	}
 }
 
@@ -5535,7 +5535,7 @@ static void sched_init_topology_cpumask(void)
 	cpumask_t *chk;
 
 	for_each_online_cpu(cpu) {
-		chk = &(per_cpu(sched_cpu_affinity_chk_masks, cpu)[0]);
+		chk = &(per_cpu(sched_cpu_affinity_masks, cpu)[0]);
 
 #ifdef CONFIG_SCHED_SMT
 		cpumask_setall(chk);
@@ -5575,7 +5575,7 @@ static void sched_init_topology_cpumask(void)
 			printk(KERN_INFO "bmq: cpu #%d affinity check mask - others 0x%08lx",
 			       cpu, (chk++)->bits[0]);
 
-		per_cpu(sched_cpu_affinity_chk_end_masks, cpu) = chk;
+		per_cpu(sched_cpu_affinity_end_mask, cpu) = chk;
 	}
 }
 #endif
-- 
2.24.0.rc0


From 817dadebe109716bbe3e897f34dea4b0caa98d8b Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 8 Oct 2019 22:50:57 +0800
Subject: [PATCH 3/6] bmq: Refine bmq_find_first_bit/bmq_find_next_bit macros.

---
 kernel/sched/bmq.c | 14 +++++++-------
 1 file changed, 7 insertions(+), 7 deletions(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 51b16c1d21fd..f7a550c6b9d9 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -161,16 +161,16 @@ static cpumask_t sched_sg_idle_mask ____cacheline_aligned_in_smp;
 static cpumask_t sched_rq_watermark[bmq_BITS] ____cacheline_aligned_in_smp;
 
 #if (bmq_BITS <= BITS_PER_LONG)
-#define bmq_find_first_bit(bm, size)		__ffs((bm[0]))
-#define bmq_find_next_bit(bm, size, start)	__ffs(BITMAP_FIRST_WORD_MASK(start) & bm[0])
+#define bmq_find_first_bit(bm)		__ffs((bm[0]))
+#define bmq_find_next_bit(bm, start)	__ffs(BITMAP_FIRST_WORD_MASK(start) & bm[0])
 #else
-#define bmq_find_first_bit(bm, size)		find_first_bit((bm), (size))
-#define bmq_find_next_bit(bm, size, start)	find_next_bit(bm, size, start)
+#define bmq_find_first_bit(bm)		find_first_bit((bm), bmq_BITS)
+#define bmq_find_next_bit(bm, start)	find_next_bit(bm, bmq_BITS, start)
 #endif
 
 static inline void update_sched_rq_watermark(struct rq *rq)
 {
-	unsigned long watermark = bmq_find_first_bit(rq->queue.bitmap, bmq_BITS);
+	unsigned long watermark = bmq_find_first_bit(rq->queue.bitmap);
 	unsigned long last_wm = rq->watermark;
 	unsigned long wm;
 	int cpu;
@@ -250,7 +250,7 @@ static inline void bmq_add_task(struct task_struct *p, struct bmq *q, int idx)
  */
 static inline struct task_struct *rq_first_bmq_task(struct rq *rq)
 {
-	unsigned long idx = bmq_find_first_bit(rq->queue.bitmap, bmq_BITS);
+	unsigned long idx = bmq_find_first_bit(rq->queue.bitmap);
 	const struct list_head *head = &rq->queue.heads[idx];
 
 	return list_first_entry(head, struct task_struct, bmq_node);
@@ -263,7 +263,7 @@ rq_next_bmq_task(struct task_struct *p, struct rq *rq)
 	struct list_head *head = &rq->queue.heads[idx];
 
 	if (list_is_last(&p->bmq_node, head)) {
-		idx = bmq_find_next_bit(rq->queue.bitmap, bmq_BITS, idx + 1);
+		idx = bmq_find_next_bit(rq->queue.bitmap, idx + 1);
 		head = &rq->queue.heads[idx];
 
 		return list_first_entry(head, struct task_struct, bmq_node);
-- 
2.24.0.rc0


From a635c2b08c923ed48ef6089a29295c68069e0b53 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sat, 12 Oct 2019 12:53:44 +0800
Subject: [PATCH 4/6] bmq: Rename wm in update_sched_rq_watermark().

---
 kernel/sched/bmq.c | 12 ++++++------
 1 file changed, 6 insertions(+), 6 deletions(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index f7a550c6b9d9..05149693d1ce 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -172,7 +172,7 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 {
 	unsigned long watermark = bmq_find_first_bit(rq->queue.bitmap);
 	unsigned long last_wm = rq->watermark;
-	unsigned long wm;
+	unsigned long i;
 	int cpu;
 
 	if (watermark == last_wm)
@@ -181,9 +181,9 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 	rq->watermark = watermark;
 	cpu = cpu_of(rq);
 	if (watermark < last_wm) {
-		for (wm = watermark + 1; wm <= last_wm; wm++)
-			cpumask_andnot(&sched_rq_watermark[wm],
-				       &sched_rq_watermark[wm], cpumask_of(cpu));
+		for (i = watermark + 1; i <= last_wm; i++)
+			cpumask_andnot(&sched_rq_watermark[i],
+				       &sched_rq_watermark[i], cpumask_of(cpu));
 #ifdef CONFIG_SCHED_SMT
 		if (!static_branch_likely(&sched_smt_present))
 			return;
@@ -194,8 +194,8 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 		return;
 	}
 	/* last_wm < watermark */
-	for (wm = last_wm + 1; wm <= watermark; wm++)
-		cpumask_set_cpu(cpu, &sched_rq_watermark[wm]);
+	for (i = last_wm + 1; i <= watermark; i++)
+		cpumask_set_cpu(cpu, &sched_rq_watermark[i]);
 #ifdef CONFIG_SCHED_SMT
 	if (!static_branch_likely(&sched_smt_present))
 		return;
-- 
2.24.0.rc0


From 68ca427efa5855fd7b03da81fdfd4d8e07951cc0 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sun, 13 Oct 2019 15:31:00 +0800
Subject: [PATCH 5/6] bmq: Optimize migrate_pending_tasks().

---
 kernel/sched/bmq.c | 17 +++++------------
 1 file changed, 5 insertions(+), 12 deletions(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 05149693d1ce..a61d5263c144 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -2775,16 +2775,13 @@ static inline void check_curr(struct task_struct *p, struct rq *rq)
 static inline int
 migrate_pending_tasks(struct rq *rq, struct rq *dest_rq, const int dest_cpu)
 {
-	struct task_struct *p, *next;
+	struct task_struct *p, *skip = rq->curr;
 	int nr_migrated = 0;
-	int nr_tries = min((rq->nr_running + 1) / 2, SCHED_RQ_NR_MIGRATION);
+	int nr_tries = min(rq->nr_running / 2, SCHED_RQ_NR_MIGRATION);
 
-	for (p = rq_first_bmq_task(rq);
-	     nr_tries && p != rq->idle;
-	     p = rq_next_bmq_task(p, rq)) {
-		if (task_running(p))
-			continue;
-		next = rq_next_bmq_task(p, rq);
+	while (skip != rq->idle && nr_tries &&
+	       (p = rq_next_bmq_task(skip, rq)) != rq->idle) {
+		skip = rq_next_bmq_task(p, rq);
 		if (cpumask_test_cpu(dest_cpu, p->cpus_ptr)) {
 			dequeue_task(p, rq, 0);
 			set_task_cpu(p, dest_cpu);
@@ -2792,10 +2789,6 @@ migrate_pending_tasks(struct rq *rq, struct rq *dest_rq, const int dest_cpu)
 			nr_migrated++;
 		}
 		nr_tries--;
-		/* make a jump */
-		if (next == rq->idle)
-			break;
-		p = next;
 	}
 
 	return nr_migrated;
-- 
2.24.0.rc0


From 47ffd0f5cc54f202e124f948209dbf2fcaa2a6ec Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 22 Oct 2019 15:35:50 +0800
Subject: [PATCH 6/6] Tag BMQ v5.3-r2

---
 kernel/sched/bmq.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index a61d5263c144..4302cfe1048c 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -68,7 +68,7 @@
 
 static inline void print_scheduler_version(void)
 {
-	printk(KERN_INFO "bmq: BMQ CPU Scheduler 5.3.1 by Alfred Chen.\n");
+	printk(KERN_INFO "bmq: BMQ CPU Scheduler 5.3-r2 by Alfred Chen.\n");
 }
 
 /**
-- 
2.24.0.rc0

