From e0c161a7420c86436698b46f7b8b0fecec3f6354 Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Mon, 16 Sep 2019 14:24:11 +0200
Subject: [PATCH] LL: Implement ll-branding v5.3

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 block/elevator.c       |  6 ++++++
 include/linux/blkdev.h |  4 ++++
 init/Kconfig           |  4 ++++
 kernel/sched/bmq.c     |  4 ++++
 kernel/sched/core.c    |  8 ++++++++
 kernel/sched/fair.c    | 25 +++++++++++++++++++++++++
 mm/huge_memory.c       |  4 ++++
 mm/page-writeback.c    |  8 ++++++++
 8 files changed, 63 insertions(+)

diff --git a/block/elevator.c b/block/elevator.c
index 2f17d66d0..225ebcea4 100644
--- a/block/elevator.c
+++ b/block/elevator.c
@@ -604,8 +604,10 @@ int elevator_init_mq(struct request_queue *q)
 	struct elevator_type *e;
 	int err = 0;
 
+#ifndef CONFIG_LL_BRANDING
 	if (q->nr_hw_queues != 1)
 		return 0;
+#endif
 
 	/*
 	 * q->sysfs_lock must be held to provide mutual exclusion between
@@ -615,7 +617,11 @@ int elevator_init_mq(struct request_queue *q)
 	if (unlikely(q->elevator))
 		goto out_unlock;
 
+#if defined(CONFIG_LL_BRANDING) && defined(CONFIG_IOSCHED_BFQ)
+	e = elevator_get(q, "bfq", false);
+#else
 	e = elevator_get(q, "mq-deadline", false);
+#endif
 	if (!e)
 		goto out_unlock;
 
diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h
index 1ef375daf..720563f96 100644
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -45,7 +45,11 @@ struct blk_queue_stats;
 struct blk_stat_callback;
 
 #define BLKDEV_MIN_RQ	4
+#ifdef CONFIG_LL_BRANDING
+#define BLKDEV_MAX_RQ	512
+#else
 #define BLKDEV_MAX_RQ	128	/* Default maximum */
+#endif
 
 /* Must be consistent with blk_mq_poll_stats_bkt() */
 #define BLK_MQ_POLL_STATS_BKTS 16
diff --git a/init/Kconfig b/init/Kconfig
index e182f53bb..ee2bc700d 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -68,6 +68,10 @@ config THREAD_INFO_IN_TASK
 
 menu "General setup"
 
+config LL_BRANDING
+	bool "Add Linux Lucjan branding"
+	default y
+
 config SCHED_BMQ
 	bool "BMQ CPU scheduler"
 	help
diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 10c91a6e9..7c0fb8f2f 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -77,7 +77,11 @@ static inline void print_scheduler_version(void)
  * 1: Deboost and requeue task. (default)
  * 2: Set rq skip task.
  */
+#ifdef CONFIG_LL_BRANDING
+int sched_yield_type __read_mostly = 2;
+#else
 int sched_yield_type __read_mostly = 1;
+#endif
 
 #define rq_switch_time(rq)	((rq)->clock - (rq)->last_ts_switch)
 #define boost_threshold(p)	(SCHED_TIMESLICE_NS >>\
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index df9f1fe56..d109a84a4 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -56,7 +56,11 @@ const_debug unsigned int sysctl_sched_features =
  * Number of tasks to iterate in a single balance run.
  * Limited because this is done with IRQs disabled.
  */
+#ifdef CONFIG_LL_BRANDING
+const_debug unsigned int sysctl_sched_nr_migrate = 128;
+#else
 const_debug unsigned int sysctl_sched_nr_migrate = 32;
+#endif
 
 /*
  * period over which we measure -rt task CPU usage in us.
@@ -70,7 +74,11 @@ __read_mostly int scheduler_running;
  * part of the period that we allow rt tasks to run in us.
  * default: 0.95s
  */
+#ifdef CONFIG_LL_BRANDING
+int sysctl_sched_rt_runtime = 980000;
+#else
 int sysctl_sched_rt_runtime = 950000;
+#endif
 
 /*
  * __task_rq_lock - lock the rq @p resides on.
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 500f5db0d..5e7c46931 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -37,8 +37,13 @@
  *
  * (default: 6ms * (1 + ilog(ncpus)), units: nanoseconds)
  */
+#ifdef CONFIG_LL_BRANDING
+unsigned int sysctl_sched_latency			= 4000000ULL;
+static unsigned int normalized_sysctl_sched_latency	= 4000000ULL;
+#else
 unsigned int sysctl_sched_latency			= 6000000ULL;
 static unsigned int normalized_sysctl_sched_latency	= 6000000ULL;
+#endif
 
 /*
  * The initial- and re-scaling of tunables is configurable
@@ -58,13 +63,22 @@ enum sched_tunable_scaling sysctl_sched_tunable_scaling = SCHED_TUNABLESCALING_L
  *
  * (default: 0.75 msec * (1 + ilog(ncpus)), units: nanoseconds)
  */
+#ifdef CONFIG_LL_BRANDING
+unsigned int sysctl_sched_min_granularity			= 400000ULL;
+static unsigned int normalized_sysctl_sched_min_granularity	= 400000ULL;
+#else
 unsigned int sysctl_sched_min_granularity			= 750000ULL;
 static unsigned int normalized_sysctl_sched_min_granularity	= 750000ULL;
+#endif
 
 /*
  * This value is kept at sysctl_sched_latency/sysctl_sched_min_granularity
  */
+#ifdef CONFIG_LL_BRANDING
+static unsigned int sched_nr_latency = 10;
+#else
 static unsigned int sched_nr_latency = 8;
+#endif
 
 /*
  * After fork, child runs first. If set to 0 (default) then
@@ -81,10 +95,17 @@ unsigned int sysctl_sched_child_runs_first __read_mostly;
  *
  * (default: 1 msec * (1 + ilog(ncpus)), units: nanoseconds)
  */
+#ifdef CONFIG_LL_BRANDING
+unsigned int sysctl_sched_wakeup_granularity			= 500000UL;
+static unsigned int normalized_sysctl_sched_wakeup_granularity	= 500000UL;
+
+const_debug unsigned int sysctl_sched_migration_cost	= 250000UL;
+#else
 unsigned int sysctl_sched_wakeup_granularity			= 1000000UL;
 static unsigned int normalized_sysctl_sched_wakeup_granularity	= 1000000UL;
 
 const_debug unsigned int sysctl_sched_migration_cost	= 500000UL;
+#endif
 
 #ifdef CONFIG_SMP
 /*
@@ -115,8 +136,12 @@ static unsigned int capacity_margin			= 1280;
  *
  * (default: 5 msec, units: microseconds)
  */
+#ifdef CONFIG_LL_BRANDING
+unsigned int sysctl_sched_cfs_bandwidth_slice		= 3000UL;
+#else
 unsigned int sysctl_sched_cfs_bandwidth_slice		= 5000UL;
 #endif
+#endif
 
 static inline void update_load_add(struct load_weight *lw, unsigned long inc)
 {
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index de1f15969..7024241fc 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -53,7 +53,11 @@ unsigned long transparent_hugepage_flags __read_mostly =
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE_MADVISE
 	(1<<TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG)|
 #endif
+#ifdef CONFIG_LL_BRANDING
+	(1<<TRANSPARENT_HUGEPAGE_DEFRAG_KSWAPD_OR_MADV_FLAG)|
+#else
 	(1<<TRANSPARENT_HUGEPAGE_DEFRAG_REQ_MADV_FLAG)|
+#endif
 	(1<<TRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG)|
 	(1<<TRANSPARENT_HUGEPAGE_USE_ZERO_PAGE_FLAG);
 
diff --git a/mm/page-writeback.c b/mm/page-writeback.c
index 1804f64ff..c9a6b006c 100644
--- a/mm/page-writeback.c
+++ b/mm/page-writeback.c
@@ -71,7 +71,11 @@ static long ratelimit_pages = 32;
 /*
  * Start background writeback (via writeback threads) at this percentage
  */
+#ifdef CONFIG_LL_BRANDING
+int dirty_background_ratio = 20;
+#else
 int dirty_background_ratio = 10;
+#endif
 
 /*
  * dirty_background_bytes starts at 0 (disabled) so that it is a function of
@@ -88,7 +92,11 @@ int vm_highmem_is_dirtyable;
 /*
  * The generator of dirty data starts writeback at this percentage
  */
+#ifdef CONFIG_LL_BRANDING
+int vm_dirty_ratio = 50;
+#else
 int vm_dirty_ratio = 20;
+#endif
 
 /*
  * vm_dirty_bytes starts at 0 (disabled) so that it is a function of
-- 
2.23.0.162.gf1d4a28250

