From 4ee3999e9c28446b2a6870fb12162e667f39039c Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Mon, 11 Jun 2018 18:51:01 +0200
Subject: [PATCH] pds-4.17: merge PDS v0.98r

---
 kernel/sched/cpufreq_schedutil.c |   1 -
 kernel/sched/pds.c               | 101 ++++++++++++-------------------
 2 files changed, 38 insertions(+), 64 deletions(-)

diff --git a/kernel/sched/cpufreq_schedutil.c b/kernel/sched/cpufreq_schedutil.c
index 23f58aa30b41..2f1e2b8d7eb1 100644
--- a/kernel/sched/cpufreq_schedutil.c
+++ b/kernel/sched/cpufreq_schedutil.c
@@ -177,7 +177,6 @@ static void sugov_get_util(struct sugov_cpu *sg_cpu)
 	sg_cpu->max = arch_scale_cpu_capacity(NULL, sg_cpu->cpu);
 #else
 	struct rq *rq = cpu_rq(sg_cpu->cpu);
-	struct rq *rq = cpu_rq(cpu);
 
 	sg_cpu->max = arch_scale_cpu_capacity(NULL, sg_cpu->cpu);
 	sg_cpu->util_cfs = cpu_util_cfs(rq);
diff --git a/kernel/sched/pds.c b/kernel/sched/pds.c
index ec911c3f1bc7..af5c2d99c079 100644
--- a/kernel/sched/pds.c
+++ b/kernel/sched/pds.c
@@ -94,7 +94,7 @@ enum {
 
 static inline void print_scheduler_version(void)
 {
-	printk(KERN_INFO "pds: PDS-mq CPU Scheduler 0.98q by Alfred Chen.\n");
+	printk(KERN_INFO "pds: PDS-mq CPU Scheduler 0.98r by Alfred Chen.\n");
 }
 
 /* task_struct::on_rq states: */
@@ -1069,6 +1069,26 @@ void set_task_cpu(struct task_struct *p, unsigned int new_cpu)
 	__set_task_cpu(p, new_cpu);
 }
 
+static inline bool is_per_cpu_kthread(struct task_struct *p)
+{
+	return ((p->flags & PF_KTHREAD) && (1 == p->nr_cpus_allowed));
+}
+
+/*
+ * Per-CPU kthreads are allowed to run on !actie && online CPUs, see
+ * __set_cpus_allowed_ptr() and select_fallback_rq().
+ */
+static inline bool is_cpu_allowed(struct task_struct *p, int cpu)
+{
+	if (!cpumask_test_cpu(cpu, &p->cpus_allowed))
+		return false;
+
+	if (is_per_cpu_kthread(p))
+		return cpu_online(cpu);
+
+	return cpu_active(cpu);
+}
+
 /*
  * This is how migration works:
  *
@@ -1154,16 +1174,8 @@ struct migration_arg {
 static struct rq *__migrate_task(struct rq *rq, struct task_struct *p, int
 				 dest_cpu)
 {
-	if (p->flags & PF_KTHREAD) {
-		if (unlikely(!cpu_online(dest_cpu)))
-			return rq;
-	} else {
-		if (unlikely(!cpu_active(dest_cpu)))
-			return rq;
-	}
-
 	/* Affinity changed (again). */
-	if (unlikely(!cpumask_test_cpu(dest_cpu, &p->cpus_allowed)))
+	if (!is_cpu_allowed(p, dest_cpu))
 		return rq;
 
 	update_rq_clock(rq);
@@ -1413,9 +1425,7 @@ static int select_fallback_rq(int cpu, struct task_struct *p)
 	for (;;) {
 		/* Any allowed, online CPU? */
 		for_each_cpu(dest_cpu, &p->cpus_allowed) {
-			if (!(p->flags & PF_KTHREAD) && !cpu_active(dest_cpu))
-				continue;
-			if (!cpu_online(dest_cpu))
+			if (!is_cpu_allowed(p, dest_cpu))
 				continue;
 			goto out;
 		}
@@ -1493,13 +1503,10 @@ task_preemptible_rq(struct task_struct *p, cpumask_t *chk_mask,
 
 #ifdef CONFIG_SCHED_SMT
 	if (SCHED_RQ_EMPTY == level) {
-		if(cpumask_and(&tmp, chk_mask, &sched_rq_queued_masks[level])) {
-			cpumask_t smt_tmp;
-
-			if (cpumask_and(&smt_tmp, &tmp, &sched_cpu_sg_idle_mask))
-				return best_mask_cpu(task_cpu(p), &smt_tmp);
+		if (cpumask_and(&tmp, chk_mask, &sched_cpu_sg_idle_mask) ||
+		    cpumask_and(&tmp, chk_mask, &sched_rq_queued_masks[level]))
 			return best_mask_cpu(task_cpu(p), &tmp);
-		}
+
 		level = find_next_bit(sched_rq_queued_masks_bitmap,
 				      NR_SCHED_RQ_QUEUED_LEVEL,
 				      level + 1);
@@ -1507,7 +1514,7 @@ task_preemptible_rq(struct task_struct *p, cpumask_t *chk_mask,
 #endif
 
 	while (level < preempt_level) {
-		if(cpumask_and(&tmp, chk_mask, &sched_rq_queued_masks[level]))
+		if (cpumask_and(&tmp, chk_mask, &sched_rq_queued_masks[level]))
 			return best_mask_cpu(task_cpu(p), &tmp);
 
 		level = find_next_bit(sched_rq_queued_masks_bitmap,
@@ -1520,14 +1527,14 @@ task_preemptible_rq(struct task_struct *p, cpumask_t *chk_mask,
 	 * policy task than p
 	 */
 	if (only_preempt_low_policy)
-		return nr_cpu_ids;
+		return best_mask_cpu(task_cpu(p), chk_mask);
 
 	if (unlikely(level != preempt_level))
-		return nr_cpu_ids;
+		return best_mask_cpu(task_cpu(p), chk_mask);
 
 	/* IDLEPRIO tasks never preempt anything but idle */
 	if (idleprio_task(p))
-		return nr_cpu_ids;
+		return best_mask_cpu(task_cpu(p), chk_mask);
 
 	if (cpumask_and(&tmp, chk_mask, &sched_rq_queued_masks[preempt_level])) {
 		if (unlikely((SCHED_RQ_RT == level))) {
@@ -1540,23 +1547,7 @@ task_preemptible_rq(struct task_struct *p, cpumask_t *chk_mask,
 		return best_mask_cpu(task_cpu(p), &tmp);
 	}
 
-	return nr_cpu_ids;
-}
-
-/*
- * cpumask_random - get a random cpu from a cpumask
- * @rand: random seed cpu to start with
- * @srcp: the cpumask pointer
- *
- * Returns >= nr_cpu_ids if no cpus set.
- */
-static inline unsigned int cpumask_random(unsigned int rand,
-					  const cpumask_t *srcp)
-{
-	if ((rand = cpumask_next(rand, srcp)) >= nr_cpu_ids)
-		rand = cpumask_first(srcp);
-
-	return rand;
+	return best_mask_cpu(task_cpu(p), chk_mask);
 }
 
 /*
@@ -1569,7 +1560,6 @@ static inline unsigned int cpumask_random(unsigned int rand,
 static inline int select_task_rq(struct task_struct *p, int wake_flags)
 {
 	cpumask_t chk_mask;
-	int cpu;
 
 	if (unlikely(!cpumask_and(&chk_mask, &p->cpus_allowed, cpu_online_mask)))
 		return select_fallback_rq(task_cpu(p), p);
@@ -1580,11 +1570,7 @@ static inline int select_task_rq(struct task_struct *p, int wake_flags)
 	 * don't trigger a preemption if there are no idle cpus,
 	 * instead waiting for current to deschedule.
 	 */
-	cpu = task_preemptible_rq(p, &chk_mask, wake_flags & WF_SYNC);
-	if (cpu >= nr_cpu_ids)
-		return cpumask_random(task_cpu(p), &chk_mask);
-
-	return cpu;
+	return task_preemptible_rq(p, &chk_mask, wake_flags & WF_SYNC);
 }
 #else /* CONFIG_SMP */
 static inline int select_task_rq(struct task_struct *p, int wake_flags)
@@ -2841,8 +2827,7 @@ static int active_load_balance_cpu_stop(void *data)
 	/*
 	 * _something_ may have changed the task, double check again
 	 */
-	if (task_queued(p) &&
-	    task_rq(p) == rq &&
+	if (task_queued(p) && task_rq(p) == rq &&
 	    cpumask_and(&tmp, &p->cpus_allowed, &sched_cpu_sg_idle_mask))
 		rq = __migrate_task(rq, p, cpumask_any(&tmp));
 
@@ -2908,7 +2893,7 @@ static inline bool pds_sg_balance(struct rq *rq)
 	 * First cpu in smt group does not do smt balance, unless
 	 * other cpu is smt balance suppressed.
 	 */
-	if (cpu == cpumask_first(cpu_smt_mask(cpu)) &&
+	if (cpu == per_cpu(sd_llc_id, cpu) &&
 	    !cpumask_intersects(cpu_smt_mask(cpu), &sched_cpu_sb_suppress_mask))
 		return false;
 
@@ -2944,10 +2929,10 @@ static inline bool pds_load_balance(struct rq *rq)
 
 	rq->next_balance = (rq->clock & BALANCE_INTERVAL_MASK) + rq->balance_inc;
 
-	if (unlikely(rq->curr == rq->idle))
-		return false;
-
-	if ((node = rq->curr->sl_node.next[0]) == &rq->sl_header)
+	/*
+	 * this function is called when rq is locked and nr_running >= 2
+	 */
+	if (unlikely((node = rq->sl_header.next[0]->next[0]) == &rq->sl_header))
 		return false;
 
 	p = skiplist_entry(node, struct task_struct, sl_node);
@@ -5982,8 +5967,6 @@ int sched_cpu_activate(unsigned int cpu)
 	set_rq_online(rq);
 	raw_spin_unlock_irqrestore(&rq->lock, flags);
 
-	/* TODO: ?set sched_rq_queued_masks[SCHED_RQ_EMPTY] when CPU is online */
-
 	return 0;
 }
 
@@ -6040,14 +6023,6 @@ int sched_cpu_dying(unsigned int cpu)
 	raw_spin_lock_irqsave(&rq->lock, flags);
 	set_rq_offline(rq);
 	migrate_tasks(rq);
-
-	/*
-	 * PDS: TODO debug load to test still need set rq to idle?
-	 * clear sched_rq_queued_masks[SCHED_RQ_EMPTY]when CPU is offline,
-	 * let it looks *busy*
-	 */
-	set_rq_task(rq, rq->idle);
-	update_rq_clock(rq);
 	raw_spin_unlock_irqrestore(&rq->lock, flags);
 
 	hrtick_clear(rq);
-- 
2.18.0.rc1

