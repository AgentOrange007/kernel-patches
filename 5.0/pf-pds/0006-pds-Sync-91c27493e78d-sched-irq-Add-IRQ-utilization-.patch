From a61012906b2de5b6489bdbdefd0b342c4de07654 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sat, 29 Sep 2018 14:58:03 +0800
Subject: [PATCH 06/70] pds: [Sync] 91c27493e78d sched/irq: Add IRQ utilization
 tracking

---
 kernel/sched/Makefile    |  4 ++--
 kernel/sched/pds.c       | 15 ++++++++++++---
 kernel/sched/pds_sched.h | 13 +++++++++++++
 kernel/sched/pelt.c      |  2 ++
 kernel/sched/pelt.h      |  6 ++++++
 5 files changed, 35 insertions(+), 5 deletions(-)

diff --git a/kernel/sched/Makefile b/kernel/sched/Makefile
index 64a3db823d69..8ebe4e33fb5f 100644
--- a/kernel/sched/Makefile
+++ b/kernel/sched/Makefile
@@ -21,7 +21,7 @@ obj-y += pds.o
 else
 obj-y += core.o
 obj-y += fair.o rt.o deadline.o
-obj-$(CONFIG_SMP) += cpudeadline.o topology.o stop_task.o pelt.o
+obj-$(CONFIG_SMP) += cpudeadline.o topology.o stop_task.o
 obj-$(CONFIG_SCHED_AUTOGROUP) += autogroup.o
 obj-$(CONFIG_SCHED_DEBUG) += debug.o
 obj-$(CONFIG_CGROUP_CPUACCT) += cpuacct.o
@@ -29,7 +29,7 @@ endif
 obj-y += loadavg.o clock.o cputime.o
 obj-y += idle.o
 obj-y += wait.o wait_bit.o swait.o completion.o
-obj-$(CONFIG_SMP) += cpupri.o
+obj-$(CONFIG_SMP) += cpupri.o pelt.o
 obj-$(CONFIG_SCHEDSTATS) += stats.o
 obj-$(CONFIG_CPU_FREQ) += cpufreq.o
 obj-$(CONFIG_CPU_FREQ_GOV_SCHEDUTIL) += cpufreq_schedutil.o
diff --git a/kernel/sched/pds.c b/kernel/sched/pds.c
index 8e25595b73b2..0925a5ac8840 100644
--- a/kernel/sched/pds.c
+++ b/kernel/sched/pds.c
@@ -37,6 +37,8 @@
 #include "../workqueue_internal.h"
 #include "../smpboot.h"
 
+#include "pelt.h"
+
 #define CREATE_TRACE_POINTS
 #include <trace/events/sched.h>
 
@@ -329,8 +331,11 @@ static void update_rq_clock_task(struct rq *rq, s64 delta)
  * In theory, the compile should just see 0 here, and optimize out the call
  * to sched_rt_avg_update. But I don't trust it...
  */
+#ifdef HAVE_SCHED_AVG_IRQ
+	s64 steal = 0, irq_delta = 0;
+#endif
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
-	s64 irq_delta = irq_time_read(cpu_of(rq)) - rq->prev_irq_time;
+	irq_delta = irq_time_read(cpu_of(rq)) - rq->prev_irq_time;
 
 	/*
 	 * Since irq_time is only updated on {soft,}irq_exit, we might run into
@@ -355,8 +360,7 @@ static void update_rq_clock_task(struct rq *rq, s64 delta)
 #endif
 #ifdef CONFIG_PARAVIRT_TIME_ACCOUNTING
 	if (static_key_false((&paravirt_steal_rq_enabled))) {
-		s64 steal = paravirt_steal_clock(cpu_of(rq));
-
+		steal = paravirt_steal_clock(cpu_of(rq));
 		steal -= rq->prev_steal_time_rq;
 
 		if (unlikely(steal > delta))
@@ -369,6 +373,11 @@ static void update_rq_clock_task(struct rq *rq, s64 delta)
 #endif
 
 	rq->clock_task += delta;
+
+#ifdef HAVE_SCHED_AVG_IRQ
+	if ((irq_delta + steal))
+		update_irq_load_avg(rq, irq_delta + steal);
+#endif
 }
 
 static inline void update_rq_clock(struct rq *rq)
diff --git a/kernel/sched/pds_sched.h b/kernel/sched/pds_sched.h
index 6d20158b123a..b18fe343171a 100644
--- a/kernel/sched/pds_sched.h
+++ b/kernel/sched/pds_sched.h
@@ -69,6 +69,11 @@ struct rq {
 	int cpu;		/* cpu of this runqueue */
 	bool online;
 
+#if defined(CONFIG_IRQ_TIME_ACCOUNTING) || defined(CONFIG_PARAVIRT_TIME_ACCOUNTING)
+#define HAVE_SCHED_AVG_IRQ
+	struct sched_avg	avg_irq;
+#endif
+
 	unsigned long queued_level;
 	unsigned long pending_level;
 
@@ -166,6 +171,14 @@ static inline void unregister_sched_domain_sysctl(void)
 
 #endif /* CONFIG_SMP */
 
+#ifndef arch_scale_freq_capacity
+static __always_inline
+unsigned long arch_scale_freq_capacity(int cpu)
+{
+		return SCHED_CAPACITY_SCALE;
+}
+#endif
+
 static inline u64 __rq_clock_broken(struct rq *rq)
 {
 	return READ_ONCE(rq->clock);
diff --git a/kernel/sched/pelt.c b/kernel/sched/pelt.c
index 90fb5bc12ad4..d25064beaf23 100644
--- a/kernel/sched/pelt.c
+++ b/kernel/sched/pelt.c
@@ -240,6 +240,7 @@ ___update_load_avg(struct sched_avg *sa, unsigned long load, unsigned long runna
 	WRITE_ONCE(sa->util_avg, sa->util_sum / divider);
 }
 
+#ifndef CONFIG_SCHED_PDS
 /*
  * sched_entity:
  *
@@ -351,6 +352,7 @@ int update_dl_rq_load_avg(u64 now, struct rq *rq, int running)
 
 	return 0;
 }
+#endif
 
 #ifdef CONFIG_HAVE_SCHED_AVG_IRQ
 /*
diff --git a/kernel/sched/pelt.h b/kernel/sched/pelt.h
index 7e56b489ff32..24d5f54659eb 100644
--- a/kernel/sched/pelt.h
+++ b/kernel/sched/pelt.h
@@ -1,10 +1,12 @@
 #ifdef CONFIG_SMP
 
+#ifndef CONFIG_SCHED_PDS
 int __update_load_avg_blocked_se(u64 now, int cpu, struct sched_entity *se);
 int __update_load_avg_se(u64 now, int cpu, struct cfs_rq *cfs_rq, struct sched_entity *se);
 int __update_load_avg_cfs_rq(u64 now, int cpu, struct cfs_rq *cfs_rq);
 int update_rt_rq_load_avg(u64 now, struct rq *rq, int running);
 int update_dl_rq_load_avg(u64 now, struct rq *rq, int running);
+#endif
 
 #ifdef CONFIG_HAVE_SCHED_AVG_IRQ
 int update_irq_load_avg(struct rq *rq, u64 running);
@@ -16,6 +18,7 @@ update_irq_load_avg(struct rq *rq, u64 running)
 }
 #endif
 
+#ifndef CONFIG_SCHED_PDS
 /*
  * When a task is dequeued, its estimated utilization should not be update if
  * its util_avg has not been updated at least once.
@@ -41,9 +44,11 @@ static inline void cfs_se_util_change(struct sched_avg *avg)
 	enqueued &= ~UTIL_AVG_UNCHANGED;
 	WRITE_ONCE(avg->util_est.enqueued, enqueued);
 }
+#endif
 
 #else
 
+#ifndef CONFIG_SCHED_PDS
 static inline int
 update_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq)
 {
@@ -61,6 +66,7 @@ update_dl_rq_load_avg(u64 now, struct rq *rq, int running)
 {
 	return 0;
 }
+#endif
 
 static inline int
 update_irq_load_avg(struct rq *rq, u64 running)
-- 
2.21.0

