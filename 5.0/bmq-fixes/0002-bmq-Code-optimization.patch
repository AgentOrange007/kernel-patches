From 3631c5eb646c3d9c1d4549af000988adf769a0b5 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Wed, 13 Mar 2019 22:31:46 +0800
Subject: [PATCH 2/6] bmq: Code optimization.

---
 kernel/sched/bmq.c | 36 ++++++++++++++++++------------------
 1 file changed, 18 insertions(+), 18 deletions(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 5d4a0022bf38..1eea5905d93f 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -161,7 +161,7 @@ ____cacheline_aligned_in_smp;
 
 static inline void update_sched_rq_watermark(struct rq *rq)
 {
-	unsigned long watermark = find_first_bit(&rq->queue.bitmap[0], bmq_BITS);
+	unsigned long watermark = find_first_bit(rq->queue.bitmap, bmq_BITS);
 	unsigned long last_wm = rq->watermark;
 	int cpu;
 
@@ -172,9 +172,9 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 	cpu = cpu_of(rq);
 	if (!cpumask_andnot(&sched_rq_watermark[last_wm],
 			    &sched_rq_watermark[last_wm], cpumask_of(cpu)))
-		clear_bit(last_wm, &sched_rq_watermark_bitmap[0]);
+		clear_bit(last_wm, sched_rq_watermark_bitmap);
 	cpumask_set_cpu(cpu, &sched_rq_watermark[watermark]);
-	set_bit(watermark, &sched_rq_watermark_bitmap[0]);
+	set_bit(watermark, sched_rq_watermark_bitmap);
 	rq->watermark = watermark;
 
 #ifdef CONFIG_SCHED_SMT
@@ -184,7 +184,7 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 	if (1ULL == last_wm) {
 		if (!cpumask_andnot(&sched_rq_watermark[0],
 				    &sched_rq_watermark[0], cpu_smt_mask(cpu)))
-			clear_bit(0, &sched_rq_watermark_bitmap[0]);
+			clear_bit(0, sched_rq_watermark_bitmap);
 	} else if (1ULL == watermark) {
 		cpumask_t tmp;
 
@@ -192,7 +192,7 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 		if (cpumask_equal(&tmp, cpu_smt_mask(cpu))) {
 			cpumask_or(&sched_rq_watermark[0], cpu_smt_mask(cpu),
 				   &sched_rq_watermark[0]);
-			set_bit(0, &sched_rq_watermark_bitmap[0]);
+			set_bit(0, sched_rq_watermark_bitmap);
 		}
 	}
 #endif
@@ -210,7 +210,7 @@ static inline void bmq_init(struct bmq *q)
 {
 	int i;
 
-	bitmap_zero(&q->bitmap[0], bmq_BITS);
+	bitmap_zero(q->bitmap, bmq_BITS);
 	for(i = 0; i < bmq_BITS; i++)
 		INIT_LIST_HEAD(&q->heads[i]);
 }
@@ -219,7 +219,7 @@ static inline void bmq_init_idle(struct bmq *q, struct task_struct *idle)
 {
 	INIT_LIST_HEAD(&q->heads[IDLE_TASK_SCHED_PRIO]);
 	list_add(&idle->bmq_node, &q->heads[IDLE_TASK_SCHED_PRIO]);
-	set_bit(IDLE_TASK_SCHED_PRIO, &q->bitmap[0]);
+	set_bit(IDLE_TASK_SCHED_PRIO, q->bitmap);
 }
 
 static inline void bmq_add_task(struct task_struct *p, struct bmq *q, int idx)
@@ -248,7 +248,7 @@ static inline void bmq_add_task(struct task_struct *p, struct bmq *q, int idx)
 static inline struct task_struct *
 __rq_next_bmq_task(const struct rq *rq, unsigned long offset)
 {
-	unsigned long idx = find_next_bit(&rq->queue.bitmap[0], bmq_BITS, offset);
+	unsigned long idx = find_next_bit(rq->queue.bitmap, bmq_BITS, offset);
 	const struct list_head *head = &rq->queue.heads[idx];
 
 	BUG_ON(list_empty(head));
@@ -602,7 +602,7 @@ static inline void dequeue_task(struct task_struct *p, struct rq *rq, int flags)
 
 	list_del(&p->bmq_node);
 	if (list_empty(&rq->queue.heads[p->bmq_idx])) {
-		clear_bit(p->bmq_idx, &rq->queue.bitmap[0]);
+		clear_bit(p->bmq_idx, rq->queue.bitmap);
 		update_sched_rq_watermark(rq);
 	}
 	if (1 == --rq->nr_running)
@@ -628,7 +628,7 @@ static inline void enqueue_task(struct task_struct *p, struct rq *rq, int flags)
 
 	p->bmq_idx = task_sched_prio(p);
 	bmq_add_task(p, &rq->queue, p->bmq_idx);
-	set_bit(p->bmq_idx, &rq->queue.bitmap[0]);
+	set_bit(p->bmq_idx, rq->queue.bitmap);
 	update_sched_rq_watermark(rq);
 	if (2 == ++rq->nr_running)
 		cpumask_set_cpu(cpu_of(rq), &sched_rq_pending_mask);
@@ -659,9 +659,9 @@ static inline void requeue_task(struct task_struct *p, struct rq *rq)
 	bmq_add_task(p, &rq->queue, idx);
 	if (idx != p->bmq_idx) {
 		if (list_empty(&rq->queue.heads[p->bmq_idx]))
-			clear_bit(p->bmq_idx, &rq->queue.bitmap[0]);
+			clear_bit(p->bmq_idx, rq->queue.bitmap);
 		p->bmq_idx = idx;
-		set_bit(p->bmq_idx, &rq->queue.bitmap[0]);
+		set_bit(p->bmq_idx, rq->queue.bitmap);
 		update_sched_rq_watermark(rq);
 	}
 }
@@ -680,9 +680,9 @@ static inline int requeue_task_lazy(struct task_struct *p, struct rq *rq)
 	list_del(&p->bmq_node);
 	bmq_add_task(p, &rq->queue, idx);
 	if (list_empty(&rq->queue.heads[p->bmq_idx]))
-		clear_bit(p->bmq_idx, &rq->queue.bitmap[0]);
+		clear_bit(p->bmq_idx, rq->queue.bitmap);
 	p->bmq_idx = idx;
-	set_bit(p->bmq_idx, &rq->queue.bitmap[0]);
+	set_bit(p->bmq_idx, rq->queue.bitmap);
 	update_sched_rq_watermark(rq);
 
 	return 1;
@@ -1406,12 +1406,12 @@ task_preemptible_rq(struct task_struct *p, cpumask_t *chk_mask,
 	cpumask_t tmp;
 	unsigned long level;
 
-	level = find_first_bit(&sched_rq_watermark_bitmap[0], WM_BITS);
+	level = find_first_bit(sched_rq_watermark_bitmap, WM_BITS);
 	while (level < preempt_level) {
 		if (cpumask_and(&tmp, chk_mask, &sched_rq_watermark[level]))
 			return best_mask_cpu(task_cpu(p), &tmp);
 
-		level = find_next_bit(&sched_rq_watermark_bitmap[0], WM_BITS,
+		level = find_next_bit(sched_rq_watermark_bitmap, WM_BITS,
 				      level + 1);
 	}
 
@@ -5540,7 +5540,7 @@ int sched_cpu_deactivate(unsigned int cpu)
 	if (cpumask_weight(cpu_smt_mask(cpu)) == 2) {
 		static_branch_dec_cpuslocked(&sched_smt_present);
 		if (!static_branch_likely(&sched_smt_present)) {
-			clear_bit(0, &sched_rq_watermark_bitmap[0]);
+			clear_bit(0, sched_rq_watermark_bitmap);
 			cpumask_clear(&sched_rq_watermark[0]);
 		}
 	}
@@ -5719,7 +5719,7 @@ void __init sched_init(void)
 
 #ifdef CONFIG_SMP
 	cpumask_copy(&sched_rq_watermark[1], cpu_present_mask);
-	set_bit(1, &sched_rq_watermark_bitmap[0]);
+	set_bit(1, sched_rq_watermark_bitmap);
 #else
 	uprq = &per_cpu(runqueues, 0);
 #endif
-- 
2.21.0.155.ge902e9bcae

