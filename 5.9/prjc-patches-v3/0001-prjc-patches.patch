From d8364b7e262efeb533b55b1276192313fd3fd22a Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Thu, 1 Oct 2020 00:20:50 +0800
Subject: [PATCH 1/5] sched/pds: Rework pds_skiplist_random_level().

---
 kernel/sched/pds_imp.h | 18 +++++++++---------
 1 file changed, 9 insertions(+), 9 deletions(-)

diff --git a/kernel/sched/pds_imp.h b/kernel/sched/pds_imp.h
index 6baee5e961b9..e1f98a83cfcb 100644
--- a/kernel/sched/pds_imp.h
+++ b/kernel/sched/pds_imp.h
@@ -192,11 +192,9 @@ static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
  * pds_skiplist_random_level -- Returns a pseudo-random level number for skip
  * list node which is used in PDS run queue.
  *
- * In current implementation, based on testing, the first 8 bits in microseconds
- * of niffies are suitable for random level population.
- * find_first_bit() is used to satisfy p = 0.5 between each levels, and there
- * should be platform hardware supported instruction(known as ctz/clz) to speed
- * up this function.
+ * __ffs() is used to satisfy p = 0.5 between each levels, and there should be
+ * platform instruction(known as ctz/clz) for acceleration.
+ *
  * The skiplist level for a task is populated when task is created and doesn't
  * change in task's life time. When task is being inserted into run queue, this
  * skiplist level is set to task's sl_node->level, the skiplist insert function
@@ -204,8 +202,6 @@ static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
  */
 static inline int pds_skiplist_random_level(const struct task_struct *p)
 {
-	long unsigned int randseed;
-
 	/*
 	 * 1. Some architectures don't have better than microsecond resolution
 	 * so mask out ~microseconds as a factor of the random seed for skiplist
@@ -213,9 +209,13 @@ static inline int pds_skiplist_random_level(const struct task_struct *p)
 	 * 2. Use address of task structure pointer as another factor of the
 	 * random seed for task burst forking scenario.
 	 */
-	randseed = (task_rq(p)->clock ^ (long unsigned int)p) >> 10;
+	unsigned long randseed = (task_rq(p)->clock ^ (unsigned long)p) >> 10;
+
+	randseed &= __GENMASK(NUM_SKIPLIST_LEVEL - 1, 0);
+	if (randseed)
+		return __ffs(randseed);
 
-	return find_first_bit(&randseed, NUM_SKIPLIST_LEVEL - 1);
+	return (NUM_SKIPLIST_LEVEL - 1);
 }
 
 static void sched_task_fork(struct task_struct *p, struct rq *rq)
-- 
2.29.0.rc2


From 2089efe0365168629f16468b333a3a841903c48d Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sun, 11 Oct 2020 09:22:57 +0800
Subject: [PATCH 2/5] sched/alt: Rework best cpu selection.

Based on testing, selecting first set CPU provide better performance
than current CPU affinity based best_mask_cpu().

Macro SCHED_CPUMASK_FIRST_BIT() and routine sched_cpumask_first_and()
are introduced to reduce overhead calling cpumask_xxxx() routines when
NR_CPUS <= 64.
---
 kernel/sched/alt_core.c | 36 ++++++++++++++++++++++++++++--------
 1 file changed, 28 insertions(+), 8 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index d43ca62fd00f..f6d5c9768701 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -105,6 +105,29 @@ EXPORT_SYMBOL_GPL(sched_smt_present);
  * domain, see cpus_share_cache().
  */
 DEFINE_PER_CPU(int, sd_llc_id);
+
+#if NR_CPUS <= 64
+#define SCHED_CPUMASK_FIRST_BIT(mask)	(__ffs((mask).bits[0]))
+
+static inline unsigned int sched_cpumask_first_and(const struct cpumask *srcp,
+						   const struct cpumask *andp)
+{
+	unsigned long t = srcp->bits[0] & andp->bits[0];
+
+	if (t)
+		return __ffs(t);
+
+	return nr_cpu_ids;
+}
+#else
+#define SCHED_CPUMASK_FIRST_BIT(mask)	(cpumask_fist_bit(&(mask)))
+static inline unsigned int sched_cpumask_first_and(const struct cpumask *srcp,
+						   const struct cpumask *andp)
+{
+	return cpumask_first_and(srcp, andp);
+}
+#endif
+
 #endif /* CONFIG_SMP */
 
 static DEFINE_MUTEX(sched_hotcpu_mutex);
@@ -1520,9 +1543,9 @@ static inline int select_task_rq(struct task_struct *p, struct rq *rq)
 	    cpumask_and(&tmp, &chk_mask, &sched_rq_watermark[IDLE_WM]) ||
 	    cpumask_and(&tmp, &chk_mask,
 			&sched_rq_watermark[task_sched_prio(p, rq) + 1]))
-		return best_mask_cpu(task_cpu(p), &tmp);
+		return SCHED_CPUMASK_FIRST_BIT(tmp);
 
-	return best_mask_cpu(task_cpu(p), &chk_mask);
+	return SCHED_CPUMASK_FIRST_BIT(chk_mask);
 }
 
 void sched_set_stop_task(int cpu, struct task_struct *stop)
@@ -3094,8 +3117,8 @@ static inline int active_load_balance_cpu_stop(void *data)
 {
 	struct rq *rq = this_rq();
 	struct task_struct *p = data;
-	cpumask_t tmp;
 	unsigned long flags;
+	int dcpu;
 
 	local_irq_save(flags);
 
@@ -3105,12 +3128,9 @@ static inline int active_load_balance_cpu_stop(void *data)
 	rq->active_balance = 0;
 	/* _something_ may have changed the task, double check again */
 	if (task_on_rq_queued(p) && task_rq(p) == rq &&
-	    cpumask_and(&tmp, p->cpus_ptr, &sched_sg_idle_mask)) {
-		int cpu = cpu_of(rq);
-		int dcpu = __best_mask_cpu(cpu, &tmp,
-					   per_cpu(sched_cpu_llc_mask, cpu));
+	    (dcpu = sched_cpumask_first_and(p->cpus_ptr, &sched_sg_idle_mask)) <
+	    nr_cpu_ids)
 		rq = move_queued_task(rq, p, dcpu);
-	}
 
 	raw_spin_unlock(&rq->lock);
 	raw_spin_unlock(&p->pi_lock);
-- 
2.29.0.rc2


From 6249d6d02f1bae57029d405cfc1d98839d56fcf6 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sun, 11 Oct 2020 11:15:21 +0800
Subject: [PATCH 3/5] sched/alt: Remove unused sched_cpu_llc_mask.

---
 kernel/sched/alt_core.c | 10 ++--------
 1 file changed, 2 insertions(+), 8 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index f6d5c9768701..40b059846496 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -92,7 +92,6 @@ static cpumask_t sched_rq_pending_mask ____cacheline_aligned_in_smp;
 
 DEFINE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_CHK_LEVEL], sched_cpu_affinity_masks);
 DEFINE_PER_CPU(cpumask_t *, sched_cpu_affinity_end_mask);
-DEFINE_PER_CPU(cpumask_t *, sched_cpu_llc_mask);
 
 #ifdef CONFIG_SCHED_SMT
 DEFINE_STATIC_KEY_FALSE(sched_smt_present);
@@ -5892,8 +5891,6 @@ static void sched_init_topology_cpumask_early(void)
 			cpumask_copy(tmp, cpu_possible_mask);
 			cpumask_clear_cpu(cpu, tmp);
 		}
-		per_cpu(sched_cpu_llc_mask, cpu) =
-			&(per_cpu(sched_cpu_affinity_masks, cpu)[0]);
 		per_cpu(sched_cpu_affinity_end_mask, cpu) =
 			&(per_cpu(sched_cpu_affinity_masks, cpu)[1]);
 		/*per_cpu(sd_llc_id, cpu) = cpu;*/
@@ -5923,7 +5920,6 @@ static void sched_init_topology_cpumask(void)
 		TOPOLOGY_CPUMASK(smt, topology_sibling_cpumask(cpu), false);
 #endif
 		per_cpu(sd_llc_id, cpu) = cpumask_first(cpu_coregroup_mask(cpu));
-		per_cpu(sched_cpu_llc_mask, cpu) = chk;
 		TOPOLOGY_CPUMASK(coregroup, cpu_coregroup_mask(cpu), false);
 
 		TOPOLOGY_CPUMASK(core, topology_core_cpumask(cpu), false);
@@ -5931,10 +5927,8 @@ static void sched_init_topology_cpumask(void)
 		TOPOLOGY_CPUMASK(others, cpu_online_mask, true);
 
 		per_cpu(sched_cpu_affinity_end_mask, cpu) = chk;
-		printk(KERN_INFO "sched: cpu#%02d llc_id = %d, llc_mask idx = %d\n",
-		       cpu, per_cpu(sd_llc_id, cpu),
-		       (int) (per_cpu(sched_cpu_llc_mask, cpu) -
-			      &(per_cpu(sched_cpu_affinity_masks, cpu)[0])));
+		printk(KERN_INFO "sched: cpu#%02d llc_id = %d\n",
+		       cpu, per_cpu(sd_llc_id, cpu));
 	}
 }
 #endif
-- 
2.29.0.rc2


From 00da344d4b2850d0910e2d6100de00399a22f3bd Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Fri, 16 Oct 2020 15:06:09 +0800
Subject: [PATCH 4/5] sched/alt: Introduce sched_best_cpu().

Introduce new framework which currently only work for LLC. Can be expend
for IMIT or BIG.little in the furture.
---
 kernel/sched/alt_core.c  | 51 ++++++++++++++++++++++++++++++++++------
 kernel/sched/alt_sched.h | 14 -----------
 kernel/sched/topology.c  | 10 +++++++-
 3 files changed, 53 insertions(+), 22 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 40b059846496..cec61ca0abb2 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -105,9 +105,17 @@ EXPORT_SYMBOL_GPL(sched_smt_present);
  */
 DEFINE_PER_CPU(int, sd_llc_id);
 
-#if NR_CPUS <= 64
-#define SCHED_CPUMASK_FIRST_BIT(mask)	(__ffs((mask).bits[0]))
+enum {
+	LLC_LEVEL = 1,
+	NR_BEST_CPU_LEVEL
+};
+
+#define NR_BEST_CPU_MASK (1 << (NR_BEST_CPU_LEVEL - 1))
 
+static cpumask_t
+sched_best_cpu_masks[NR_CPUS][NR_BEST_CPU_MASK] ____cacheline_aligned_in_smp;
+
+#if NR_CPUS <= 64
 static inline unsigned int sched_cpumask_first_and(const struct cpumask *srcp,
 						   const struct cpumask *andp)
 {
@@ -118,13 +126,35 @@ static inline unsigned int sched_cpumask_first_and(const struct cpumask *srcp,
 
 	return nr_cpu_ids;
 }
+
+static inline unsigned int sched_best_cpu(const unsigned int cpu,
+					  const struct cpumask *m)
+{
+	cpumask_t *chk = sched_best_cpu_masks[cpu];
+	unsigned long t;
+
+	while ((t = chk->bits[0] & m->bits[0]) == 0UL)
+		chk++;
+
+	return __ffs(t);
+}
 #else
-#define SCHED_CPUMASK_FIRST_BIT(mask)	(cpumask_fist_bit(&(mask)))
 static inline unsigned int sched_cpumask_first_and(const struct cpumask *srcp,
 						   const struct cpumask *andp)
 {
 	return cpumask_first_and(srcp, andp);
 }
+
+static inline unsigned int sched_best_cpu(const unsigned int cpu,
+					  const struct cpumask *m)
+{
+	cpumask_t t, *chk = sched_best_cpu_masks[cpu];
+
+	while (!cpumask_and(&t, chk, m))
+		chk++;
+
+	return cpumask_any(t);
+}
 #endif
 
 #endif /* CONFIG_SMP */
@@ -822,7 +852,7 @@ int get_nohz_timer_target(void)
 		default_cpu = cpu;
 	}
 
-	for (mask = &(per_cpu(sched_cpu_affinity_masks, cpu)[0]);
+	for (mask = per_cpu(sched_cpu_affinity_masks, cpu);
 	     mask < per_cpu(sched_cpu_affinity_end_mask, cpu); mask++)
 		for_each_cpu_and(i, mask, housekeeping_cpumask(HK_FLAG_TIMER))
 			if (!idle_cpu(i))
@@ -1542,9 +1572,9 @@ static inline int select_task_rq(struct task_struct *p, struct rq *rq)
 	    cpumask_and(&tmp, &chk_mask, &sched_rq_watermark[IDLE_WM]) ||
 	    cpumask_and(&tmp, &chk_mask,
 			&sched_rq_watermark[task_sched_prio(p, rq) + 1]))
-		return SCHED_CPUMASK_FIRST_BIT(tmp);
+		return sched_best_cpu(task_cpu(p), &tmp);
 
-	return SCHED_CPUMASK_FIRST_BIT(chk_mask);
+	return sched_best_cpu(task_cpu(p), &chk_mask);
 }
 
 void sched_set_stop_task(int cpu, struct task_struct *stop)
@@ -3543,7 +3573,7 @@ static inline int take_other_rq_tasks(struct rq *rq, int cpu)
 	if (cpumask_empty(&sched_rq_pending_mask))
 		return 0;
 
-	affinity_mask = &(per_cpu(sched_cpu_affinity_masks, cpu)[0]);
+	affinity_mask = per_cpu(sched_cpu_affinity_masks, cpu);
 	end_mask = per_cpu(sched_cpu_affinity_end_mask, cpu);
 	do {
 		int i;
@@ -5894,6 +5924,10 @@ static void sched_init_topology_cpumask_early(void)
 		per_cpu(sched_cpu_affinity_end_mask, cpu) =
 			&(per_cpu(sched_cpu_affinity_masks, cpu)[1]);
 		/*per_cpu(sd_llc_id, cpu) = cpu;*/
+
+		for (level = 0; level < NR_BEST_CPU_MASK; level++)
+			cpumask_copy(&sched_best_cpu_masks[cpu][level],
+				     cpu_possible_mask);
 	}
 }
 
@@ -5929,6 +5963,9 @@ static void sched_init_topology_cpumask(void)
 		per_cpu(sched_cpu_affinity_end_mask, cpu) = chk;
 		printk(KERN_INFO "sched: cpu#%02d llc_id = %d\n",
 		       cpu, per_cpu(sd_llc_id, cpu));
+
+		cpumask_copy(sched_best_cpu_masks[cpu],
+			     cpu_coregroup_mask(cpu));
 	}
 }
 #endif
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index 03f8b8b1aa27..fee65eeb1405 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -225,20 +225,6 @@ enum {
 
 DECLARE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_CHK_LEVEL], sched_cpu_affinity_masks);
 
-static inline int __best_mask_cpu(int cpu, const cpumask_t *cpumask,
-				  const cpumask_t *mask)
-{
-	while ((cpu = cpumask_any_and(cpumask, mask)) >= nr_cpu_ids)
-		mask++;
-	return cpu;
-}
-
-static inline int best_mask_cpu(int cpu, const cpumask_t *cpumask)
-{
-	return cpumask_test_cpu(cpu, cpumask)? cpu :
-		__best_mask_cpu(cpu, cpumask, &(per_cpu(sched_cpu_affinity_masks, cpu)[0]));
-}
-
 extern void flush_smp_call_function_from_idle(void);
 
 #else  /* !CONFIG_SMP */
diff --git a/kernel/sched/topology.c b/kernel/sched/topology.c
index cc946a9bd550..bbd96ce88008 100644
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@ -2331,7 +2331,15 @@ int __read_mostly		node_reclaim_distance = RECLAIM_DISTANCE;
 
 int sched_numa_find_closest(const struct cpumask *cpus, int cpu)
 {
-	return best_mask_cpu(cpu, cpus);
+	const cpumask_t *mask;
+
+	if (cpumask_test_cpu(cpu, cpus))
+		return cpu;
+
+	mask = per_cpu(sched_cpu_affinity_masks, cpu);
+	while ((cpu = cpumask_any_and(cpus, mask)) >= nr_cpu_ids)
+		mask++;
+	return cpu;
 }
 #endif /* CONFIG_NUMA */
 #endif
-- 
2.29.0.rc2


From 9affbd8ac0f37f29ca24ae0164f037aec7a1626f Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Mon, 19 Oct 2020 17:07:00 +0800
Subject: [PATCH 5/5] Project-C v5.9-r1

---
 kernel/sched/alt_core.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index cec61ca0abb2..fa0ba0d55503 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -52,7 +52,7 @@
  */
 EXPORT_TRACEPOINT_SYMBOL_GPL(pelt_irq_tp);
 
-#define ALT_SCHED_VERSION "v5.9-r0"
+#define ALT_SCHED_VERSION "v5.9-r1"
 
 /* rt_prio(prio) defined in include/linux/sched/rt.h */
 #define rt_task(p)		rt_prio((p)->prio)
-- 
2.29.0.rc2

