From 10068644cadee9408467e79b7932c5abdbce5910 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Mon, 26 Oct 2020 13:37:09 +0800
Subject: [PATCH 1/7] sched/alt: Reduce NUM_SKIPLIST_LEVEL to 4.

---
 include/linux/skip_list.h |  8 +++-----
 kernel/sched/pds_imp.h    | 21 ++++++++-------------
 2 files changed, 11 insertions(+), 18 deletions(-)

diff --git a/include/linux/skip_list.h b/include/linux/skip_list.h
index 47ca955a451d..2a8fc7c1a04f 100644
--- a/include/linux/skip_list.h
+++ b/include/linux/skip_list.h
@@ -50,7 +50,7 @@
 
 #include <linux/kernel.h>
 
-#define NUM_SKIPLIST_LEVEL (8)
+#define NUM_SKIPLIST_LEVEL (4)
 
 struct skiplist_node {
 	int level;	/* Levels in this node */
@@ -59,10 +59,8 @@ struct skiplist_node {
 };
 
 #define SKIPLIST_NODE_INIT(name) { 0,\
-				   {&name, &name, &name, &name,\
-				    &name, &name, &name, &name},\
-				   {&name, &name, &name, &name,\
-				    &name, &name, &name, &name},\
+				   {&name, &name, &name, &name},\
+				   {&name, &name, &name, &name},\
 				 }
 
 static inline void INIT_SKIPLIST_NODE(struct skiplist_node *node)
diff --git a/kernel/sched/pds_imp.h b/kernel/sched/pds_imp.h
index e1f98a83cfcb..bd3b84cbafa7 100644
--- a/kernel/sched/pds_imp.h
+++ b/kernel/sched/pds_imp.h
@@ -168,22 +168,17 @@ static inline unsigned long sched_queue_watermark(struct rq *rq)
 
 static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
 {
-	struct skiplist_node *node = p->sl_node.prev[0];
+	struct skiplist_node *node;
 
-	if (node != &rq->sl_header) {
-		struct task_struct *t = skiplist_entry(node, struct task_struct, sl_node);
-
-		if (t->priodl > p->priodl)
-			return true;
-	}
+	node = p->sl_node.prev[0];
+	if (node != &rq->sl_header &&
+	    skiplist_entry(node, struct task_struct, sl_node)->priodl > p->priodl)
+		return true;
 
 	node = p->sl_node.next[0];
-	if (node != &rq->sl_header) {
-		struct task_struct *t = skiplist_entry(node, struct task_struct, sl_node);
-
-		if (t->priodl < p->priodl)
-			return true;
-	}
+	if (node != &rq->sl_header &&
+	    skiplist_entry(node, struct task_struct, sl_node)->priodl < p->priodl)
+		return true;
 
 	return false;
 }
-- 
2.29.2.260.ge31aba42fb


From 13e0cf6276ceb95983730e4bf980cdb54452e82a Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 3 Nov 2020 22:13:13 +0800
Subject: [PATCH 2/7] Revert "sched/alt: Fix compilation when NR_CPUS > 64"

This reverts commit 9a879be8808af904d6faf63b6a9247e76a3b9d7e.
---
 kernel/sched/alt_core.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index edba089affc0..fa0ba0d55503 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -153,7 +153,7 @@ static inline unsigned int sched_best_cpu(const unsigned int cpu,
 	while (!cpumask_and(&t, chk, m))
 		chk++;
 
-	return cpumask_any(&t);
+	return cpumask_any(t);
 }
 #endif
 
-- 
2.29.2.260.ge31aba42fb


From e2bad3a80415c5a2e18a0f039ee5b85887eb8bd9 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 3 Nov 2020 22:13:28 +0800
Subject: [PATCH 3/7] Revert "sched/alt: Introduce sched_best_cpu()."

This reverts commit 7e6b0567a19b1f9b8beb97255bf3ffee5a287f01.
---
 kernel/sched/alt_core.c  | 51 ++++++----------------------------------
 kernel/sched/alt_sched.h | 14 +++++++++++
 kernel/sched/topology.c  | 10 +-------
 3 files changed, 22 insertions(+), 53 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index fa0ba0d55503..57d10ccf39b8 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -105,17 +105,9 @@ EXPORT_SYMBOL_GPL(sched_smt_present);
  */
 DEFINE_PER_CPU(int, sd_llc_id);
 
-enum {
-	LLC_LEVEL = 1,
-	NR_BEST_CPU_LEVEL
-};
-
-#define NR_BEST_CPU_MASK (1 << (NR_BEST_CPU_LEVEL - 1))
-
-static cpumask_t
-sched_best_cpu_masks[NR_CPUS][NR_BEST_CPU_MASK] ____cacheline_aligned_in_smp;
-
 #if NR_CPUS <= 64
+#define SCHED_CPUMASK_FIRST_BIT(mask)	(__ffs((mask).bits[0]))
+
 static inline unsigned int sched_cpumask_first_and(const struct cpumask *srcp,
 						   const struct cpumask *andp)
 {
@@ -126,35 +118,13 @@ static inline unsigned int sched_cpumask_first_and(const struct cpumask *srcp,
 
 	return nr_cpu_ids;
 }
-
-static inline unsigned int sched_best_cpu(const unsigned int cpu,
-					  const struct cpumask *m)
-{
-	cpumask_t *chk = sched_best_cpu_masks[cpu];
-	unsigned long t;
-
-	while ((t = chk->bits[0] & m->bits[0]) == 0UL)
-		chk++;
-
-	return __ffs(t);
-}
 #else
+#define SCHED_CPUMASK_FIRST_BIT(mask)	(cpumask_fist_bit(&(mask)))
 static inline unsigned int sched_cpumask_first_and(const struct cpumask *srcp,
 						   const struct cpumask *andp)
 {
 	return cpumask_first_and(srcp, andp);
 }
-
-static inline unsigned int sched_best_cpu(const unsigned int cpu,
-					  const struct cpumask *m)
-{
-	cpumask_t t, *chk = sched_best_cpu_masks[cpu];
-
-	while (!cpumask_and(&t, chk, m))
-		chk++;
-
-	return cpumask_any(t);
-}
 #endif
 
 #endif /* CONFIG_SMP */
@@ -852,7 +822,7 @@ int get_nohz_timer_target(void)
 		default_cpu = cpu;
 	}
 
-	for (mask = per_cpu(sched_cpu_affinity_masks, cpu);
+	for (mask = &(per_cpu(sched_cpu_affinity_masks, cpu)[0]);
 	     mask < per_cpu(sched_cpu_affinity_end_mask, cpu); mask++)
 		for_each_cpu_and(i, mask, housekeeping_cpumask(HK_FLAG_TIMER))
 			if (!idle_cpu(i))
@@ -1572,9 +1542,9 @@ static inline int select_task_rq(struct task_struct *p, struct rq *rq)
 	    cpumask_and(&tmp, &chk_mask, &sched_rq_watermark[IDLE_WM]) ||
 	    cpumask_and(&tmp, &chk_mask,
 			&sched_rq_watermark[task_sched_prio(p, rq) + 1]))
-		return sched_best_cpu(task_cpu(p), &tmp);
+		return SCHED_CPUMASK_FIRST_BIT(tmp);
 
-	return sched_best_cpu(task_cpu(p), &chk_mask);
+	return SCHED_CPUMASK_FIRST_BIT(chk_mask);
 }
 
 void sched_set_stop_task(int cpu, struct task_struct *stop)
@@ -3573,7 +3543,7 @@ static inline int take_other_rq_tasks(struct rq *rq, int cpu)
 	if (cpumask_empty(&sched_rq_pending_mask))
 		return 0;
 
-	affinity_mask = per_cpu(sched_cpu_affinity_masks, cpu);
+	affinity_mask = &(per_cpu(sched_cpu_affinity_masks, cpu)[0]);
 	end_mask = per_cpu(sched_cpu_affinity_end_mask, cpu);
 	do {
 		int i;
@@ -5924,10 +5894,6 @@ static void sched_init_topology_cpumask_early(void)
 		per_cpu(sched_cpu_affinity_end_mask, cpu) =
 			&(per_cpu(sched_cpu_affinity_masks, cpu)[1]);
 		/*per_cpu(sd_llc_id, cpu) = cpu;*/
-
-		for (level = 0; level < NR_BEST_CPU_MASK; level++)
-			cpumask_copy(&sched_best_cpu_masks[cpu][level],
-				     cpu_possible_mask);
 	}
 }
 
@@ -5963,9 +5929,6 @@ static void sched_init_topology_cpumask(void)
 		per_cpu(sched_cpu_affinity_end_mask, cpu) = chk;
 		printk(KERN_INFO "sched: cpu#%02d llc_id = %d\n",
 		       cpu, per_cpu(sd_llc_id, cpu));
-
-		cpumask_copy(sched_best_cpu_masks[cpu],
-			     cpu_coregroup_mask(cpu));
 	}
 }
 #endif
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index fee65eeb1405..03f8b8b1aa27 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -225,6 +225,20 @@ enum {
 
 DECLARE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_CHK_LEVEL], sched_cpu_affinity_masks);
 
+static inline int __best_mask_cpu(int cpu, const cpumask_t *cpumask,
+				  const cpumask_t *mask)
+{
+	while ((cpu = cpumask_any_and(cpumask, mask)) >= nr_cpu_ids)
+		mask++;
+	return cpu;
+}
+
+static inline int best_mask_cpu(int cpu, const cpumask_t *cpumask)
+{
+	return cpumask_test_cpu(cpu, cpumask)? cpu :
+		__best_mask_cpu(cpu, cpumask, &(per_cpu(sched_cpu_affinity_masks, cpu)[0]));
+}
+
 extern void flush_smp_call_function_from_idle(void);
 
 #else  /* !CONFIG_SMP */
diff --git a/kernel/sched/topology.c b/kernel/sched/topology.c
index bbd96ce88008..cc946a9bd550 100644
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@ -2331,15 +2331,7 @@ int __read_mostly		node_reclaim_distance = RECLAIM_DISTANCE;
 
 int sched_numa_find_closest(const struct cpumask *cpus, int cpu)
 {
-	const cpumask_t *mask;
-
-	if (cpumask_test_cpu(cpu, cpus))
-		return cpu;
-
-	mask = per_cpu(sched_cpu_affinity_masks, cpu);
-	while ((cpu = cpumask_any_and(cpus, mask)) >= nr_cpu_ids)
-		mask++;
-	return cpu;
+	return best_mask_cpu(cpu, cpus);
 }
 #endif /* CONFIG_NUMA */
 #endif
-- 
2.29.2.260.ge31aba42fb


From 1b79182b7f7a4b1e8031cf4f4ac6b86a76cb03f3 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 3 Nov 2020 22:13:44 +0800
Subject: [PATCH 4/7] Revert "sched/alt: Remove unused sched_cpu_llc_mask."

This reverts commit d18994d3d143830fe250b9a27e76f3c1b51459d7.
---
 kernel/sched/alt_core.c | 10 ++++++++--
 1 file changed, 8 insertions(+), 2 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 57d10ccf39b8..1e2adb3d6a7b 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -92,6 +92,7 @@ static cpumask_t sched_rq_pending_mask ____cacheline_aligned_in_smp;
 
 DEFINE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_CHK_LEVEL], sched_cpu_affinity_masks);
 DEFINE_PER_CPU(cpumask_t *, sched_cpu_affinity_end_mask);
+DEFINE_PER_CPU(cpumask_t *, sched_cpu_llc_mask);
 
 #ifdef CONFIG_SCHED_SMT
 DEFINE_STATIC_KEY_FALSE(sched_smt_present);
@@ -5891,6 +5892,8 @@ static void sched_init_topology_cpumask_early(void)
 			cpumask_copy(tmp, cpu_possible_mask);
 			cpumask_clear_cpu(cpu, tmp);
 		}
+		per_cpu(sched_cpu_llc_mask, cpu) =
+			&(per_cpu(sched_cpu_affinity_masks, cpu)[0]);
 		per_cpu(sched_cpu_affinity_end_mask, cpu) =
 			&(per_cpu(sched_cpu_affinity_masks, cpu)[1]);
 		/*per_cpu(sd_llc_id, cpu) = cpu;*/
@@ -5920,6 +5923,7 @@ static void sched_init_topology_cpumask(void)
 		TOPOLOGY_CPUMASK(smt, topology_sibling_cpumask(cpu), false);
 #endif
 		per_cpu(sd_llc_id, cpu) = cpumask_first(cpu_coregroup_mask(cpu));
+		per_cpu(sched_cpu_llc_mask, cpu) = chk;
 		TOPOLOGY_CPUMASK(coregroup, cpu_coregroup_mask(cpu), false);
 
 		TOPOLOGY_CPUMASK(core, topology_core_cpumask(cpu), false);
@@ -5927,8 +5931,10 @@ static void sched_init_topology_cpumask(void)
 		TOPOLOGY_CPUMASK(others, cpu_online_mask, true);
 
 		per_cpu(sched_cpu_affinity_end_mask, cpu) = chk;
-		printk(KERN_INFO "sched: cpu#%02d llc_id = %d\n",
-		       cpu, per_cpu(sd_llc_id, cpu));
+		printk(KERN_INFO "sched: cpu#%02d llc_id = %d, llc_mask idx = %d\n",
+		       cpu, per_cpu(sd_llc_id, cpu),
+		       (int) (per_cpu(sched_cpu_llc_mask, cpu) -
+			      &(per_cpu(sched_cpu_affinity_masks, cpu)[0])));
 	}
 }
 #endif
-- 
2.29.2.260.ge31aba42fb


From 423a59da1ce076a51fe478969eeef5b318b31af6 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 3 Nov 2020 22:13:59 +0800
Subject: [PATCH 5/7] Revert "sched/alt: Rework best cpu selection."

This reverts commit 173014cfa89544d02216612e812b950a31246c6d.
---
 kernel/sched/alt_core.c | 36 ++++++++----------------------------
 1 file changed, 8 insertions(+), 28 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 1e2adb3d6a7b..7cb0edc7fe8c 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -105,29 +105,6 @@ EXPORT_SYMBOL_GPL(sched_smt_present);
  * domain, see cpus_share_cache().
  */
 DEFINE_PER_CPU(int, sd_llc_id);
-
-#if NR_CPUS <= 64
-#define SCHED_CPUMASK_FIRST_BIT(mask)	(__ffs((mask).bits[0]))
-
-static inline unsigned int sched_cpumask_first_and(const struct cpumask *srcp,
-						   const struct cpumask *andp)
-{
-	unsigned long t = srcp->bits[0] & andp->bits[0];
-
-	if (t)
-		return __ffs(t);
-
-	return nr_cpu_ids;
-}
-#else
-#define SCHED_CPUMASK_FIRST_BIT(mask)	(cpumask_fist_bit(&(mask)))
-static inline unsigned int sched_cpumask_first_and(const struct cpumask *srcp,
-						   const struct cpumask *andp)
-{
-	return cpumask_first_and(srcp, andp);
-}
-#endif
-
 #endif /* CONFIG_SMP */
 
 static DEFINE_MUTEX(sched_hotcpu_mutex);
@@ -1543,9 +1520,9 @@ static inline int select_task_rq(struct task_struct *p, struct rq *rq)
 	    cpumask_and(&tmp, &chk_mask, &sched_rq_watermark[IDLE_WM]) ||
 	    cpumask_and(&tmp, &chk_mask,
 			&sched_rq_watermark[task_sched_prio(p, rq) + 1]))
-		return SCHED_CPUMASK_FIRST_BIT(tmp);
+		return best_mask_cpu(task_cpu(p), &tmp);
 
-	return SCHED_CPUMASK_FIRST_BIT(chk_mask);
+	return best_mask_cpu(task_cpu(p), &chk_mask);
 }
 
 void sched_set_stop_task(int cpu, struct task_struct *stop)
@@ -3117,8 +3094,8 @@ static inline int active_load_balance_cpu_stop(void *data)
 {
 	struct rq *rq = this_rq();
 	struct task_struct *p = data;
+	cpumask_t tmp;
 	unsigned long flags;
-	int dcpu;
 
 	local_irq_save(flags);
 
@@ -3128,9 +3105,12 @@ static inline int active_load_balance_cpu_stop(void *data)
 	rq->active_balance = 0;
 	/* _something_ may have changed the task, double check again */
 	if (task_on_rq_queued(p) && task_rq(p) == rq &&
-	    (dcpu = sched_cpumask_first_and(p->cpus_ptr, &sched_sg_idle_mask)) <
-	    nr_cpu_ids)
+	    cpumask_and(&tmp, p->cpus_ptr, &sched_sg_idle_mask)) {
+		int cpu = cpu_of(rq);
+		int dcpu = __best_mask_cpu(cpu, &tmp,
+					   per_cpu(sched_cpu_llc_mask, cpu));
 		rq = move_queued_task(rq, p, dcpu);
+	}
 
 	raw_spin_unlock(&rq->lock);
 	raw_spin_unlock(&p->pi_lock);
-- 
2.29.2.260.ge31aba42fb


From cf50b418811e5307eb5416bda9895f1d0f7f1580 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Mon, 9 Nov 2020 11:08:36 +0800
Subject: [PATCH 6/7] sched/alt: Enhance best_mask_cpu() for better
 performance.

Enhance best_mask_cpu() performance when NR_CPUS <= 64.
---
 kernel/sched/alt_core.c  |  6 ++++--
 kernel/sched/alt_sched.h | 31 ++++++++++++++++++++++---------
 2 files changed, 26 insertions(+), 11 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 7cb0edc7fe8c..3a4281ba65e6 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -90,7 +90,7 @@ int sched_yield_type __read_mostly = 1;
 #ifdef CONFIG_SMP
 static cpumask_t sched_rq_pending_mask ____cacheline_aligned_in_smp;
 
-DEFINE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_CHK_LEVEL], sched_cpu_affinity_masks);
+DEFINE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_LEVELS], sched_cpu_affinity_masks);
 DEFINE_PER_CPU(cpumask_t *, sched_cpu_affinity_end_mask);
 DEFINE_PER_CPU(cpumask_t *, sched_cpu_llc_mask);
 
@@ -5867,7 +5867,7 @@ static void sched_init_topology_cpumask_early(void)
 	cpumask_t *tmp;
 
 	for_each_possible_cpu(cpu) {
-		for (level = 0; level < NR_CPU_AFFINITY_CHK_LEVEL; level++) {
+		for (level = 0; level < NR_CPU_AFFINITY_LEVELS; level++) {
 			tmp = &(per_cpu(sched_cpu_affinity_masks, cpu)[level]);
 			cpumask_copy(tmp, cpu_possible_mask);
 			cpumask_clear_cpu(cpu, tmp);
@@ -5898,6 +5898,8 @@ static void sched_init_topology_cpumask(void)
 
 		chk = &(per_cpu(sched_cpu_affinity_masks, cpu)[0]);
 
+		cpumask_copy(chk++, cpumask_of(cpu));
+
 		cpumask_complement(chk, cpumask_of(cpu));
 #ifdef CONFIG_SCHED_SMT
 		TOPOLOGY_CPUMASK(smt, topology_sibling_cpumask(cpu), false);
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index 03f8b8b1aa27..4698d6d16a2d 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -213,30 +213,43 @@ static inline void unregister_sched_domain_sysctl(void)
 extern bool sched_smp_initialized;
 
 enum {
-	BASE_CPU_AFFINITY_CHK_LEVEL = 1,
+	ITSELF_LEVEL_SPACE_HOLDER,
 #ifdef CONFIG_SCHED_SMT
-	SMT_CPU_AFFINITY_CHK_LEVEL_SPACE_HOLDER,
+	SMT_LEVEL_SPACE_HOLDER,
 #endif
-#ifdef CONFIG_SCHED_MC
-	MC_CPU_AFFINITY_CHK_LEVEL_SPACE_HOLDER,
-#endif
-	NR_CPU_AFFINITY_CHK_LEVEL
+	COREGROUP_LEVEL_SPACE_HOLDER,
+	CORE_LEVEL_SPACE_HOLDER,
+	OTHER_LEVEL_SPACE_HOLDER,
+	NR_CPU_AFFINITY_LEVELS
 };
 
-DECLARE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_CHK_LEVEL], sched_cpu_affinity_masks);
+DECLARE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_LEVELS], sched_cpu_affinity_masks);
 
 static inline int __best_mask_cpu(int cpu, const cpumask_t *cpumask,
 				  const cpumask_t *mask)
 {
+#if NR_CPUS <= 64
+	unsigned long t;
+
+	while ((t = cpumask->bits[0] & mask->bits[0]) == 0UL)
+		mask++;
+
+	return __ffs(t);
+#else
 	while ((cpu = cpumask_any_and(cpumask, mask)) >= nr_cpu_ids)
 		mask++;
 	return cpu;
+#endif
 }
 
 static inline int best_mask_cpu(int cpu, const cpumask_t *cpumask)
 {
-	return cpumask_test_cpu(cpu, cpumask)? cpu :
-		__best_mask_cpu(cpu, cpumask, &(per_cpu(sched_cpu_affinity_masks, cpu)[0]));
+#if NR_CPUS <= 64
+	return __best_mask_cpu(cpu, cpumask, per_cpu(sched_cpu_affinity_masks, cpu));
+#else
+	return cpumask_test_cpu(cpu, cpumask) ? cpu:
+		__best_mask_cpu(cpu, cpumask, per_cpu(sched_cpu_affinity_masks, cpu) + 1);
+#endif
 }
 
 extern void flush_smp_call_function_from_idle(void);
-- 
2.29.2.260.ge31aba42fb


From 04ea7e2b45d67a5dd1788f94cde5b4c6b6f760f1 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Fri, 20 Nov 2020 10:26:13 +0800
Subject: [PATCH 7/7] Project-C v5.9-r2

---
 kernel/sched/alt_core.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 3a4281ba65e6..e485c76b1668 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -52,7 +52,7 @@
  */
 EXPORT_TRACEPOINT_SYMBOL_GPL(pelt_irq_tp);
 
-#define ALT_SCHED_VERSION "v5.9-r1"
+#define ALT_SCHED_VERSION "v5.9-r2"
 
 /* rt_prio(prio) defined in include/linux/sched/rt.h */
 #define rt_task(p)		rt_prio((p)->prio)
-- 
2.29.2.260.ge31aba42fb

