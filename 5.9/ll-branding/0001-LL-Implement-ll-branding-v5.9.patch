From dfd01c1b545534f3c591988e235596e749fa0574 Mon Sep 17 00:00:00 2001
From: Piotr Gorski <lucjan.lucjanov@gmail.com>
Date: Mon, 12 Oct 2020 11:47:18 +0200
Subject: [PATCH] LL: Implement ll-branding v5.9

Signed-off-by: Piotr Gorski <lucjan.lucjanov@gmail.com>
---
 block/elevator.c                   | 6 +++++-
 drivers/cpufreq/cpufreq_ondemand.c | 9 +++++++++
 fs/dcache.c                        | 4 ++++
 include/linux/pagemap.h            | 4 ++++
 init/Kconfig                       | 4 ++++
 kernel/sched/alt_core.c            | 4 ++++
 kernel/sched/core.c                | 8 ++++++++
 mm/huge_memory.c                   | 4 ++++
 mm/page-writeback.c                | 8 ++++++++
 mm/vmscan.c                        | 4 ++++
 10 files changed, 54 insertions(+), 1 deletion(-)

diff --git a/block/elevator.c b/block/elevator.c
index eacf7d559..4b8ff55c9 100644
--- a/block/elevator.c
+++ b/block/elevator.c
@@ -628,10 +628,14 @@ static inline bool elv_support_iosched(struct request_queue *q)
  */
 static struct elevator_type *elevator_get_default(struct request_queue *q)
 {
+#ifndef CONFIG_LL_BRANDING
 	if (q->nr_hw_queues != 1)
 		return NULL;
+#endif
 
-#if defined(CONFIG_MQ_IOSCHED_DEADLINE_NODEFAULT)
+#if defined(CONFIG_LL_BRANDING) && defined(CONFIG_IOSCHED_BFQ)
+	return elevator_get(q, "bfq", false);
+#elif defined(CONFIG_MQ_IOSCHED_DEADLINE_NODEFAULT)
 	return elevator_get(q, "mq-deadline-nodefault", false);
 #else
 	return elevator_get(q, "mq-deadline", false);
diff --git a/drivers/cpufreq/cpufreq_ondemand.c b/drivers/cpufreq/cpufreq_ondemand.c
index ac361a8b1..063364e8c 100644
--- a/drivers/cpufreq/cpufreq_ondemand.c
+++ b/drivers/cpufreq/cpufreq_ondemand.c
@@ -18,10 +18,19 @@
 #include "cpufreq_ondemand.h"
 
 /* On-demand governor macros */
+#if defined(CONFIG_LL_BRANDING) && defined(CONFIG_SCHED_ALT)
+#define DEF_FREQUENCY_UP_THRESHOLD		(55)
+#define DEF_SAMPLING_DOWN_FACTOR		(5)
+#else
 #define DEF_FREQUENCY_UP_THRESHOLD		(80)
 #define DEF_SAMPLING_DOWN_FACTOR		(1)
+#endif
 #define MAX_SAMPLING_DOWN_FACTOR		(100000)
+#if defined(CONFIG_LL_BRANDING) && defined(CONFIG_SCHED_ALT)
+#define MICRO_FREQUENCY_UP_THRESHOLD		(63)
+#else
 #define MICRO_FREQUENCY_UP_THRESHOLD		(95)
+#endif
 #define MICRO_FREQUENCY_MIN_SAMPLE_RATE		(10000)
 #define MIN_FREQUENCY_UP_THRESHOLD		(1)
 #define MAX_FREQUENCY_UP_THRESHOLD		(100)
diff --git a/fs/dcache.c b/fs/dcache.c
index ea0485861..a1efbdfd0 100644
--- a/fs/dcache.c
+++ b/fs/dcache.c
@@ -71,7 +71,11 @@
  * If no ancestor relationship:
  * arbitrary, since it's serialized on rename_lock
  */
+#ifdef CONFIG_LL_BRANDING
+int sysctl_vfs_cache_pressure __read_mostly = 50;
+#else
 int sysctl_vfs_cache_pressure __read_mostly = 100;
+#endif
 EXPORT_SYMBOL_GPL(sysctl_vfs_cache_pressure);
 
 __cacheline_aligned_in_smp DEFINE_SEQLOCK(rename_lock);
diff --git a/include/linux/pagemap.h b/include/linux/pagemap.h
index f1229e15e..9f8e8d04f 100644
--- a/include/linux/pagemap.h
+++ b/include/linux/pagemap.h
@@ -699,7 +699,11 @@ int replace_page_cache_page(struct page *old, struct page *new, gfp_t gfp_mask);
 void delete_from_page_cache_batch(struct address_space *mapping,
 				  struct pagevec *pvec);
 
+#ifdef CONFIG_LL_BRANDING
+#define VM_READAHEAD_PAGES	(SZ_2M / PAGE_SIZE)
+#else
 #define VM_READAHEAD_PAGES	(SZ_128K / PAGE_SIZE)
+#endif
 
 void page_cache_sync_readahead(struct address_space *, struct file_ra_state *,
 		struct file *, pgoff_t index, unsigned long req_count);
diff --git a/init/Kconfig b/init/Kconfig
index 0b4c33711..4739f762b 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -92,6 +92,10 @@ config THREAD_INFO_IN_TASK
 
 menu "General setup"
 
+config LL_BRANDING
+	bool "Add Linux Lucjan branding"
+	default y
+
 config BROKEN
 	bool
 
diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 1525bb21e..039f7314e 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -85,7 +85,11 @@ early_param("sched_timeslice", sched_timeslice);
  * 1: Deboost and requeue task. (default)
  * 2: Set rq skip task.
  */
+#ifdef CONFIG_LL_BRANDING
+int sched_yield_type __read_mostly = 0;
+#else
 int sched_yield_type __read_mostly = 1;
+#endif
 
 #ifdef CONFIG_SMP
 static cpumask_t sched_rq_pending_mask ____cacheline_aligned_in_smp;
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 8f822e49c..43d5f360d 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -63,7 +63,11 @@ const_debug unsigned int sysctl_sched_features =
  * Number of tasks to iterate in a single balance run.
  * Limited because this is done with IRQs disabled.
  */
+#ifdef CONFIG_LL_BRANDING
+const_debug unsigned int sysctl_sched_nr_migrate = 256;
+#else
 const_debug unsigned int sysctl_sched_nr_migrate = 32;
+#endif
 
 /*
  * period over which we measure -rt task CPU usage in us.
@@ -77,7 +81,11 @@ __read_mostly int scheduler_running;
  * part of the period that we allow rt tasks to run in us.
  * default: 0.95s
  */
+#ifdef CONFIG_LL_BRANDING
+int sysctl_sched_rt_runtime = 980000;
+#else
 int sysctl_sched_rt_runtime = 950000;
+#endif
 
 
 /*
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 26d0a5da6..7bd6b7faa 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -53,7 +53,11 @@ unsigned long transparent_hugepage_flags __read_mostly =
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE_MADVISE
 	(1<<TRANSPARENT_HUGEPAGE_REQ_MADV_FLAG)|
 #endif
+#ifdef CONFIG_LL_BRANDING
+	(1<<TRANSPARENT_HUGEPAGE_DEFRAG_KSWAPD_OR_MADV_FLAG)|
+#else
 	(1<<TRANSPARENT_HUGEPAGE_DEFRAG_REQ_MADV_FLAG)|
+#endif
 	(1<<TRANSPARENT_HUGEPAGE_DEFRAG_KHUGEPAGED_FLAG)|
 	(1<<TRANSPARENT_HUGEPAGE_USE_ZERO_PAGE_FLAG);
 
diff --git a/mm/page-writeback.c b/mm/page-writeback.c
index 4e4ddd67b..8428062de 100644
--- a/mm/page-writeback.c
+++ b/mm/page-writeback.c
@@ -71,7 +71,11 @@ static long ratelimit_pages = 32;
 /*
  * Start background writeback (via writeback threads) at this percentage
  */
+#ifdef CONFIG_LL_BRANDING
+int dirty_background_ratio = 20;
+#else
 int dirty_background_ratio = 10;
+#endif
 
 /*
  * dirty_background_bytes starts at 0 (disabled) so that it is a function of
@@ -88,7 +92,11 @@ int vm_highmem_is_dirtyable;
 /*
  * The generator of dirty data starts writeback at this percentage
  */
+#ifdef CONFIG_LL_BRANDING
+int vm_dirty_ratio = 50;
+#else
 int vm_dirty_ratio = 20;
+#endif
 
 /*
  * vm_dirty_bytes starts at 0 (disabled) so that it is a function of
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 046381876..b42f5cb96 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -169,7 +169,11 @@ struct scan_control {
 /*
  * From 0 .. 200.  Higher means more swappy.
  */
+#ifdef CONFIG_LL_BRANDING
+int vm_swappiness = 30;
+#else
 int vm_swappiness = 60;
+#endif
 
 static void set_task_reclaim_state(struct task_struct *task,
 				   struct reclaim_state *rs)
-- 
2.29.0.rc0

