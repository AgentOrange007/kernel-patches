From 47fad5263c3b23cc25f60e7d0b28c4ab77b95c4c Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Wed, 25 Mar 2020 13:50:09 +0800
Subject: [PATCH 094/106] bmq: Rework sched_fork().

---
 kernel/sched/bmq.c | 27 +++++++++++++--------------
 1 file changed, 13 insertions(+), 14 deletions(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 1dd61cf08fc5..e545cf6460e4 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -2035,8 +2035,7 @@ static inline void __sched_fork(unsigned long clone_flags, struct task_struct *p
 int sched_fork(unsigned long clone_flags, struct task_struct *p)
 {
 	unsigned long flags;
-	int cpu = get_cpu();
-	struct rq *rq = this_rq();
+	struct rq *rq;
 
 	__sched_fork(clone_flags, p);
 	/*
@@ -2073,12 +2072,21 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 
 	p->boost_prio = (p->boost_prio < 0) ?
 		p->boost_prio + MAX_PRIORITY_ADJ : MAX_PRIORITY_ADJ;
+	/*
+	 * The child is not yet in the pid-hash so no cgroup attach races,
+	 * and the cgroup is pinned to this child due to cgroup_fork()
+	 * is ran before sched_fork().
+	 *
+	 * Silence PROVE_RCU.
+	 */
+	raw_spin_lock_irqsave(&p->pi_lock, flags);
 	/*
 	 * Share the timeslice between parent and child, thus the
 	 * total amount of pending timeslices in the system doesn't change,
 	 * resulting in more scheduling fairness.
 	 */
-	raw_spin_lock_irqsave(&rq->lock, flags);
+	rq = this_rq();
+	raw_spin_lock(&rq->lock);
 	rq->curr->time_slice /= 2;
 	p->time_slice = rq->curr->time_slice;
 #ifdef CONFIG_SCHED_HRTICK
@@ -2089,21 +2097,13 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 		p->time_slice = sched_timeslice_ns;
 		resched_curr(rq);
 	}
-	raw_spin_unlock_irqrestore(&rq->lock, flags);
+	raw_spin_unlock(&rq->lock);
 
-	/*
-	 * The child is not yet in the pid-hash so no cgroup attach races,
-	 * and the cgroup is pinned to this child due to cgroup_fork()
-	 * is ran before sched_fork().
-	 *
-	 * Silence PROVE_RCU.
-	 */
-	raw_spin_lock_irqsave(&p->pi_lock, flags);
 	/*
 	 * We're setting the CPU for the first time, we don't migrate,
 	 * so use __set_task_cpu().
 	 */
-	__set_task_cpu(p, cpu);
+	__set_task_cpu(p, cpu_of(rq));
 	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
 
 #ifdef CONFIG_SCHED_INFO
@@ -2112,7 +2112,6 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 #endif
 	init_task_preempt_count(p);
 
-	put_cpu();
 	return 0;
 }
 
-- 
2.27.0.rc2

