From a8b1163cb4b414aedc4675e33ffc6a3e0fd8ed16 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sat, 4 Apr 2020 09:14:17 +0800
Subject: [PATCH 096/112] bmq: Rework take_other_rq_tasks().

---
 kernel/sched/bmq.c | 58 ++++++++++++++++++++++++++++++++++------------
 1 file changed, 43 insertions(+), 15 deletions(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index ad0d073666ae..331ab35962e3 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -480,6 +480,16 @@ static inline void sched_update_tick_dependency(struct rq *rq) { }
  * Add/Remove/Requeue task to/from the runqueue routines
  * Context: rq->lock
  */
+static inline void __dequeue_task(struct task_struct *p, struct rq *rq, int flags)
+{
+	psi_dequeue(p, flags & DEQUEUE_SLEEP);
+	sched_info_dequeued(rq, p);
+
+	list_del(&p->bmq_node);
+	if (list_empty(&rq->queue.heads[p->bmq_idx]))
+		clear_bit(p->bmq_idx, rq->queue.bitmap);
+}
+
 static inline void dequeue_task(struct task_struct *p, struct rq *rq, int flags)
 {
 	lockdep_assert_held(&rq->lock);
@@ -487,6 +497,9 @@ static inline void dequeue_task(struct task_struct *p, struct rq *rq, int flags)
 	WARN_ONCE(task_rq(p) != rq, "bmq: dequeue task reside on cpu%d from cpu%d\n",
 		  task_cpu(p), cpu_of(rq));
 
+	psi_dequeue(p, flags & DEQUEUE_SLEEP);
+	sched_info_dequeued(rq, p);
+
 	list_del(&p->bmq_node);
 	if (list_empty(&rq->queue.heads[p->bmq_idx])) {
 		clear_bit(p->bmq_idx, rq->queue.bitmap);
@@ -499,9 +512,16 @@ static inline void dequeue_task(struct task_struct *p, struct rq *rq, int flags)
 #endif
 
 	sched_update_tick_dependency(rq);
-	psi_dequeue(p, flags & DEQUEUE_SLEEP);
+}
 
-	sched_info_dequeued(rq, p);
+static inline void __enqueue_task(struct task_struct *p, struct rq *rq, int flags)
+{
+	sched_info_queued(rq, p);
+	psi_enqueue(p, flags);
+
+	p->bmq_idx = task_sched_prio(p);
+	list_add_tail(&p->bmq_node, &rq->queue.heads[p->bmq_idx]);
+	set_bit(p->bmq_idx, rq->queue.bitmap);
 }
 
 static inline void enqueue_task(struct task_struct *p, struct rq *rq, int flags)
@@ -511,9 +531,7 @@ static inline void enqueue_task(struct task_struct *p, struct rq *rq, int flags)
 	WARN_ONCE(task_rq(p) != rq, "bmq: enqueue task reside on cpu%d to cpu%d\n",
 		  task_cpu(p), cpu_of(rq));
 
-	p->bmq_idx = task_sched_prio(p);
-	list_add_tail(&p->bmq_node, &rq->queue.heads[p->bmq_idx]);
-	set_bit(p->bmq_idx, rq->queue.bitmap);
+	__enqueue_task(p, rq, flags);
 	update_sched_rq_watermark(rq);
 	++rq->nr_running;
 #ifdef CONFIG_SMP
@@ -523,9 +541,6 @@ static inline void enqueue_task(struct task_struct *p, struct rq *rq, int flags)
 
 	sched_update_tick_dependency(rq);
 
-	sched_info_queued(rq, p);
-	psi_enqueue(p, flags);
-
 	/*
 	 * If in_iowait is set, the code below may not trigger any cpufreq
 	 * utilization updates, so do it here explicitly with the IOWAIT flag
@@ -3215,9 +3230,9 @@ migrate_pending_tasks(struct rq *rq, struct rq *dest_rq, const int dest_cpu)
 	       (p = rq_next_bmq_task(skip, rq)) != rq->idle) {
 		skip = rq_next_bmq_task(p, rq);
 		if (cpumask_test_cpu(dest_cpu, p->cpus_ptr)) {
-			dequeue_task(p, rq, 0);
+			__dequeue_task(p, rq, 0);
 			set_task_cpu(p, dest_cpu);
-			enqueue_task(p, dest_rq, 0);
+			__enqueue_task(p, dest_rq, 0);
 			nr_migrated++;
 		}
 		nr_tries--;
@@ -3250,15 +3265,28 @@ static inline int take_other_rq_tasks(struct rq *rq, int cpu)
 			spin_acquire(&src_rq->lock.dep_map,
 				     SINGLE_DEPTH_NESTING, 1, _RET_IP_);
 
-			nr_migrated = migrate_pending_tasks(src_rq, rq, cpu);
+			if ((nr_migrated = migrate_pending_tasks(src_rq, rq, cpu))) {
+				src_rq->nr_running -= nr_migrated;
+#ifdef CONFIG_SMP
+				if (src_rq->nr_running < 2)
+					cpumask_clear_cpu(i, &sched_rq_pending_mask);
+#endif
+				rq->nr_running += nr_migrated;
+#ifdef CONFIG_SMP
+				if (rq->nr_running > 1)
+					cpumask_set_cpu(cpu, &sched_rq_pending_mask);
+#endif
+				update_sched_rq_watermark(rq);
+				cpufreq_update_util(rq, 0);
 
-			spin_release(&src_rq->lock.dep_map, _RET_IP_);
-			do_raw_spin_unlock(&src_rq->lock);
+				spin_release(&src_rq->lock.dep_map, _RET_IP_);
+				do_raw_spin_unlock(&src_rq->lock);
 
-			if (nr_migrated) {
-				cpufreq_update_util(rq, 0);
 				return 1;
 			}
+
+			spin_release(&src_rq->lock.dep_map, _RET_IP_);
+			do_raw_spin_unlock(&src_rq->lock);
 		}
 	} while (++affinity_mask < end_mask);
 
-- 
2.27.0.112.g101b3204f3

