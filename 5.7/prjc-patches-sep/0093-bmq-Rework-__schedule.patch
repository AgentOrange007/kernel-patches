From e79f9c90f0621f656ceeae1af28e6fb6f532d325 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Wed, 25 Mar 2020 11:35:33 +0800
Subject: [PATCH 093/112] bmq: Rework __schedule().

---
 kernel/sched/bmq.c | 11 ++++++-----
 1 file changed, 6 insertions(+), 5 deletions(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 6efb8778ada7..1dd61cf08fc5 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -3273,7 +3273,7 @@ static inline int take_other_rq_tasks(struct rq *rq, int cpu)
  */
 static inline void check_curr(struct task_struct *p, struct rq *rq)
 {
-	if (rq->idle == p)
+	if (unlikely(rq->idle == p))
 		return;
 
 	update_curr(rq, p);
@@ -3413,7 +3413,7 @@ static void __sched notrace __schedule(bool preempt)
 		hrtick_start(rq, next->time_slice);
 #endif
 
-	if (prev != next) {
+	if (likely(prev != next)) {
 		next->last_ran = rq->clock_task;
 		rq->last_ts_switch = rq->clock;
 
@@ -3443,11 +3443,12 @@ static void __sched notrace __schedule(bool preempt)
 
 		/* Also unlocks the rq: */
 		rq = context_switch(rq, prev, next);
-#ifdef CONFIG_SCHED_SMT
-		sg_balance_check(rq);
-#endif
 	} else
 		raw_spin_unlock_irq(&rq->lock);
+
+#ifdef CONFIG_SCHED_SMT
+	sg_balance_check(rq);
+#endif
 }
 
 void __noreturn do_task_dead(void)
-- 
2.27.0.112.g101b3204f3

