From 064c770bb6c08b0995c7145e37ce1016b87cf0c6 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sun, 24 May 2020 22:34:07 +0800
Subject: [PATCH 110/112] bmq: Adapt to Project C infrastructure

---
 .../admin-guide/kernel-parameters.txt         |  11 +-
 include/linux/sched.h                         |  18 +-
 include/linux/sched/deadline.h                |  15 +-
 include/linux/sched/prio.h                    |   2 +-
 include/linux/sched/rt.h                      |   2 +-
 init/Kconfig                                  |  17 +-
 init/init_task.c                              |   6 +-
 kernel/cgroup/cpuset.c                        |   4 +-
 kernel/livepatch/transition.c                 |   2 +-
 kernel/locking/rtmutex.c                      |   4 +
 kernel/sched/Makefile                         |   4 +-
 kernel/sched/{bmq.c => alt_core.c}            | 166 +++++-------------
 kernel/sched/{bmq_debug.c => alt_debug.c}     |   4 +-
 kernel/sched/{bmq_sched.h => alt_sched.h}     |  22 +--
 kernel/sched/bmq.h                            |  14 ++
 kernel/sched/bmq_imp.h                        |  86 +++++++++
 kernel/sched/cpufreq_schedutil.c              |  10 +-
 kernel/sched/idle.c                           |   2 +-
 kernel/sched/pelt.c                           |   2 +-
 kernel/sched/pelt.h                           |   8 +-
 kernel/sched/sched.h                          |   6 +-
 kernel/sched/stats.c                          |   4 +-
 kernel/sched/topology.c                       |  12 +-
 kernel/sysctl.c                               |  10 +-
 kernel/time/hrtimer.c                         |   2 +
 kernel/time/posix-cpu-timers.c                |   6 +-
 kernel/trace/trace_selftest.c                 |   2 +-
 27 files changed, 235 insertions(+), 206 deletions(-)
 rename kernel/sched/{bmq.c => alt_core.c} (97%)
 rename kernel/sched/{bmq_debug.c => alt_debug.c} (91%)
 rename kernel/sched/{bmq_sched.h => alt_sched.h} (97%)
 create mode 100644 kernel/sched/bmq.h
 create mode 100644 kernel/sched/bmq_imp.h

diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt
index d629137ad529..eda08ad54201 100644
--- a/Documentation/admin-guide/kernel-parameters.txt
+++ b/Documentation/admin-guide/kernel-parameters.txt
@@ -438,11 +438,6 @@
 			embedded devices based on command line input.
 			See Documentation/block/cmdline-partition.rst
 
-	bmq.timeslice=	[KNL] Time slice in us for BMQ scheduler.
-			Format: <int> (must be >= 1000)
-			Default: 4000
-			See Documentation/scheduler/sched-BMQ.txt
-
 	boot_delay=	Milliseconds to delay each printk during boot.
 			Values larger than 10 seconds (10000) are changed to
 			no delay (0).
@@ -4450,6 +4445,12 @@
 
 	sbni=		[NET] Granch SBNI12 leased line adapter
 
+	sched_timeslice=
+			[KNL] Time slice in us for BMQ scheduler.
+			Format: <int> (must be >= 1000)
+			Default: 4000
+			See Documentation/scheduler/sched-BMQ.txt
+
 	sched_debug	[KNL] Enables verbose scheduler debug messages.
 
 	schedstats=	[KNL,X86] Enable or disable scheduled statistics.
diff --git a/include/linux/sched.h b/include/linux/sched.h
index dc8799c314c9..9393d324c946 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -652,10 +652,10 @@ struct task_struct {
 	unsigned int			flags;
 	unsigned int			ptrace;
 
-#if defined(CONFIG_SMP) && !defined(CONFIG_SCHED_BMQ)
+#if defined(CONFIG_SMP) && !defined(CONFIG_SCHED_ALT)
 	struct llist_node		wake_entry;
 #endif
-#if defined(CONFIG_SMP) || defined(CONFIG_SCHED_BMQ)
+#if defined(CONFIG_SMP) || defined(CONFIG_SCHED_ALT)
 	int				on_cpu;
 #endif
 #ifdef CONFIG_SMP
@@ -663,7 +663,7 @@ struct task_struct {
 	/* Current CPU: */
 	unsigned int			cpu;
 #endif
-#ifndef CONFIG_SCHED_BMQ
+#ifndef CONFIG_SCHED_ALT
 	unsigned int			wakee_flips;
 	unsigned long			wakee_flip_decay_ts;
 	struct task_struct		*last_wakee;
@@ -677,7 +677,7 @@ struct task_struct {
 	 */
 	int				recent_used_cpu;
 	int				wake_cpu;
-#endif /* !CONFIG_SCHED_BMQ */
+#endif /* !CONFIG_SCHED_ALT */
 #endif
 	int				on_rq;
 
@@ -686,15 +686,17 @@ struct task_struct {
 	int				normal_prio;
 	unsigned int			rt_priority;
 
-#ifdef CONFIG_SCHED_BMQ
+#ifdef CONFIG_SCHED_ALT
 	u64				last_ran;
 	s64				time_slice;
 	int				boost_prio;
+#ifdef CONFIG_SCHED_BMQ
 	int				bmq_idx;
 	struct list_head		bmq_node;
+#endif /* CONFIG_SCHED_BMQ */
 	/* sched_clock time spent running */
 	u64				sched_time;
-#else /* !CONFIG_SCHED_BMQ */
+#else /* !CONFIG_SCHED_ALT */
 	const struct sched_class	*sched_class;
 	struct sched_entity		se;
 	struct sched_rt_entity		rt;
@@ -1322,14 +1324,14 @@ struct task_struct {
 	 */
 };
 
-#ifdef CONFIG_SCHED_BMQ
+#ifdef CONFIG_SCHED_ALT
 #define tsk_seruntime(t)		((t)->sched_time)
 /* replace the uncertian rt_timeout with 0UL */
 #define tsk_rttimeout(t)		(0UL)
 #else /* CFS */
 #define tsk_seruntime(t)	((t)->se.sum_exec_runtime)
 #define tsk_rttimeout(t)	((t)->rt.timeout)
-#endif /* !CONFIG_SCHED_BMQ */
+#endif /* !CONFIG_SCHED_ALT */
 
 static inline struct pid *task_pid(struct task_struct *task)
 {
diff --git a/include/linux/sched/deadline.h b/include/linux/sched/deadline.h
index 02a3c5d34ee4..babbd495ce81 100644
--- a/include/linux/sched/deadline.h
+++ b/include/linux/sched/deadline.h
@@ -1,18 +1,11 @@
 /* SPDX-License-Identifier: GPL-2.0 */
 
-#ifdef CONFIG_SCHED_BMQ
+#ifdef CONFIG_SCHED_ALT
 
+#ifdef CONFIG_SCHED_BMQ
 #define __tsk_deadline(p)	(0UL)
+#endif
 
-static inline int dl_prio(int prio)
-{
-	return 0;
-}
-
-static inline int dl_task(struct task_struct *p)
-{
-	return (SCHED_NORMAL == p->policy);
-}
 #else
 
 #define __tsk_deadline(p)	((p)->dl.deadline)
@@ -36,7 +29,7 @@ static inline int dl_task(struct task_struct *p)
 {
 	return dl_prio(p->prio);
 }
-#endif /* CONFIG_SCHED_BMQ */
+#endif /* CONFIG_SCHED_ALT */
 
 static inline bool dl_time_before(u64 a, u64 b)
 {
diff --git a/include/linux/sched/prio.h b/include/linux/sched/prio.h
index d9dc5d3ccd2e..ba6fd6a5b4b1 100644
--- a/include/linux/sched/prio.h
+++ b/include/linux/sched/prio.h
@@ -26,7 +26,7 @@
 #define MAX_PRIO		(MAX_RT_PRIO + NICE_WIDTH)
 #define DEFAULT_PRIO		(MAX_RT_PRIO + NICE_WIDTH / 2)
 
-#ifdef CONFIG_SCHED_BMQ
+#ifdef CONFIG_SCHED_ALT
 /* +/- priority levels from the base priority */
 #define MAX_PRIORITY_ADJ	4
 #endif
diff --git a/include/linux/sched/rt.h b/include/linux/sched/rt.h
index 6387c8ea9832..0a7565d0d3cf 100644
--- a/include/linux/sched/rt.h
+++ b/include/linux/sched/rt.h
@@ -24,7 +24,7 @@ static inline bool task_is_realtime(struct task_struct *tsk)
 
 	if (policy == SCHED_FIFO || policy == SCHED_RR)
 		return true;
-#ifndef CONFIG_SCHED_BMQ
+#ifndef CONFIG_SCHED_ALT
 	if (policy == SCHED_DEADLINE)
 		return true;
 #endif
diff --git a/init/Kconfig b/init/Kconfig
index 850f730faef5..4ef358fc7b51 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -689,6 +689,18 @@ config GENERIC_SCHED_CLOCK
 
 menu "Scheduler features"
 
+menuconfig SCHED_ALT
+	bool "Alternative CPU Schedulers"
+	default y
+	help
+	  This feature enable alternative CPU scheduler"
+
+if SCHED_ALT
+
+choice
+	prompt "Alternative CPU Scheduler"
+	default SCHED_BMQ
+
 config SCHED_BMQ
 	bool "BMQ CPU scheduler"
 	help
@@ -696,8 +708,9 @@ config SCHED_BMQ
 	  responsiveness on the desktop and solid scalability on normal
 	  hardware and commodity servers.
 
-          Say Y here.
-	default y
+endchoice
+
+endif
 
 config UCLAMP_TASK
 	bool "Enable utilization clamping for RT/FAIR tasks"
diff --git a/init/init_task.c b/init/init_task.c
index 530a8cfc2c43..737a814482d6 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -67,7 +67,7 @@ struct task_struct init_task
 	.stack		= init_stack,
 	.usage		= REFCOUNT_INIT(2),
 	.flags		= PF_KTHREAD,
-#ifdef CONFIG_SCHED_BMQ
+#ifdef CONFIG_SCHED_ALT
 	.prio		= DEFAULT_PRIO + MAX_PRIORITY_ADJ,
 	.static_prio	= DEFAULT_PRIO,
 	.normal_prio	= DEFAULT_PRIO + MAX_PRIORITY_ADJ,
@@ -85,10 +85,12 @@ struct task_struct init_task
 	.restart_block	= {
 		.fn = do_no_restart_syscall,
 	},
-#ifdef CONFIG_SCHED_BMQ
+#ifdef CONFIG_SCHED_ALT
 	.boost_prio	= 0,
+#ifdef CONFIG_SCHED_BMQ
 	.bmq_idx	= 15,
 	.bmq_node	= LIST_HEAD_INIT(init_task.bmq_node),
+#endif
 	.time_slice	= HZ,
 #else
 	.se		= {
diff --git a/kernel/cgroup/cpuset.c b/kernel/cgroup/cpuset.c
index 88a05ddd5527..1e3dac9b6a43 100644
--- a/kernel/cgroup/cpuset.c
+++ b/kernel/cgroup/cpuset.c
@@ -636,7 +636,7 @@ static int validate_change(struct cpuset *cur, struct cpuset *trial)
 	return ret;
 }
 
-#if defined(CONFIG_SMP) && !defined(CONFIG_SCHED_BMQ)
+#if defined(CONFIG_SMP) && !defined(CONFIG_SCHED_ALT)
 /*
  * Helper routine for generate_sched_domains().
  * Do cpusets a, b have overlapping effective cpus_allowed masks?
@@ -1009,7 +1009,7 @@ static void rebuild_sched_domains_locked(void)
 	/* Have scheduler rebuild the domains */
 	partition_and_rebuild_sched_domains(ndoms, doms, attr);
 }
-#else /* !CONFIG_SMP || CONFIG_SCHED_BMQ */
+#else /* !CONFIG_SMP || CONFIG_SCHED_ALT */
 static void rebuild_sched_domains_locked(void)
 {
 }
diff --git a/kernel/livepatch/transition.c b/kernel/livepatch/transition.c
index 3ad290e9fed8..4176ad070bc9 100644
--- a/kernel/livepatch/transition.c
+++ b/kernel/livepatch/transition.c
@@ -306,7 +306,7 @@ static bool klp_try_switch_task(struct task_struct *task)
 	 */
 	rq = task_rq_lock(task, &flags);
 
-#ifdef	CONFIG_SCHED_BMQ
+#ifdef	CONFIG_SCHED_ALT
 	if (task_running(task) && task != current) {
 #else
 	if (task_running(rq, task) && task != current) {
diff --git a/kernel/locking/rtmutex.c b/kernel/locking/rtmutex.c
index 063d15a1ab8b..b5d0c7088021 100644
--- a/kernel/locking/rtmutex.c
+++ b/kernel/locking/rtmutex.c
@@ -238,6 +238,7 @@ rt_mutex_waiter_less(struct rt_mutex_waiter *left,
 	if (left->prio < right->prio)
 		return 1;
 
+#ifndef CONFIG_SCHED_BMQ
 	/*
 	 * If both waiters have dl_prio(), we check the deadlines of the
 	 * associated tasks.
@@ -246,6 +247,7 @@ rt_mutex_waiter_less(struct rt_mutex_waiter *left,
 	 */
 	if (dl_prio(left->prio))
 		return dl_time_before(left->deadline, right->deadline);
+#endif
 
 	return 0;
 }
@@ -257,6 +259,7 @@ rt_mutex_waiter_equal(struct rt_mutex_waiter *left,
 	if (left->prio != right->prio)
 		return 0;
 
+#ifndef CONFIG_SCHED_BMQ
 	/*
 	 * If both waiters have dl_prio(), we check the deadlines of the
 	 * associated tasks.
@@ -265,6 +268,7 @@ rt_mutex_waiter_equal(struct rt_mutex_waiter *left,
 	 */
 	if (dl_prio(left->prio))
 		return left->deadline == right->deadline;
+#endif
 
 	return 1;
 }
diff --git a/kernel/sched/Makefile b/kernel/sched/Makefile
index ac31239aa51a..1cad9ff599a4 100644
--- a/kernel/sched/Makefile
+++ b/kernel/sched/Makefile
@@ -16,8 +16,8 @@ ifneq ($(CONFIG_SCHED_OMIT_FRAME_POINTER),y)
 CFLAGS_core.o := $(PROFILING) -fno-omit-frame-pointer
 endif
 
-ifdef CONFIG_SCHED_BMQ
-obj-y += bmq.o bmq_debug.o
+ifdef CONFIG_SCHED_ALT
+obj-y += alt_core.o alt_debug.o
 else
 obj-y += core.o
 obj-y += fair.o rt.o deadline.o
diff --git a/kernel/sched/bmq.c b/kernel/sched/alt_core.c
similarity index 97%
rename from kernel/sched/bmq.c
rename to kernel/sched/alt_core.c
index 8e801de6551d..9b84fbc2237d 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/alt_core.c
@@ -1,7 +1,7 @@
 /*
- *  kernel/sched/bmq.c
+ *  kernel/sched/alt_core.c
  *
- *  BMQ Core kernel scheduler code and related syscalls
+ *  Core alternative kernel scheduler code and related syscalls
  *
  *  Copyright (C) 1991-2002  Linus Torvalds
  *
@@ -11,7 +11,7 @@
  *		scheduler by Alfred Chen.
  *  2019-02-20	BMQ(BitMap Queue) kernel scheduler by Alfred Chen.
  */
-#include "bmq_sched.h"
+#include "sched.h"
 
 #include <linux/sched/rt.h>
 
@@ -50,7 +50,7 @@
 
 #define STOP_PRIO		(MAX_RT_PRIO - 1)
 
-/* Default time slice is 4 in ms, can be set via kernel parameter "bmq.timeslice" */
+/* Default time slice is 4 in ms, can be set via kernel parameter "sched_timeslice" */
 u64 sched_timeslice_ns __read_mostly = (4 * 1000 * 1000);
 
 static int __init sched_timeslice(char *str)
@@ -63,16 +63,11 @@ static int __init sched_timeslice(char *str)
 
 	return 0;
 }
-early_param("bmq.timeslice", sched_timeslice);
+early_param("sched_timeslice", sched_timeslice);
 
 /* Reschedule if less than this many μs left */
 #define RESCHED_NS		(100 * 1000)
 
-static inline void print_scheduler_version(void)
-{
-	printk(KERN_INFO "bmq: BMQ CPU Scheduler 5.7-r1 by Alfred Chen.\n");
-}
-
 /**
  * sched_yield_type - Choose what sort of yield sched_yield will perform.
  * 0: No yield.
@@ -145,11 +140,11 @@ DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);
 #define IDLE_WM	(IDLE_TASK_SCHED_PRIO)
 
 static cpumask_t sched_sg_idle_mask ____cacheline_aligned_in_smp;
-static cpumask_t sched_rq_watermark[bmq_BITS] ____cacheline_aligned_in_smp;
+static cpumask_t sched_rq_watermark[SCHED_BITS] ____cacheline_aligned_in_smp;
 
 static inline void update_sched_rq_watermark(struct rq *rq)
 {
-	unsigned long watermark = find_first_bit(rq->queue.bitmap, bmq_BITS);
+	unsigned long watermark = find_first_bit(rq->queue.bitmap, SCHED_BITS);
 	unsigned long last_wm = rq->watermark;
 	unsigned long i;
 	int cpu;
@@ -193,55 +188,14 @@ static inline int task_sched_prio(struct task_struct *p)
 	return (p->prio < MAX_RT_PRIO)? p->prio : p->prio + p->boost_prio;
 }
 
-static inline void bmq_init(struct bmq *q)
-{
-	int i;
-
-	bitmap_zero(q->bitmap, bmq_BITS);
-	for(i = 0; i < bmq_BITS; i++)
-		INIT_LIST_HEAD(&q->heads[i]);
-}
-
-static inline void bmq_init_idle(struct bmq *q, struct task_struct *idle)
-{
-	INIT_LIST_HEAD(&q->heads[IDLE_TASK_SCHED_PRIO]);
-	list_add(&idle->bmq_node, &q->heads[IDLE_TASK_SCHED_PRIO]);
-	set_bit(IDLE_TASK_SCHED_PRIO, q->bitmap);
-}
-
-/*
- * This routine used in bmq scheduler only which assume the idle task in the bmq
- */
-static inline struct task_struct *rq_first_bmq_task(struct rq *rq)
-{
-	unsigned long idx = find_first_bit(rq->queue.bitmap, bmq_BITS);
-	const struct list_head *head = &rq->queue.heads[idx];
-
-	return list_first_entry(head, struct task_struct, bmq_node);
-}
-
-static inline struct task_struct *
-rq_next_bmq_task(struct task_struct *p, struct rq *rq)
-{
-	unsigned long idx = p->bmq_idx;
-	struct list_head *head = &rq->queue.heads[idx];
-
-	if (list_is_last(&p->bmq_node, head)) {
-		idx = find_next_bit(rq->queue.bitmap, bmq_BITS, idx + 1);
-		head = &rq->queue.heads[idx];
-
-		return list_first_entry(head, struct task_struct, bmq_node);
-	}
-
-	return list_next_entry(p, bmq_node);
-}
+#include "bmq_imp.h"
 
 static inline struct task_struct *rq_runnable_task(struct rq *rq)
 {
-	struct task_struct *next = rq_first_bmq_task(rq);
+	struct task_struct *next = sched_rq_first_task(rq);
 
 	if (unlikely(next == rq->skip))
-		next = rq_next_bmq_task(next, rq);
+		next = sched_rq_next_task(next, rq);
 
 	return next;
 }
@@ -480,31 +434,14 @@ static inline void sched_update_tick_dependency(struct rq *rq) { }
  * Add/Remove/Requeue task to/from the runqueue routines
  * Context: rq->lock
  */
-static inline void __dequeue_task(struct task_struct *p, struct rq *rq, int flags)
-{
-	psi_dequeue(p, flags & DEQUEUE_SLEEP);
-	sched_info_dequeued(rq, p);
-
-	list_del(&p->bmq_node);
-	if (list_empty(&rq->queue.heads[p->bmq_idx]))
-		clear_bit(p->bmq_idx, rq->queue.bitmap);
-}
-
 static inline void dequeue_task(struct task_struct *p, struct rq *rq, int flags)
 {
 	lockdep_assert_held(&rq->lock);
 
-	WARN_ONCE(task_rq(p) != rq, "bmq: dequeue task reside on cpu%d from cpu%d\n",
+	WARN_ONCE(task_rq(p) != rq, "sched: dequeue task reside on cpu%d from cpu%d\n",
 		  task_cpu(p), cpu_of(rq));
 
-	psi_dequeue(p, flags & DEQUEUE_SLEEP);
-	sched_info_dequeued(rq, p);
-
-	list_del(&p->bmq_node);
-	if (list_empty(&rq->queue.heads[p->bmq_idx])) {
-		clear_bit(p->bmq_idx, rq->queue.bitmap);
-		update_sched_rq_watermark(rq);
-	}
+	__SCHED_DEQUEUE_TASK(p, rq, flags, update_sched_rq_watermark(rq));
 	--rq->nr_running;
 #ifdef CONFIG_SMP
 	if (1 == rq->nr_running)
@@ -514,24 +451,14 @@ static inline void dequeue_task(struct task_struct *p, struct rq *rq, int flags)
 	sched_update_tick_dependency(rq);
 }
 
-static inline void __enqueue_task(struct task_struct *p, struct rq *rq, int flags)
-{
-	sched_info_queued(rq, p);
-	psi_enqueue(p, flags);
-
-	p->bmq_idx = task_sched_prio(p);
-	list_add_tail(&p->bmq_node, &rq->queue.heads[p->bmq_idx]);
-	set_bit(p->bmq_idx, rq->queue.bitmap);
-}
-
 static inline void enqueue_task(struct task_struct *p, struct rq *rq, int flags)
 {
 	lockdep_assert_held(&rq->lock);
 
-	WARN_ONCE(task_rq(p) != rq, "bmq: enqueue task reside on cpu%d to cpu%d\n",
+	WARN_ONCE(task_rq(p) != rq, "sched: enqueue task reside on cpu%d to cpu%d\n",
 		  task_cpu(p), cpu_of(rq));
 
-	__enqueue_task(p, rq, flags);
+	__SCHED_ENQUEUE_TASK(p, rq, flags);
 	update_sched_rq_watermark(rq);
 	++rq->nr_running;
 #ifdef CONFIG_SMP
@@ -552,21 +479,11 @@ static inline void enqueue_task(struct task_struct *p, struct rq *rq, int flags)
 
 static inline void requeue_task(struct task_struct *p, struct rq *rq)
 {
-	int idx = task_sched_prio(p);
-
 	lockdep_assert_held(&rq->lock);
-	WARN_ONCE(task_rq(p) != rq, "bmq: cpu[%d] requeue task reside on cpu%d\n",
+	WARN_ONCE(task_rq(p) != rq, "sched: cpu[%d] requeue task reside on cpu%d\n",
 		  cpu_of(rq), task_cpu(p));
 
-	list_del(&p->bmq_node);
-	list_add_tail(&p->bmq_node, &rq->queue.heads[idx]);
-	if (idx != p->bmq_idx) {
-		if (list_empty(&rq->queue.heads[p->bmq_idx]))
-			clear_bit(p->bmq_idx, rq->queue.bitmap);
-		p->bmq_idx = idx;
-		set_bit(p->bmq_idx, rq->queue.bitmap);
-		update_sched_rq_watermark(rq);
-	}
+	__requeue_task(p, rq);
 }
 
 /*
@@ -858,7 +775,7 @@ void wake_up_nohz_cpu(int cpu)
 
 static inline void check_preempt_curr(struct rq *rq)
 {
-	if (rq_first_bmq_task(rq) != rq->curr)
+	if (sched_rq_first_task(rq) != rq->curr)
 		resched_curr(rq);
 }
 
@@ -901,7 +818,7 @@ static enum hrtimer_restart hrtick(struct hrtimer *timer)
 static inline int hrtick_enabled(struct rq *rq)
 {
 	/**
-	 * BMQ doesn't support sched_feat yet
+	 * Alt schedule FW doesn't support sched_feat yet
 	if (!sched_feat(HRTICK))
 		return 0;
 	*/
@@ -1643,7 +1560,7 @@ ttwu_stat(struct task_struct *p, int cpu, int wake_flags)
 	if (cpu == rq->cpu)
 		__schedstat_inc(rq->ttwu_local);
 	else {
-		/** BMQ ToDo:
+		/** Alt schedule FW ToDo:
 		 * How to do ttwu_wake_remote
 		 */
 	}
@@ -3233,12 +3150,12 @@ migrate_pending_tasks(struct rq *rq, struct rq *dest_rq, const int dest_cpu)
 	int nr_tries = min(rq->nr_running / 2, SCHED_RQ_NR_MIGRATION);
 
 	while (skip != rq->idle && nr_tries &&
-	       (p = rq_next_bmq_task(skip, rq)) != rq->idle) {
-		skip = rq_next_bmq_task(p, rq);
+	       (p = sched_rq_next_task(skip, rq)) != rq->idle) {
+		skip = sched_rq_next_task(p, rq);
 		if (cpumask_test_cpu(dest_cpu, p->cpus_ptr)) {
-			__dequeue_task(p, rq, 0);
+			__SCHED_DEQUEUE_TASK(p, rq, 0, );
 			set_task_cpu(p, dest_cpu);
-			__enqueue_task(p, dest_rq, 0);
+			__SCHED_ENQUEUE_TASK(p, dest_rq, 0);
 			nr_migrated++;
 		}
 		nr_tries--;
@@ -3347,7 +3264,7 @@ choose_next_task(struct rq *rq, int cpu, struct task_struct *prev)
 		return next;
 	}
 
-	next = rq_first_bmq_task(rq);
+	next = sched_rq_first_task(rq);
 	if (next == rq->idle) {
 #ifdef	CONFIG_SMP
 		if (!take_other_rq_tasks(rq, cpu)) {
@@ -3356,7 +3273,7 @@ choose_next_task(struct rq *rq, int cpu, struct task_struct *prev)
 			return next;
 #ifdef	CONFIG_SMP
 		}
-		next = rq_first_bmq_task(rq);
+		next = sched_rq_first_task(rq);
 #endif
 	}
 #ifdef CONFIG_HIGH_RES_TIMERS
@@ -3417,7 +3334,7 @@ static void __sched notrace __schedule(bool preempt)
 
 	schedule_debug(prev, preempt);
 
-	/* by passing sched_feat(HRTICK) checking which BMQ doesn't support */
+	/* by passing sched_feat(HRTICK) checking which Alt schedule FW doesn't support */
 	hrtick_clear(rq);
 
 	local_irq_disable();
@@ -3767,7 +3684,7 @@ EXPORT_SYMBOL(default_wake_function);
 static inline void check_task_changed(struct rq *rq, struct task_struct *p)
 {
 	/* Trigger resched if task sched_prio has been modified. */
-	if (task_on_rq_queued(p) && task_sched_prio(p) != p->bmq_idx) {
+	if (task_on_rq_queued(p) && sched_task_need_requeue(p)) {
 		requeue_task(p, rq);
 		check_preempt_curr(rq);
 	}
@@ -4083,7 +4000,7 @@ static int __sched_setscheduler(struct task_struct *p,
 	BUG_ON(pi && in_interrupt());
 
 	/*
-	 * BMQ supports SCHED_DEADLINE by squash it as prio 0 SCHED_FIFO
+	 * Alt schedule FW supports SCHED_DEADLINE by squash it as prio 0 SCHED_FIFO
 	 */
 	if (unlikely(SCHED_DEADLINE == policy)) {
 		attr = &dl_squash_attr;
@@ -4908,7 +4825,7 @@ EXPORT_SYMBOL(yield);
  * It's the caller's job to ensure that the target task struct
  * can't go away on us before we can do any checks.
  *
- * In BMQ, yield_to is not supported.
+ * In Alt schedule FW, yield_to is not supported.
  *
  * Return:
  *	true (>0) if we indeed boosted the target task.
@@ -5159,7 +5076,7 @@ void show_state_filter(unsigned long state_filter)
 	}
 
 #ifdef CONFIG_SCHED_DEBUG
-	/* TODO: BMQ should support this
+	/* TODO: Alt schedule FW should support this
 	if (!state_filter)
 		sysrq_sched_debug_show();
 	*/
@@ -5200,8 +5117,7 @@ void init_idle(struct task_struct *idle, int cpu)
 	idle->last_ran = rq->clock_task;
 	idle->state = TASK_RUNNING;
 	idle->flags |= PF_IDLE;
-	idle->bmq_idx = IDLE_TASK_SCHED_PRIO;
-	bmq_init_idle(&rq->queue, idle);
+	sched_queue_init_idle(rq, idle);
 
 	kasan_unpoison_task_stack(idle);
 
@@ -5311,13 +5227,13 @@ static void migrate_tasks(struct rq *dead_rq)
 	 */
 	rq->stop = NULL;
 
-	p = rq_first_bmq_task(rq);
+	p = sched_rq_first_task(rq);
 	while (p != rq->idle) {
 		int dest_cpu;
 
 		/* skip the running task */
 		if (task_running(p) || 1 == p->nr_cpus_allowed) {
-			p = rq_next_bmq_task(p, rq);
+			p = sched_rq_next_task(p, rq);
 			continue;
 		}
 
@@ -5341,7 +5257,7 @@ static void migrate_tasks(struct rq *dead_rq)
 		 */
 		if (WARN_ON(task_rq(p) != rq || !task_on_rq_queued(p))) {
 			raw_spin_unlock(&p->pi_lock);
-			p = rq_next_bmq_task(p, rq);
+			p = sched_rq_next_task(p, rq);
 			continue;
 		}
 
@@ -5355,7 +5271,7 @@ static void migrate_tasks(struct rq *dead_rq)
 		rq = dead_rq;
 		raw_spin_lock(&rq->lock);
 		/* Check queued task all over from the header again */
-		p = rq_first_bmq_task(rq);
+		p = sched_rq_first_task(rq);
 	}
 
 	rq->stop = stop;
@@ -5543,7 +5459,7 @@ static void sched_init_topology_cpumask_early(void)
 
 #define TOPOLOGY_CPUMASK(name, mask, last) \
 	if (cpumask_and(chk, chk, mask))					\
-		printk(KERN_INFO "bmq: cpu#%02d affinity mask: 0x%08lx - "#name,\
+		printk(KERN_INFO "sched: cpu#%02d affinity mask: 0x%08lx - "#name,\
 		       cpu, (chk++)->bits[0]);					\
 	if (!last)								\
 		cpumask_complement(chk, mask)
@@ -5572,7 +5488,7 @@ static void sched_init_topology_cpumask(void)
 		TOPOLOGY_CPUMASK(others, cpu_online_mask, true);
 
 		per_cpu(sched_cpu_affinity_end_mask, cpu) = chk;
-		printk(KERN_INFO "bmq: cpu#%02d llc_id = %d, llc_mask idx = %d\n",
+		printk(KERN_INFO "sched: cpu#%02d llc_id = %d, llc_mask idx = %d\n",
 		       cpu, per_cpu(sd_llc_id, cpu),
 		       (int) (per_cpu(sched_cpu_llc_mask, cpu) -
 			      &(per_cpu(sched_cpu_affinity_masks, cpu)[0])));
@@ -5633,12 +5549,12 @@ void __init sched_init(void)
 	int i;
 	struct rq *rq;
 
-	print_scheduler_version();
+	printk(KERN_INFO ALT_SCHED_VERSION_MSG);
 
 	wait_bit_init();
 
 #ifdef CONFIG_SMP
-	for (i = 0; i < bmq_BITS; i++)
+	for (i = 0; i < SCHED_BITS; i++)
 		cpumask_copy(&sched_rq_watermark[i], cpu_present_mask);
 #endif
 
@@ -5652,7 +5568,7 @@ void __init sched_init(void)
 	for_each_possible_cpu(i) {
 		rq = cpu_rq(i);
 
-		bmq_init(&rq->queue);
+		sched_queue_init(rq);
 		rq->watermark = IDLE_WM;
 		rq->skip = NULL;
 
@@ -5676,7 +5592,6 @@ void __init sched_init(void)
 	/* Set rq->online for cpu 0 */
 	cpu_rq(0)->online = true;
 #endif
-
 	/*
 	 * The boot idle thread does lazy MMU switching as well:
 	 */
@@ -5994,6 +5909,7 @@ static struct cftype cpu_legacy_files[] = {
 	{ }	/* Terminate */
 };
 
+
 static struct cftype cpu_files[] = {
 	{ }	/* terminate */
 };
diff --git a/kernel/sched/bmq_debug.c b/kernel/sched/alt_debug.c
similarity index 91%
rename from kernel/sched/bmq_debug.c
rename to kernel/sched/alt_debug.c
index 375a1a805d86..835e6bb98dda 100644
--- a/kernel/sched/bmq_debug.c
+++ b/kernel/sched/alt_debug.c
@@ -1,12 +1,12 @@
 /*
- * kernel/sched/bmq_debug.c
+ * kernel/sched/alt_debug.c
  *
  * Print the BMQ debugging details
  *
  * Author: Alfred Chen
  * Date  : 2020
  */
-#include "bmq_sched.h"
+#include "sched.h"
 
 /*
  * This allows printing both to /proc/sched_debug and
diff --git a/kernel/sched/bmq_sched.h b/kernel/sched/alt_sched.h
similarity index 97%
rename from kernel/sched/bmq_sched.h
rename to kernel/sched/alt_sched.h
index c0fae307869f..0936cf766514 100644
--- a/kernel/sched/bmq_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -1,5 +1,5 @@
-#ifndef BMQ_SCHED_H
-#define BMQ_SCHED_H
+#ifndef ALT_SCHED_H
+#define ALT_SCHED_H
 
 #include <linux/sched.h>
 
@@ -46,6 +46,10 @@
 
 #include "cpupri.h"
 
+#ifdef CONFIG_SCHED_BMQ
+#include "bmq.h"
+#endif
+
 /* task_struct::on_rq states: */
 #define TASK_ON_RQ_QUEUED	1
 #define TASK_ON_RQ_MIGRATING	2
@@ -67,16 +71,6 @@ static inline int task_on_rq_migrating(struct task_struct *p)
 #define WF_FORK		0x02		/* child wakeup after fork */
 #define WF_MIGRATED	0x04		/* internal use, task got migrated */
 
-/* bits:
- * RT(0-99), Low prio adj range, nice width, high prio adj range, cpu idle task */
-#define bmq_BITS	(MAX_RT_PRIO + NICE_WIDTH + 2 * MAX_PRIORITY_ADJ + 1)
-#define IDLE_TASK_SCHED_PRIO	(bmq_BITS - 1)
-
-struct bmq {
-	DECLARE_BITMAP(bitmap, bmq_BITS);
-	struct list_head heads[bmq_BITS];
-};
-
 /*
  * This is the main, per-CPU runqueue data structure.
  * This data should only be modified by the local cpu.
@@ -89,7 +83,9 @@ struct rq {
 	struct task_struct *idle, *stop, *skip;
 	struct mm_struct *prev_mm;
 
+#ifdef CONFIG_SCHED_BMQ
 	struct bmq queue;
+#endif
 	unsigned long watermark;
 
 	/* switch count */
@@ -522,4 +518,4 @@ static inline int sched_numa_find_closest(const struct cpumask *cpus, int cpu)
 void swake_up_all_locked(struct swait_queue_head *q);
 void __prepare_to_swait(struct swait_queue_head *q, struct swait_queue *wait);
 
-#endif /* BMQ_SCHED_H */
+#endif /* ALT_SCHED_H */
diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
new file mode 100644
index 000000000000..4ce30c30bd3e
--- /dev/null
+++ b/kernel/sched/bmq.h
@@ -0,0 +1,14 @@
+#ifndef BMQ_H
+#define BMQ_H
+
+/* bits:
+ * RT(0-99), Low prio adj range, nice width, high prio adj range, cpu idle task */
+#define SCHED_BITS	(MAX_RT_PRIO + NICE_WIDTH + 2 * MAX_PRIORITY_ADJ + 1)
+#define IDLE_TASK_SCHED_PRIO	(SCHED_BITS - 1)
+
+struct bmq {
+	DECLARE_BITMAP(bitmap, SCHED_BITS);
+	struct list_head heads[SCHED_BITS];
+};
+
+#endif
diff --git a/kernel/sched/bmq_imp.h b/kernel/sched/bmq_imp.h
new file mode 100644
index 000000000000..d6b10e485390
--- /dev/null
+++ b/kernel/sched/bmq_imp.h
@@ -0,0 +1,86 @@
+#define ALT_SCHED_VERSION_MSG "sched/bmq: BMQ CPU Scheduler 5.7-r1 by Alfred Chen.\n"
+
+static inline void sched_queue_init(struct rq *rq)
+{
+	struct bmq *q = &rq->queue;
+	int i;
+
+	bitmap_zero(q->bitmap, SCHED_BITS);
+	for(i = 0; i < SCHED_BITS; i++)
+		INIT_LIST_HEAD(&q->heads[i]);
+}
+
+static inline void sched_queue_init_idle(struct rq *rq, struct task_struct *idle)
+{
+	struct bmq *q = &rq->queue;
+
+	idle->bmq_idx = IDLE_TASK_SCHED_PRIO;
+	INIT_LIST_HEAD(&q->heads[idle->bmq_idx]);
+	list_add(&idle->bmq_node, &q->heads[idle->bmq_idx]);
+	set_bit(idle->bmq_idx, q->bitmap);
+}
+
+/*
+ * This routine used in bmq scheduler only which assume the idle task in the bmq
+ */
+static inline struct task_struct *sched_rq_first_task(struct rq *rq)
+{
+	unsigned long idx = find_first_bit(rq->queue.bitmap, SCHED_BITS);
+	const struct list_head *head = &rq->queue.heads[idx];
+
+	return list_first_entry(head, struct task_struct, bmq_node);
+}
+
+static inline struct task_struct *
+sched_rq_next_task(struct task_struct *p, struct rq *rq)
+{
+	unsigned long idx = p->bmq_idx;
+	struct list_head *head = &rq->queue.heads[idx];
+
+	if (list_is_last(&p->bmq_node, head)) {
+		idx = find_next_bit(rq->queue.bitmap, SCHED_BITS, idx + 1);
+		head = &rq->queue.heads[idx];
+
+		return list_first_entry(head, struct task_struct, bmq_node);
+	}
+
+	return list_next_entry(p, bmq_node);
+}
+
+#define __SCHED_DEQUEUE_TASK(p, rq, flags, func)	\
+	psi_dequeue(p, flags & DEQUEUE_SLEEP);		\
+	sched_info_dequeued(rq, p);			\
+							\
+	list_del(&p->bmq_node);				\
+	if (list_empty(&rq->queue.heads[p->bmq_idx])) {	\
+		clear_bit(p->bmq_idx, rq->queue.bitmap);\
+		func;					\
+	}
+
+#define __SCHED_ENQUEUE_TASK(p, rq, flags)				\
+	sched_info_queued(rq, p);					\
+	psi_enqueue(p, flags);						\
+									\
+	p->bmq_idx = task_sched_prio(p);				\
+	list_add_tail(&p->bmq_node, &rq->queue.heads[p->bmq_idx]);	\
+	set_bit(p->bmq_idx, rq->queue.bitmap)
+
+static inline void __requeue_task(struct task_struct *p, struct rq *rq)
+{
+	int idx = task_sched_prio(p);
+
+	list_del(&p->bmq_node);
+	list_add_tail(&p->bmq_node, &rq->queue.heads[idx]);
+	if (idx != p->bmq_idx) {
+		if (list_empty(&rq->queue.heads[p->bmq_idx]))
+			clear_bit(p->bmq_idx, rq->queue.bitmap);
+		p->bmq_idx = idx;
+		set_bit(p->bmq_idx, rq->queue.bitmap);
+		update_sched_rq_watermark(rq);
+	}
+}
+
+static inline bool sched_task_need_requeue(struct task_struct *p)
+{
+	return (task_sched_prio(p) != p->bmq_idx);
+}
diff --git a/kernel/sched/cpufreq_schedutil.c b/kernel/sched/cpufreq_schedutil.c
index af350d0afa56..0d7ad05b84fe 100644
--- a/kernel/sched/cpufreq_schedutil.c
+++ b/kernel/sched/cpufreq_schedutil.c
@@ -183,7 +183,7 @@ static unsigned int get_next_freq(struct sugov_policy *sg_policy,
 	return cpufreq_driver_resolve_freq(policy, freq);
 }
 
-#ifndef CONFIG_SCHED_BMQ
+#ifndef CONFIG_SCHED_ALT
 /*
  * This function computes an effective utilization for the given CPU, to be
  * used for frequency selection given the linear relation: f = u * f_max.
@@ -301,7 +301,7 @@ static unsigned long sugov_get_util(struct sugov_cpu *sg_cpu)
 
 	return schedutil_cpu_util(sg_cpu->cpu, util, max, FREQUENCY_UTIL, NULL);
 }
-#else /* CONFIG_SCHED_BMQ */
+#else /* CONFIG_SCHED_ALT */
 static unsigned long sugov_get_util(struct sugov_cpu *sg_cpu)
 {
 	sg_cpu->max = arch_scale_cpu_capacity(sg_cpu->cpu);
@@ -451,7 +451,7 @@ static inline bool sugov_cpu_is_busy(struct sugov_cpu *sg_cpu) { return false; }
  */
 static inline void ignore_dl_rate_limit(struct sugov_cpu *sg_cpu, struct sugov_policy *sg_policy)
 {
-#ifndef CONFIG_SCHED_BMQ
+#ifndef CONFIG_SCHED_ALT
 	if (cpu_bw_dl(cpu_rq(sg_cpu->cpu)) > sg_cpu->bw_dl)
 #endif
 		sg_policy->limits_changed = true;
@@ -927,7 +927,7 @@ static int __init sugov_register(void)
 core_initcall(sugov_register);
 
 #ifdef CONFIG_ENERGY_MODEL
-#ifndef CONFIG_SCHED_BMQ
+#ifndef CONFIG_SCHED_ALT
 extern bool sched_energy_update;
 extern struct mutex sched_energy_mutex;
 
@@ -958,7 +958,7 @@ void sched_cpufreq_governor_change(struct cpufreq_policy *policy,
 	}
 
 }
-#else /* CONFIG_SCHED_BMQ */
+#else /* CONFIG_SCHED_ALT */
 void sched_cpufreq_governor_change(struct cpufreq_policy *policy,
 				  struct cpufreq_governor *old_gov)
 {
diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 5b19fde0c0ca..472478a4f2a8 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -361,7 +361,7 @@ void cpu_startup_entry(enum cpuhp_state state)
 		do_idle();
 }
 
-#ifndef CONFIG_SCHED_BMQ
+#ifndef CONFIG_SCHED_ALT
 /*
  * idle-task scheduling class.
  */
diff --git a/kernel/sched/pelt.c b/kernel/sched/pelt.c
index 78fcac8198ab..f1983eb87f13 100644
--- a/kernel/sched/pelt.c
+++ b/kernel/sched/pelt.c
@@ -250,7 +250,7 @@ ___update_load_avg(struct sched_avg *sa, unsigned long load)
 	WRITE_ONCE(sa->util_avg, sa->util_sum / divider);
 }
 
-#ifndef CONFIG_SCHED_BMQ
+#ifndef CONFIG_SCHED_ALT
 /*
  * sched_entity:
  *
diff --git a/kernel/sched/pelt.h b/kernel/sched/pelt.h
index 48cddd35444d..49aa805750c5 100644
--- a/kernel/sched/pelt.h
+++ b/kernel/sched/pelt.h
@@ -1,7 +1,7 @@
 #ifdef CONFIG_SMP
 #include "sched-pelt.h"
 
-#ifndef CONFIG_SCHED_BMQ
+#ifndef CONFIG_SCHED_ALT
 int __update_load_avg_blocked_se(u64 now, struct sched_entity *se);
 int __update_load_avg_se(u64 now, struct cfs_rq *cfs_rq, struct sched_entity *se);
 int __update_load_avg_cfs_rq(u64 now, struct cfs_rq *cfs_rq);
@@ -39,7 +39,7 @@ update_irq_load_avg(struct rq *rq, u64 running)
 }
 #endif
 
-#ifndef CONFIG_SCHED_BMQ
+#ifndef CONFIG_SCHED_ALT
 /*
  * When a task is dequeued, its estimated utilization should not be update if
  * its util_avg has not been updated at least once.
@@ -160,11 +160,11 @@ static inline u64 cfs_rq_clock_pelt(struct cfs_rq *cfs_rq)
 	return rq_clock_pelt(rq_of(cfs_rq));
 }
 #endif
-#endif /* CONFIG_SCHED_BMQ */
+#endif /* CONFIG_SCHED_ALT */
 
 #else
 
-#ifndef CONFIG_SCHED_BMQ
+#ifndef CONFIG_SCHED_ALT
 static inline int
 update_cfs_rq_load_avg(u64 now, struct cfs_rq *cfs_rq)
 {
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 32e4177aeea3..682e6b3802c1 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2,8 +2,8 @@
 /*
  * Scheduler internal types and methods:
  */
-#ifdef CONFIG_SCHED_BMQ
-#include "bmq_sched.h"
+#ifdef CONFIG_SCHED_ALT
+#include "alt_sched.h"
 #else
 
 #include <linux/sched.h>
@@ -2557,4 +2557,4 @@ static inline int task_running_nice(struct task_struct *p)
 {
 	return (task_nice(p) > 0);
 }
-#endif /* !CONFIG_SCHED_BMQ */
+#endif /* !CONFIG_SCHED_ALT */
diff --git a/kernel/sched/stats.c b/kernel/sched/stats.c
index 0cc040a28d3f..108422ebc7bf 100644
--- a/kernel/sched/stats.c
+++ b/kernel/sched/stats.c
@@ -22,7 +22,7 @@ static int show_schedstat(struct seq_file *seq, void *v)
 	} else {
 		struct rq *rq;
 #ifdef CONFIG_SMP
-#ifndef CONFIG_SCHED_BMQ
+#ifndef CONFIG_SCHED_ALT
 		struct sched_domain *sd;
 		int dcount = 0;
 #endif
@@ -42,7 +42,7 @@ static int show_schedstat(struct seq_file *seq, void *v)
 		seq_printf(seq, "\n");
 
 #ifdef CONFIG_SMP
-#ifndef CONFIG_SCHED_BMQ
+#ifndef CONFIG_SCHED_ALT
 		/* domain-specific stats */
 		rcu_read_lock();
 		for_each_domain(cpu, sd) {
diff --git a/kernel/sched/topology.c b/kernel/sched/topology.c
index a613249f2375..558ce8a70926 100644
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@ -4,7 +4,7 @@
  */
 #include "sched.h"
 
-#ifndef CONFIG_SCHED_BMQ
+#ifndef CONFIG_SCHED_ALT
 DEFINE_MUTEX(sched_domains_mutex);
 
 /* Protected by sched_domains_mutex: */
@@ -1191,10 +1191,10 @@ static void init_sched_groups_capacity(int cpu, struct sched_domain *sd)
  */
 
 static int default_relax_domain_level = -1;
-#endif /* CONFIG_SCHED_BMQ */
+#endif /* CONFIG_SCHED_ALT */
 int sched_domain_level_max;
 
-#ifndef CONFIG_SCHED_BMQ
+#ifndef CONFIG_SCHED_ALT
 static int __init setup_relax_domain_level(char *str)
 {
 	if (kstrtoint(str, 0, &default_relax_domain_level))
@@ -1427,7 +1427,7 @@ sd_init(struct sched_domain_topology_level *tl,
 
 	return sd;
 }
-#endif /* CONFIG_SCHED_BMQ */
+#endif /* CONFIG_SCHED_ALT */
 
 /*
  * Topology list, bottom-up.
@@ -1457,7 +1457,7 @@ void set_sched_topology(struct sched_domain_topology_level *tl)
 	sched_domain_topology = tl;
 }
 
-#ifndef CONFIG_SCHED_BMQ
+#ifndef CONFIG_SCHED_ALT
 #ifdef CONFIG_NUMA
 
 static const struct cpumask *sd_numa_mask(int cpu)
@@ -2332,7 +2332,7 @@ void partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],
 	partition_sched_domains_locked(ndoms_new, doms_new, dattr_new);
 	mutex_unlock(&sched_domains_mutex);
 }
-#else /* CONFIG_SCHED_BMQ */
+#else /* CONFIG_SCHED_ALT */
 void partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],
 			     struct sched_domain_attr *dattr_new)
 {}
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 5754e28ce21a..8e2ba49be0e1 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -132,7 +132,7 @@ static unsigned long one_ul = 1;
 static unsigned long long_max = LONG_MAX;
 static int one_hundred = 100;
 static int one_thousand = 1000;
-#ifdef CONFIG_SCHED_BMQ
+#ifdef CONFIG_SCHED_ALT
 static int __maybe_unused zero = 0;
 extern int sched_yield_type;
 #endif
@@ -292,7 +292,7 @@ static struct ctl_table sysctl_base_table[] = {
 	{ }
 };
 
-#if defined(CONFIG_SCHED_DEBUG) && !defined(CONFIG_SCHED_BMQ)
+#if defined(CONFIG_SCHED_DEBUG) && !defined(CONFIG_SCHED_ALT)
 static int min_sched_granularity_ns = 100000;		/* 100 usecs */
 static int max_sched_granularity_ns = NSEC_PER_SEC;	/* 1 second */
 static int min_wakeup_granularity_ns;			/* 0 usecs */
@@ -309,7 +309,7 @@ static int max_extfrag_threshold = 1000;
 #endif
 
 static struct ctl_table kern_table[] = {
-#ifndef CONFIG_SCHED_BMQ
+#ifndef CONFIG_SCHED_ALT
 	{
 		.procname	= "sched_child_runs_first",
 		.data		= &sysctl_sched_child_runs_first,
@@ -491,7 +491,7 @@ static struct ctl_table kern_table[] = {
 		.extra2		= SYSCTL_ONE,
 	},
 #endif
-#endif /* !CONFIG_SCHED_BMQ */
+#endif /* !CONFIG_SCHED_ALT */
 #ifdef CONFIG_PROVE_LOCKING
 	{
 		.procname	= "prove_locking",
@@ -1055,7 +1055,7 @@ static struct ctl_table kern_table[] = {
 		.proc_handler	= proc_dointvec,
 	},
 #endif
-#ifdef CONFIG_SCHED_BMQ
+#ifdef CONFIG_SCHED_ALT
 	{
 		.procname	= "yield_type",
 		.data		= &sched_yield_type,
diff --git a/kernel/time/hrtimer.c b/kernel/time/hrtimer.c
index d89da1c7e005..a73adff9f309 100644
--- a/kernel/time/hrtimer.c
+++ b/kernel/time/hrtimer.c
@@ -1923,8 +1923,10 @@ long hrtimer_nanosleep(ktime_t rqtp, const enum hrtimer_mode mode,
 	int ret = 0;
 	u64 slack;
 
+#ifndef CONFIG_SCHED_ALT
 	slack = current->timer_slack_ns;
 	if (dl_task(current) || rt_task(current))
+#endif
 		slack = 0;
 
 	hrtimer_init_sleeper_on_stack(&t, clockid, mode);
diff --git a/kernel/time/posix-cpu-timers.c b/kernel/time/posix-cpu-timers.c
index 8c417f3ea628..e053bc56c019 100644
--- a/kernel/time/posix-cpu-timers.c
+++ b/kernel/time/posix-cpu-timers.c
@@ -806,7 +806,7 @@ static void collect_posix_cputimers(struct posix_cputimers *pct, u64 *samples,
 	}
 }
 
-#ifndef CONFIG_SCHED_BMQ
+#ifndef CONFIG_SCHED_ALT
 static inline void check_dl_overrun(struct task_struct *tsk)
 {
 	if (tsk->dl.dl_overrun) {
@@ -842,7 +842,7 @@ static void check_thread_timers(struct task_struct *tsk,
 	u64 samples[CPUCLOCK_MAX];
 	unsigned long soft;
 
-#ifndef CONFIG_SCHED_BMQ
+#ifndef CONFIG_SCHED_ALT
 	if (dl_task(tsk))
 		check_dl_overrun(tsk);
 #endif
@@ -1095,7 +1095,7 @@ static inline bool fastpath_timer_check(struct task_struct *tsk)
 			return true;
 	}
 
-#ifndef CONFIG_SCHED_BMQ
+#ifndef CONFIG_SCHED_ALT
 	if (dl_task(tsk) && tsk->dl.dl_overrun)
 		return true;
 #endif
diff --git a/kernel/trace/trace_selftest.c b/kernel/trace/trace_selftest.c
index 545be2c4f07c..cfbae0a21cef 100644
--- a/kernel/trace/trace_selftest.c
+++ b/kernel/trace/trace_selftest.c
@@ -1048,7 +1048,7 @@ static int trace_wakeup_test_thread(void *data)
 {
 	/* Make this a -deadline thread */
 	static const struct sched_attr attr = {
-#ifdef CONFIG_SCHED_BMQ
+#ifdef CONFIG_SCHED_ALT
 		/* No deadline on BMQ, use RR */
 		.sched_policy = SCHED_RR,
 #else
-- 
2.27.0.112.g101b3204f3

