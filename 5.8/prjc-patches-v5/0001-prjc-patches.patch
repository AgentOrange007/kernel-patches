From 9cedbff47849ce1c814bf1e85066f53fd3e89a31 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sun, 9 Aug 2020 08:49:45 +0800
Subject: [PATCH 1/6] sched/pds: Port of PDS

Port PDS from 5.0 to current Project C.
---
 include/linux/sched.h          |  11 +-
 include/linux/sched/deadline.h |   4 +
 include/linux/sched/prio.h     |   5 +-
 include/linux/skip_list.h      | 177 ++++++++++++++++++++++++++++
 init/Kconfig                   |   6 +
 init/init_task.c               |   7 +-
 kernel/locking/rtmutex.c       |   8 ++
 kernel/sched/alt_core.c        | 110 ++++++------------
 kernel/sched/alt_sched.h       |  11 +-
 kernel/sched/bmq.h             |   6 +
 kernel/sched/bmq_imp.h         | 127 +++++++++++++++++---
 kernel/sched/pds.h             |  14 +++
 kernel/sched/pds_imp.h         | 205 +++++++++++++++++++++++++++++++++
 13 files changed, 593 insertions(+), 98 deletions(-)
 create mode 100644 include/linux/skip_list.h
 create mode 100644 kernel/sched/pds.h
 create mode 100644 kernel/sched/pds_imp.h

diff --git a/include/linux/sched.h b/include/linux/sched.h
index efee54cbe711..d25f2501daf3 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -32,6 +32,7 @@
 #include <linux/posix-timers.h>
 #include <linux/rseq.h>
 #include <linux/kcsan.h>
+#include <linux/skip_list.h>
 
 /* task_struct member predeclarations (sorted alphabetically): */
 struct audit_context;
@@ -687,11 +688,19 @@ struct task_struct {
 #ifdef CONFIG_SCHED_ALT
 	u64				last_ran;
 	s64				time_slice;
-	int				boost_prio;
 #ifdef CONFIG_SCHED_BMQ
+	int				boost_prio;
 	int				bmq_idx;
 	struct list_head		bmq_node;
 #endif /* CONFIG_SCHED_BMQ */
+#ifdef CONFIG_SCHED_PDS
+	u64				deadline;
+	u64				priodl;
+	/* skip list level */
+	int				sl_level;
+	/* skip list node */
+	struct skiplist_node		sl_node;
+#endif /* CONFIG_SCHED_PDS */
 	/* sched_clock time spent running */
 	u64				sched_time;
 #else /* !CONFIG_SCHED_ALT */
diff --git a/include/linux/sched/deadline.h b/include/linux/sched/deadline.h
index da0306d2fedb..45f0b0f3616c 100644
--- a/include/linux/sched/deadline.h
+++ b/include/linux/sched/deadline.h
@@ -11,6 +11,10 @@ static inline int dl_task(struct task_struct *p)
 }
 #endif
 
+#ifdef CONFIG_SCHED_PDS
+#define __tsk_deadline(p)	((p)->priodl)
+#endif
+
 #else
 
 #define __tsk_deadline(p)	((p)->dl.deadline)
diff --git a/include/linux/sched/prio.h b/include/linux/sched/prio.h
index 1b2ebeeb45eb..42730d27ceb5 100644
--- a/include/linux/sched/prio.h
+++ b/include/linux/sched/prio.h
@@ -26,10 +26,13 @@
 #define MAX_PRIO		(MAX_RT_PRIO + NICE_WIDTH)
 #define DEFAULT_PRIO		(MAX_RT_PRIO + NICE_WIDTH / 2)
 
-#ifdef CONFIG_SCHED_ALT
 /* +/- priority levels from the base priority */
+#ifdef CONFIG_SCHED_BMQ
 #define MAX_PRIORITY_ADJ	7
 #endif
+#ifdef CONFIG_SCHED_PDS
+#define MAX_PRIORITY_ADJ	0
+#endif
 
 /*
  * Convert user-nice values [ -20 ... 0 ... 19 ]
diff --git a/include/linux/skip_list.h b/include/linux/skip_list.h
new file mode 100644
index 000000000000..47ca955a451d
--- /dev/null
+++ b/include/linux/skip_list.h
@@ -0,0 +1,177 @@
+/*
+ * Copyright (C) 2016 Alfred Chen.
+ *
+ * Code based on Con Kolivas's skip list implementation for BFS, and
+ * which is based on example originally by William Pugh.
+ *
+ * Skip Lists are a probabilistic alternative to balanced trees, as
+ * described in the June 1990 issue of CACM and were invented by
+ * William Pugh in 1987.
+ *
+ * A couple of comments about this implementation:
+ *
+ * This file only provides a infrastructure of skip list.
+ *
+ * skiplist_node is embedded into container data structure, to get rid
+ * the dependency of kmalloc/kfree operation in scheduler code.
+ *
+ * A customized search function should be defined using DEFINE_SKIPLIST_INSERT
+ * macro and be used for skip list insert operation.
+ *
+ * Random Level is also not defined in this file, instead, it should be
+ * customized implemented and set to node->level then pass to the customized
+ * skiplist_insert function.
+ *
+ * Levels start at zero and go up to (NUM_SKIPLIST_LEVEL -1)
+ *
+ * NUM_SKIPLIST_LEVEL in this implementation is 8 instead of origin 16,
+ * considering that there will be 256 entries to enable the top level when using
+ * random level p=0.5, and that number is more than enough for a run queue usage
+ * in a scheduler usage. And it also help to reduce the memory usage of the
+ * embedded skip list node in task_struct to about 50%.
+ *
+ * The insertion routine has been implemented so as to use the
+ * dirty hack described in the CACM paper: if a random level is
+ * generated that is more than the current maximum level, the
+ * current maximum level plus one is used instead.
+ *
+ * BFS Notes: In this implementation of skiplists, there are bidirectional
+ * next/prev pointers and the insert function returns a pointer to the actual
+ * node the value is stored. The key here is chosen by the scheduler so as to
+ * sort tasks according to the priority list requirements and is no longer used
+ * by the scheduler after insertion. The scheduler lookup, however, occurs in
+ * O(1) time because it is always the first item in the level 0 linked list.
+ * Since the task struct stores a copy of the node pointer upon skiplist_insert,
+ * it can also remove it much faster than the original implementation with the
+ * aid of prev<->next pointer manipulation and no searching.
+ */
+#ifndef _LINUX_SKIP_LIST_H
+#define _LINUX_SKIP_LIST_H
+
+#include <linux/kernel.h>
+
+#define NUM_SKIPLIST_LEVEL (8)
+
+struct skiplist_node {
+	int level;	/* Levels in this node */
+	struct skiplist_node *next[NUM_SKIPLIST_LEVEL];
+	struct skiplist_node *prev[NUM_SKIPLIST_LEVEL];
+};
+
+#define SKIPLIST_NODE_INIT(name) { 0,\
+				   {&name, &name, &name, &name,\
+				    &name, &name, &name, &name},\
+				   {&name, &name, &name, &name,\
+				    &name, &name, &name, &name},\
+				 }
+
+static inline void INIT_SKIPLIST_NODE(struct skiplist_node *node)
+{
+	/* only level 0 ->next matters in skiplist_empty() */
+	WRITE_ONCE(node->next[0], node);
+}
+
+/**
+ * FULL_INIT_SKIPLIST_NODE -- fully init a skiplist_node, expecially for header
+ * @node: the skip list node to be inited.
+ */
+static inline void FULL_INIT_SKIPLIST_NODE(struct skiplist_node *node)
+{
+	int i;
+
+	node->level = 0;
+	for (i = 0; i < NUM_SKIPLIST_LEVEL; i++) {
+		WRITE_ONCE(node->next[i], node);
+		node->prev[i] = node;
+	}
+}
+
+/**
+ * skiplist_empty - test whether a skip list is empty
+ * @head: the skip list to test.
+ */
+static inline int skiplist_empty(const struct skiplist_node *head)
+{
+	return READ_ONCE(head->next[0]) == head;
+}
+
+/**
+ * skiplist_entry - get the struct for this entry
+ * @ptr: the &struct skiplist_node pointer.
+ * @type:       the type of the struct this is embedded in.
+ * @member:     the name of the skiplist_node within the struct.
+ */
+#define skiplist_entry(ptr, type, member) \
+	container_of(ptr, type, member)
+
+/**
+ * DEFINE_SKIPLIST_INSERT_FUNC -- macro to define a customized skip list insert
+ * function, which takes two parameters, first one is the header node of the
+ * skip list, second one is the skip list node to be inserted
+ * @func_name: the customized skip list insert function name
+ * @search_func: the search function to be used, which takes two parameters,
+ * 1st one is the itrator of skiplist_node in the list, the 2nd is the skip list
+ * node to be inserted, the function should return true if search should be
+ * continued, otherwise return false.
+ * Returns 1 if @node is inserted as the first item of skip list at level zero,
+ * otherwise 0
+ */
+#define DEFINE_SKIPLIST_INSERT_FUNC(func_name, search_func)\
+static inline int func_name(struct skiplist_node *head, struct skiplist_node *node)\
+{\
+	struct skiplist_node *update[NUM_SKIPLIST_LEVEL];\
+	struct skiplist_node *p, *q;\
+	int k = head->level;\
+\
+	p = head;\
+	do {\
+		while (q = p->next[k], q != head && search_func(q, node))\
+			p = q;\
+		update[k] = p;\
+	} while (--k >= 0);\
+\
+	k = node->level;\
+	if (unlikely(k > head->level)) {\
+		node->level = k = ++head->level;\
+		update[k] = head;\
+	}\
+\
+	do {\
+		p = update[k];\
+		q = p->next[k];\
+		node->next[k] = q;\
+		p->next[k] = node;\
+		node->prev[k] = p;\
+		q->prev[k] = node;\
+	} while (--k >= 0);\
+\
+	return (p == head);\
+}
+
+/**
+ * skiplist_del_init -- delete skip list node from a skip list and reset it's
+ * init state
+ * @head: the header node of the skip list to be deleted from.
+ * @node: the skip list node to be deleted, the caller need to ensure @node is
+ * in skip list which @head represent.
+ * Returns 1 if @node is the first item of skip level at level zero, otherwise 0
+ */
+static inline int
+skiplist_del_init(struct skiplist_node *head, struct skiplist_node *node)
+{
+	int l, m = node->level;
+
+	for (l = 0; l <= m; l++) {
+		node->prev[l]->next[l] = node->next[l];
+		node->next[l]->prev[l] = node->prev[l];
+	}
+	if (m == head->level && m > 0) {
+		while (head->next[m] == head && m > 0)
+			m--;
+		head->level = m;
+	}
+	INIT_SKIPLIST_NODE(node);
+
+	return (node->prev[0] == head);
+}
+#endif /* _LINUX_SKIP_LIST_H */
diff --git a/init/Kconfig b/init/Kconfig
index 3da07510e895..09a302641ba6 100644
--- a/init/Kconfig
+++ b/init/Kconfig
@@ -761,6 +761,12 @@ config SCHED_BMQ
 	  responsiveness on the desktop and solid scalability on normal
 	  hardware and commodity servers.
 
+config SCHED_PDS
+	bool "PDS CPU scheduler"
+	help
+	  The Priority and Deadline based Skip list multiple queue CPU
+	  Scheduler.
+
 endchoice
 
 endif
diff --git a/init/init_task.c b/init/init_task.c
index 3310178d4c89..6bc94553d79a 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -93,10 +93,15 @@ struct task_struct init_task
 		.fn = do_no_restart_syscall,
 	},
 #ifdef CONFIG_SCHED_ALT
-	.boost_prio	= 0,
 #ifdef CONFIG_SCHED_BMQ
+	.boost_prio	= 0,
 	.bmq_idx	= 15,
 	.bmq_node	= LIST_HEAD_INIT(init_task.bmq_node),
+#endif
+#ifdef CONFIG_SCHED_PDS
+	.deadline	= 0,
+	.sl_level	= 0,
+	.sl_node	= SKIPLIST_NODE_INIT(init_task.sl_node),
 #endif
 	.time_slice	= HZ,
 #else
diff --git a/kernel/locking/rtmutex.c b/kernel/locking/rtmutex.c
index 56ef920b54f0..84c284eb544a 100644
--- a/kernel/locking/rtmutex.c
+++ b/kernel/locking/rtmutex.c
@@ -233,6 +233,9 @@ static inline int
 rt_mutex_waiter_less(struct rt_mutex_waiter *left,
 		     struct rt_mutex_waiter *right)
 {
+#ifdef CONFIG_SCHED_PDS
+	return (left->deadline < right->deadline);
+#else
 	if (left->prio < right->prio)
 		return 1;
 
@@ -248,12 +251,16 @@ rt_mutex_waiter_less(struct rt_mutex_waiter *left,
 #endif
 
 	return 0;
+#endif
 }
 
 static inline int
 rt_mutex_waiter_equal(struct rt_mutex_waiter *left,
 		      struct rt_mutex_waiter *right)
 {
+#ifdef CONFIG_SCHED_PDS
+	return (left->deadline == right->deadline);
+#else
 	if (left->prio != right->prio)
 		return 0;
 
@@ -269,6 +276,7 @@ rt_mutex_waiter_equal(struct rt_mutex_waiter *left,
 #endif
 
 	return 1;
+#endif
 }
 
 static void
diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 5db1f74f3559..407bc46de451 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -78,36 +78,6 @@ early_param("sched_timeslice", sched_timeslice);
  */
 int sched_yield_type __read_mostly = 1;
 
-#define rq_switch_time(rq)	((rq)->clock - (rq)->last_ts_switch)
-#define boost_threshold(p)	(sched_timeslice_ns >>\
-				 (15 - MAX_PRIORITY_ADJ -  (p)->boost_prio))
-
-static inline void boost_task(struct task_struct *p)
-{
-	int limit;
-
-	switch (p->policy) {
-	case SCHED_NORMAL:
-		limit = -MAX_PRIORITY_ADJ;
-		break;
-	case SCHED_BATCH:
-	case SCHED_IDLE:
-		limit = 0;
-		break;
-	default:
-		return;
-	}
-
-	if (p->boost_prio > limit)
-		p->boost_prio--;
-}
-
-static inline void deboost_task(struct task_struct *p)
-{
-	if (p->boost_prio < MAX_PRIORITY_ADJ)
-		p->boost_prio++;
-}
-
 #ifdef CONFIG_SMP
 static cpumask_t sched_rq_pending_mask ____cacheline_aligned_in_smp;
 
@@ -146,13 +116,22 @@ static cpumask_t sched_sg_idle_mask ____cacheline_aligned_in_smp;
 #endif
 static cpumask_t sched_rq_watermark[SCHED_BITS] ____cacheline_aligned_in_smp;
 
+#ifdef CONFIG_SCHED_BMQ
+#include "bmq_imp.h"
+#endif
+#ifdef CONFIG_SCHED_PDS
+#include "pds_imp.h"
+#endif
+
 static inline void update_sched_rq_watermark(struct rq *rq)
 {
-	unsigned long watermark = find_first_bit(rq->queue.bitmap, SCHED_BITS);
+	unsigned long watermark = sched_queue_watermark(rq);
 	unsigned long last_wm = rq->watermark;
 	unsigned long i;
 	int cpu;
 
+	/*printk(KERN_INFO "sched: watermark(%d) %d, last %d\n",
+	       cpu_of(rq), watermark, last_wm);*/
 	if (watermark == last_wm)
 		return;
 
@@ -187,13 +166,6 @@ static inline void update_sched_rq_watermark(struct rq *rq)
 #endif
 }
 
-static inline int task_sched_prio(struct task_struct *p)
-{
-	return (p->prio < MAX_RT_PRIO)? p->prio : MAX_RT_PRIO / 2 + (p->prio + p->boost_prio) / 2;
-}
-
-#include "bmq_imp.h"
-
 static inline struct task_struct *rq_runnable_task(struct rq *rq)
 {
 	struct task_struct *next = sched_rq_first_task(rq);
@@ -456,6 +428,7 @@ static inline void dequeue_task(struct task_struct *p, struct rq *rq, int flags)
 {
 	lockdep_assert_held(&rq->lock);
 
+	/*printk(KERN_INFO "sched: dequeue(%d) %px %016llx\n", cpu_of(rq), p, p->priodl);*/
 	WARN_ONCE(task_rq(p) != rq, "sched: dequeue task reside on cpu%d from cpu%d\n",
 		  task_cpu(p), cpu_of(rq));
 
@@ -473,6 +446,7 @@ static inline void enqueue_task(struct task_struct *p, struct rq *rq, int flags)
 {
 	lockdep_assert_held(&rq->lock);
 
+	/*printk(KERN_INFO "sched: enqueue(%d) %px %016llx\n", cpu_of(rq), p, p->priodl);*/
 	WARN_ONCE(task_rq(p) != rq, "sched: enqueue task reside on cpu%d to cpu%d\n",
 		  task_cpu(p), cpu_of(rq));
 
@@ -498,10 +472,11 @@ static inline void enqueue_task(struct task_struct *p, struct rq *rq, int flags)
 static inline void requeue_task(struct task_struct *p, struct rq *rq)
 {
 	lockdep_assert_held(&rq->lock);
+	/*printk(KERN_INFO "sched: requeue(%d) %px %016llx\n", cpu_of(rq), p, p->priodl);*/
 	WARN_ONCE(task_rq(p) != rq, "sched: cpu[%d] requeue task reside on cpu%d\n",
 		  cpu_of(rq), task_cpu(p));
 
-	__requeue_task(p, rq);
+	__SCHED_REQUEUE_TASK(p, rq, update_sched_rq_watermark(rq));
 }
 
 /*
@@ -1428,7 +1403,7 @@ static int select_fallback_rq(int cpu, struct task_struct *p)
 	return dest_cpu;
 }
 
-static inline int select_task_rq(struct task_struct *p)
+static inline int select_task_rq(struct task_struct *p, struct rq *rq)
 {
 	cpumask_t chk_mask, tmp;
 
@@ -1441,7 +1416,7 @@ static inline int select_task_rq(struct task_struct *p)
 #endif
 	    cpumask_and(&tmp, &chk_mask, &sched_rq_watermark[IDLE_WM]) ||
 	    cpumask_and(&tmp, &chk_mask,
-			&sched_rq_watermark[task_sched_prio(p) + 1]))
+			&sched_rq_watermark[task_sched_prio(p, rq) + 1]))
 		return best_mask_cpu(task_cpu(p), &tmp);
 
 	return best_mask_cpu(task_cpu(p), &chk_mask);
@@ -1573,7 +1548,7 @@ EXPORT_SYMBOL_GPL(set_cpus_allowed_ptr);
 
 #else /* CONFIG_SMP */
 
-static inline int select_task_rq(struct task_struct *p)
+static inline int select_task_rq(struct task_struct *p, struct rq *rq)
 {
 	return 0;
 }
@@ -2039,10 +2014,9 @@ static int try_to_wake_up(struct task_struct *p, unsigned int state,
 	 */
 	smp_cond_load_acquire(&p->on_cpu, !VAL);
 
-	if(this_rq()->clock_task - p->last_ran > sched_timeslice_ns)
-		boost_task(p);
+	sched_task_ttwu(p);
 
-	cpu = select_task_rq(p);
+	cpu = select_task_rq(p, this_rq());
 
 	if (cpu != task_cpu(p)) {
 		wake_flags |= WF_MIGRATED;
@@ -2198,9 +2172,9 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 		 */
 		p->sched_reset_on_fork = 0;
 	}
+	update_task_priodl(p);
 
-	p->boost_prio = (p->boost_prio < 0) ?
-		p->boost_prio + MAX_PRIORITY_ADJ : MAX_PRIORITY_ADJ;
+	sched_task_fork(p);
 	/*
 	 * The child is not yet in the pid-hash so no cgroup attach races,
 	 * and the cgroup is pinned to this child due to cgroup_fork()
@@ -2224,6 +2198,7 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 
 	if (p->time_slice < RESCHED_NS) {
 		p->time_slice = sched_timeslice_ns;
+		time_slice_expired(p, rq);
 		resched_curr(rq);
 	}
 	raw_spin_unlock(&rq->lock);
@@ -2338,7 +2313,7 @@ void wake_up_new_task(struct task_struct *p)
 
 	p->state = TASK_RUNNING;
 
-	rq = cpu_rq(select_task_rq(p));
+	rq = cpu_rq(select_task_rq(p, this_rq()));
 #ifdef CONFIG_SMP
 	rseq_migrate(p);
 	/*
@@ -3436,11 +3411,7 @@ static inline void check_curr(struct task_struct *p, struct rq *rq)
 
 	if (p->time_slice < RESCHED_NS) {
 		p->time_slice = sched_timeslice_ns;
-		if (SCHED_FIFO != p->policy && task_on_rq_queued(p)) {
-			if (SCHED_RR != p->policy)
-				deboost_task(p);
-			requeue_task(p, rq);
-		}
+		time_slice_expired(p, rq);
 	}
 }
 
@@ -3476,6 +3447,7 @@ choose_next_task(struct rq *rq, int cpu, struct task_struct *prev)
 		if (!take_other_rq_tasks(rq, cpu)) {
 #endif
 			schedstat_inc(rq->sched_goidle);
+			/*printk(KERN_INFO "sched: choose_next_task(%d) idle %px\n", cpu, next);*/
 			return next;
 #ifdef	CONFIG_SMP
 		}
@@ -3485,6 +3457,8 @@ choose_next_task(struct rq *rq, int cpu, struct task_struct *prev)
 #ifdef CONFIG_HIGH_RES_TIMERS
 	hrtick_start(rq, next->time_slice);
 #endif
+	/*printk(KERN_INFO "sched: choose_next_task(%d) next %px\n", cpu,
+	 * next);*/
 	return next;
 }
 
@@ -3599,8 +3573,7 @@ static void __sched notrace __schedule(bool preempt)
 			 *
 			 * After this, schedule() must not care about p->state any more.
 			 */
-			if (rq_switch_time(rq) < boost_threshold(prev))
-				boost_task(prev);
+			sched_task_deactivate(prev, rq);
 			deactivate_task(prev, rq);
 
 			if (prev->in_iowait) {
@@ -3926,7 +3899,7 @@ EXPORT_SYMBOL(default_wake_function);
 static inline void check_task_changed(struct rq *rq, struct task_struct *p)
 {
 	/* Trigger resched if task sched_prio has been modified. */
-	if (task_on_rq_queued(p) && sched_task_need_requeue(p)) {
+	if (task_on_rq_queued(p) && sched_task_need_requeue(p, rq)) {
 		requeue_task(p, rq);
 		check_preempt_curr(rq);
 	}
@@ -4014,6 +3987,7 @@ void rt_mutex_setprio(struct task_struct *p, struct task_struct *pi_task)
 
 	trace_sched_pi_setprio(p, pi_task);
 	p->prio = prio;
+	update_task_priodl(p);
 
 	check_task_changed(rq, p);
 out_unlock:
@@ -4052,6 +4026,8 @@ void set_user_nice(struct task_struct *p, long nice)
 		goto out_unlock;
 
 	p->prio = effective_prio(p);
+	update_task_priodl(p);
+
 	check_task_changed(rq, p);
 out_unlock:
 	__task_access_unlock(p, lock);
@@ -4109,21 +4085,6 @@ SYSCALL_DEFINE1(nice, int, increment)
 
 #endif
 
-/**
- * task_prio - return the priority value of a given task.
- * @p: the task in question.
- *
- * Return: The priority value as seen by users in /proc.
- * RT tasks are offset by -100. Normal tasks are centered around 1, value goes
- * from 0(SCHED_ISO) up to 82 (nice +19 SCHED_IDLE).
- */
-int task_prio(const struct task_struct *p)
-{
-	if (p->prio < MAX_RT_PRIO)
-		return (p->prio - MAX_RT_PRIO);
-	return (p->prio - MAX_RT_PRIO + p->boost_prio);
-}
-
 /**
  * idle_cpu - is a given CPU idle currently?
  * @cpu: the processor in question.
@@ -4215,6 +4176,7 @@ static void __setscheduler(struct rq *rq, struct task_struct *p,
 	p->prio = normal_prio(p);
 	if (keep_boost)
 		p->prio = rt_effective_prio(p, p->prio);
+	update_task_priodl(p);
 }
 
 /*
@@ -4974,10 +4936,8 @@ static void do_sched_yield(void)
 	schedstat_inc(rq->yld_count);
 
 	if (1 == sched_yield_type) {
-		if (!rt_task(current)) {
-			current->boost_prio = MAX_PRIORITY_ADJ;
-			requeue_task(current, rq);
-		}
+		if (!rt_task(current))
+			do_sched_yield_type_1(current, rq);
 	} else if (2 == sched_yield_type) {
 		if (rq->nr_running > 1)
 			rq->skip = current;
diff --git a/kernel/sched/alt_sched.h b/kernel/sched/alt_sched.h
index d8887f377455..99be2c51c88d 100644
--- a/kernel/sched/alt_sched.h
+++ b/kernel/sched/alt_sched.h
@@ -49,6 +49,9 @@
 #ifdef CONFIG_SCHED_BMQ
 #include "bmq.h"
 #endif
+#ifdef CONFIG_SCHED_PDS
+#include "pds.h"
+#endif
 
 /* task_struct::on_rq states: */
 #define TASK_ON_RQ_QUEUED	1
@@ -86,6 +89,9 @@ struct rq {
 
 #ifdef CONFIG_SCHED_BMQ
 	struct bmq queue;
+#endif
+#ifdef CONFIG_SCHED_PDS
+	struct skiplist_node sl_header;
 #endif
 	unsigned long watermark;
 
@@ -534,11 +540,6 @@ static inline void membarrier_switch_mm(struct rq *rq,
 }
 #endif
 
-static inline int task_running_nice(struct task_struct *p)
-{
-	return (p->prio + p->boost_prio > DEFAULT_PRIO + MAX_PRIORITY_ADJ);
-}
-
 #ifdef CONFIG_NUMA
 extern int sched_numa_find_closest(const struct cpumask *cpus, int cpu);
 #else
diff --git a/kernel/sched/bmq.h b/kernel/sched/bmq.h
index aba3c98759f8..aff0bb30a884 100644
--- a/kernel/sched/bmq.h
+++ b/kernel/sched/bmq.h
@@ -11,4 +11,10 @@ struct bmq {
 	struct list_head heads[SCHED_BITS];
 };
 
+
+static inline int task_running_nice(struct task_struct *p)
+{
+	return (p->prio + p->boost_prio > DEFAULT_PRIO + MAX_PRIORITY_ADJ);
+}
+
 #endif
diff --git a/kernel/sched/bmq_imp.h b/kernel/sched/bmq_imp.h
index 86d496ec23b3..d7df1d3f9495 100644
--- a/kernel/sched/bmq_imp.h
+++ b/kernel/sched/bmq_imp.h
@@ -1,5 +1,64 @@
 #define ALT_SCHED_VERSION_MSG "sched/bmq: BMQ CPU Scheduler 5.8-r1 by Alfred Chen.\n"
 
+/*
+ * BMQ only routines
+ */
+#define rq_switch_time(rq)	((rq)->clock - (rq)->last_ts_switch)
+#define boost_threshold(p)	(sched_timeslice_ns >>\
+				 (15 - MAX_PRIORITY_ADJ -  (p)->boost_prio))
+
+static inline void boost_task(struct task_struct *p)
+{
+	int limit;
+
+	switch (p->policy) {
+	case SCHED_NORMAL:
+		limit = -MAX_PRIORITY_ADJ;
+		break;
+	case SCHED_BATCH:
+	case SCHED_IDLE:
+		limit = 0;
+		break;
+	default:
+		return;
+	}
+
+	if (p->boost_prio > limit)
+		p->boost_prio--;
+}
+
+static inline void deboost_task(struct task_struct *p)
+{
+	if (p->boost_prio < MAX_PRIORITY_ADJ)
+		p->boost_prio++;
+}
+
+/*
+ * Common interfaces
+ */
+static inline int task_sched_prio(struct task_struct *p, struct rq *rq)
+{
+	return (p->prio < MAX_RT_PRIO)? p->prio : MAX_RT_PRIO / 2 + (p->prio + p->boost_prio) / 2;
+}
+
+static inline void requeue_task(struct task_struct *p, struct rq *rq);
+
+static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
+{
+	if (SCHED_FIFO != p->policy && task_on_rq_queued(p)) {
+		if (SCHED_RR != p->policy)
+			deboost_task(p);
+		requeue_task(p, rq);
+	}
+}
+
+static inline void update_task_priodl(struct task_struct *p) {}
+
+static inline unsigned long sched_queue_watermark(struct rq *rq)
+{
+	return find_first_bit(rq->queue.bitmap, SCHED_BITS);
+}
+
 static inline void sched_queue_init(struct rq *rq)
 {
 	struct bmq *q = &rq->queue;
@@ -61,26 +120,64 @@ sched_rq_next_task(struct task_struct *p, struct rq *rq)
 	sched_info_queued(rq, p);					\
 	psi_enqueue(p, flags);						\
 									\
-	p->bmq_idx = task_sched_prio(p);				\
+	p->bmq_idx = task_sched_prio(p, rq);				\
 	list_add_tail(&p->bmq_node, &rq->queue.heads[p->bmq_idx]);	\
 	set_bit(p->bmq_idx, rq->queue.bitmap)
 
-static inline void __requeue_task(struct task_struct *p, struct rq *rq)
+#define __SCHED_REQUEUE_TASK(p, rq, func)				\
+{									\
+	int idx = task_sched_prio(p, rq);				\
+\
+	list_del(&p->bmq_node);						\
+	list_add_tail(&p->bmq_node, &rq->queue.heads[idx]);		\
+	if (idx != p->bmq_idx) {					\
+		if (list_empty(&rq->queue.heads[p->bmq_idx]))		\
+			clear_bit(p->bmq_idx, rq->queue.bitmap);	\
+		p->bmq_idx = idx;					\
+		set_bit(p->bmq_idx, rq->queue.bitmap);			\
+		func;							\
+	}								\
+}
+
+static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
 {
-	int idx = task_sched_prio(p);
-
-	list_del(&p->bmq_node);
-	list_add_tail(&p->bmq_node, &rq->queue.heads[idx]);
-	if (idx != p->bmq_idx) {
-		if (list_empty(&rq->queue.heads[p->bmq_idx]))
-			clear_bit(p->bmq_idx, rq->queue.bitmap);
-		p->bmq_idx = idx;
-		set_bit(p->bmq_idx, rq->queue.bitmap);
-		update_sched_rq_watermark(rq);
-	}
+	return (task_sched_prio(p, rq) != p->bmq_idx);
+}
+
+static void sched_task_fork(struct task_struct *p)
+{
+	p->boost_prio = (p->boost_prio < 0) ?
+		p->boost_prio + MAX_PRIORITY_ADJ : MAX_PRIORITY_ADJ;
+}
+
+/**
+ * task_prio - return the priority value of a given task.
+ * @p: the task in question.
+ *
+ * Return: The priority value as seen by users in /proc.
+ * RT tasks are offset by -100. Normal tasks are centered around 1, value goes
+ * from 0(SCHED_ISO) up to 82 (nice +19 SCHED_IDLE).
+ */
+int task_prio(const struct task_struct *p)
+{
+	if (p->prio < MAX_RT_PRIO)
+		return (p->prio - MAX_RT_PRIO);
+	return (p->prio - MAX_RT_PRIO + p->boost_prio);
+}
+
+static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
+{
+	p->boost_prio = MAX_PRIORITY_ADJ;
+}
+
+static void sched_task_ttwu(struct task_struct *p)
+{
+	if(this_rq()->clock_task - p->last_ran > sched_timeslice_ns)
+		boost_task(p);
 }
 
-static inline bool sched_task_need_requeue(struct task_struct *p)
+static void sched_task_deactivate(struct task_struct *p, struct rq *rq)
 {
-	return (task_sched_prio(p) != p->bmq_idx);
+	if (rq_switch_time(rq) < boost_threshold(p))
+		boost_task(p);
 }
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
new file mode 100644
index 000000000000..9b9addc205a9
--- /dev/null
+++ b/kernel/sched/pds.h
@@ -0,0 +1,14 @@
+#ifndef PDS_H
+#define PDS_H
+
+/* bits:
+ * RT(0-99), (Low prio adj range, nice width, high prio adj range) / 2, cpu idle task */
+#define SCHED_BITS	(MAX_RT_PRIO + MAX_PRIORITY_ADJ * 2 + 8 + 1)
+#define IDLE_TASK_SCHED_PRIO	(SCHED_BITS - 1)
+
+static inline int task_running_nice(struct task_struct *p)
+{
+	return (p->prio > DEFAULT_PRIO);
+}
+
+#endif
diff --git a/kernel/sched/pds_imp.h b/kernel/sched/pds_imp.h
new file mode 100644
index 000000000000..b970879f1d2e
--- /dev/null
+++ b/kernel/sched/pds_imp.h
@@ -0,0 +1,205 @@
+#define ALT_SCHED_VERSION_MSG "sched/bmq: PDS CPU Scheduler 5.8-r0 by Alfred Chen.\n"
+
+static const u64 user_prio2deadline[NICE_WIDTH] = {
+/* -20 */	  6291456,   6920601,   7612661,   8373927,   9211319,
+/* -15 */	 10132450,  11145695,  12260264,  13486290,  14834919,
+/* -10 */	 16318410,  17950251,  19745276,  21719803,  23891783,
+/*  -5 */	 26280961,  28909057,  31799962,  34979958,  38477953,
+/*   0 */	 42325748,  46558322,  51214154,  56335569,  61969125,
+/*   5 */	 68166037,  74982640,  82480904,  90728994,  99801893,
+/*  10 */	109782082, 120760290, 132836319, 146119950, 160731945,
+/*  15 */	176805139, 194485652, 213934217, 235327638, 258860401
+};
+
+static const int dl_level_map[] = {
+/*      0           4           8           12           */
+	0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
+/*      16          20          24          28           */
+	1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 4, 4, 5, 6, 7
+};
+
+static inline int
+task_sched_prio(const struct task_struct *p, const struct rq *rq)
+{
+	u64 delta = (rq->clock + user_prio2deadline[39] - p->deadline) >> 23;
+
+	delta = min((size_t)delta, ARRAY_SIZE(dl_level_map) - 1);
+
+	return (p->prio < MAX_RT_PRIO)? p->prio : MAX_RT_PRIO + dl_level_map[delta];
+}
+
+static inline void update_task_priodl(struct task_struct *p)
+{
+	p->priodl = (((u64) (p->prio))<<56) | ((p->deadline)>>8);
+}
+
+static inline void requeue_task(struct task_struct *p, struct rq *rq);
+
+static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
+{
+	/*printk(KERN_INFO "sched: time_slice_expired(%d) - %px\n", cpu_of(rq), p);*/
+
+	if (p->prio >= MAX_RT_PRIO)
+		p->deadline = rq->clock + user_prio2deadline[TASK_USER_PRIO(p)];
+	update_task_priodl(p);
+
+	if (SCHED_FIFO != p->policy && task_on_rq_queued(p))
+		requeue_task(p, rq);
+}
+
+/*
+ * pds_skiplist_task_search -- search function used in PDS run queue skip list
+ * node insert operation.
+ * @it: iterator pointer to the node in the skip list
+ * @node: pointer to the skiplist_node to be inserted
+ *
+ * Returns true if key of @it is less or equal to key value of @node, otherwise
+ * false.
+ */
+static inline bool
+pds_skiplist_task_search(struct skiplist_node *it, struct skiplist_node *node)
+{
+	return (skiplist_entry(it, struct task_struct, sl_node)->priodl <=
+		skiplist_entry(node, struct task_struct, sl_node)->priodl);
+}
+
+/*
+ * Define the skip list insert function for PDS
+ */
+DEFINE_SKIPLIST_INSERT_FUNC(pds_skiplist_insert, pds_skiplist_task_search);
+
+/*
+ * Init the queue structure in rq
+ */
+static inline void sched_queue_init(struct rq *rq)
+{
+	FULL_INIT_SKIPLIST_NODE(&rq->sl_header);
+}
+
+/*
+ * Init idle task and put into queue structure of rq
+ * IMPORTANT: may be called multiple times for a single cpu
+ */
+static inline void sched_queue_init_idle(struct rq *rq, struct task_struct *idle)
+{
+	/*printk(KERN_INFO "sched: init(%d) - %px\n", cpu_of(rq), idle);*/
+	int default_prio = idle->prio;
+
+	idle->prio = MAX_PRIO;
+	idle->deadline = 0ULL;
+	update_task_priodl(idle);
+
+	FULL_INIT_SKIPLIST_NODE(&rq->sl_header);
+
+	idle->sl_node.level = idle->sl_level;
+	pds_skiplist_insert(&rq->sl_header, &idle->sl_node);
+
+	idle->prio = default_prio;
+}
+
+/*
+ * This routine assume that the idle task always in queue
+ */
+static inline struct task_struct *sched_rq_first_task(struct rq *rq)
+{
+	struct skiplist_node *node = rq->sl_header.next[0];
+
+	BUG_ON(node == &rq->sl_header);
+	return skiplist_entry(node, struct task_struct, sl_node);
+}
+
+static inline struct task_struct *
+sched_rq_next_task(struct task_struct *p, struct rq *rq)
+{
+	struct skiplist_node *next = p->sl_node.next[0];
+
+	BUG_ON(next == &rq->sl_header);
+	return skiplist_entry(next, struct task_struct, sl_node);
+}
+
+static inline unsigned long sched_queue_watermark(struct rq *rq)
+{
+	return task_sched_prio(sched_rq_first_task(rq), rq);
+}
+
+#define __SCHED_DEQUEUE_TASK(p, rq, flags, func)		\
+	psi_dequeue(p, flags & DEQUEUE_SLEEP);			\
+	sched_info_dequeued(rq, p);				\
+								\
+	if (skiplist_del_init(&rq->sl_header, &p->sl_node)) {	\
+		func;						\
+	}
+
+#define __SCHED_ENQUEUE_TASK(p, rq, flags)				\
+	sched_info_queued(rq, p);					\
+	psi_enqueue(p, flags);						\
+									\
+	p->sl_node.level = p->sl_level;					\
+	pds_skiplist_insert(&rq->sl_header, &p->sl_node)
+
+/*
+ * Requeue a task @p to @rq
+ */
+#define __SCHED_REQUEUE_TASK(p, rq, func)					\
+{\
+	bool b_first = skiplist_del_init(&rq->sl_header, &p->sl_node);		\
+\
+	p->sl_node.level = p->sl_level;						\
+	if (pds_skiplist_insert(&rq->sl_header, &p->sl_node) || b_first) {	\
+		func;								\
+	}									\
+}
+
+static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
+{
+	struct skiplist_node *node = p->sl_node.prev[0];
+
+	if (node != &rq->sl_header) {
+		struct task_struct *t = skiplist_entry(node, struct task_struct, sl_node);
+
+		if (t->priodl > p->priodl)
+			return true;
+	}
+
+	node = p->sl_node.next[0];
+	if (node != &rq->sl_header) {
+		struct task_struct *t = skiplist_entry(node, struct task_struct, sl_node);
+
+		if (t->priodl < p->priodl)
+			return true;
+	}
+
+	return false;
+}
+
+static void sched_task_fork(struct task_struct *p) {}
+
+/**
+ * task_prio - return the priority value of a given task.
+ * @p: the task in question.
+ *
+ * Return: The priority value as seen by users in /proc.
+ * RT tasks are offset by -100. Normal tasks are centered around 1, value goes
+ * from 0(SCHED_ISO) up to 82 (nice +19 SCHED_IDLE).
+ */
+int task_prio(const struct task_struct *p)
+{
+	int ret;
+
+	if (p->prio < MAX_RT_PRIO)
+		return (p->prio - MAX_RT_PRIO);
+
+	preempt_disable();
+	ret = task_sched_prio(p, this_rq()) - MAX_RT_PRIO;
+	preempt_enable();
+
+	return ret;
+}
+
+static void do_sched_yield_type_1(struct task_struct *p, struct rq *rq)
+{
+	time_slice_expired(p, rq);
+}
+
+static void sched_task_ttwu(struct task_struct *p) {}
+static void sched_task_deactivate(struct task_struct *p, struct rq *rq) {}
-- 
2.28.0.337.ge9b77c84a0


From 52ad8acf02c60d4ffc4d86e22ae08b64e36aa2f8 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Wed, 2 Sep 2020 08:24:30 +0800
Subject: [PATCH 2/6] sched/alt: Refill time_slice in time_slice_expired().

---
 kernel/sched/alt_core.c | 5 +----
 kernel/sched/bmq_imp.h  | 2 ++
 kernel/sched/pds_imp.h  | 1 +
 3 files changed, 4 insertions(+), 4 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 407bc46de451..5187d23f27df 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -2197,7 +2197,6 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 #endif
 
 	if (p->time_slice < RESCHED_NS) {
-		p->time_slice = sched_timeslice_ns;
 		time_slice_expired(p, rq);
 		resched_curr(rq);
 	}
@@ -3409,10 +3408,8 @@ static inline void check_curr(struct task_struct *p, struct rq *rq)
 
 	update_curr(rq, p);
 
-	if (p->time_slice < RESCHED_NS) {
-		p->time_slice = sched_timeslice_ns;
+	if (p->time_slice < RESCHED_NS)
 		time_slice_expired(p, rq);
-	}
 }
 
 static inline struct task_struct *
diff --git a/kernel/sched/bmq_imp.h b/kernel/sched/bmq_imp.h
index d7df1d3f9495..c9f0c708dd61 100644
--- a/kernel/sched/bmq_imp.h
+++ b/kernel/sched/bmq_imp.h
@@ -45,6 +45,8 @@ static inline void requeue_task(struct task_struct *p, struct rq *rq);
 
 static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 {
+	p->time_slice = sched_timeslice_ns;
+
 	if (SCHED_FIFO != p->policy && task_on_rq_queued(p)) {
 		if (SCHED_RR != p->policy)
 			deboost_task(p);
diff --git a/kernel/sched/pds_imp.h b/kernel/sched/pds_imp.h
index b970879f1d2e..aa7e933f08b8 100644
--- a/kernel/sched/pds_imp.h
+++ b/kernel/sched/pds_imp.h
@@ -38,6 +38,7 @@ static inline void requeue_task(struct task_struct *p, struct rq *rq);
 static inline void time_slice_expired(struct task_struct *p, struct rq *rq)
 {
 	/*printk(KERN_INFO "sched: time_slice_expired(%d) - %px\n", cpu_of(rq), p);*/
+	p->time_slice = sched_timeslice_ns;
 
 	if (p->prio >= MAX_RT_PRIO)
 		p->deadline = rq->clock + user_prio2deadline[TASK_USER_PRIO(p)];
-- 
2.28.0.337.ge9b77c84a0


From e3b4bdf0048d9a84a2fcecde8d2149751f2736a2 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sat, 5 Sep 2020 16:25:49 +0800
Subject: [PATCH 3/6] sched/alt: PDS rework.

alt:
Rework bmq&pds routine in sched_fork().
Sync-up mainline implement in sched_exec(), add task pi locking.
Add alt_sched_debug() and control by ALT_SCHED_DEBUG macro.

pds:
Update user_prio2deadline which now based on default 4ms time slice.
Update dl_level_map which provides 20 levels instead of the original 8
levels.
Fix issue that task_sched_prio() doesn't return corrent sched prio for
idle task.
Implement sched_task_for() routine.
---
 kernel/sched/alt_core.c  | 57 ++++++++++++++++++++++++++++------------
 kernel/sched/alt_debug.c |  2 +-
 kernel/sched/bmq_imp.h   |  2 +-
 kernel/sched/pds.h       |  2 +-
 kernel/sched/pds_imp.h   | 52 +++++++++++++++++++++++++-----------
 5 files changed, 79 insertions(+), 36 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 5187d23f27df..091f6919195c 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -2172,9 +2172,7 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 		 */
 		p->sched_reset_on_fork = 0;
 	}
-	update_task_priodl(p);
 
-	sched_task_fork(p);
 	/*
 	 * The child is not yet in the pid-hash so no cgroup attach races,
 	 * and the cgroup is pinned to this child due to cgroup_fork()
@@ -2190,6 +2188,7 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 	 */
 	rq = this_rq();
 	raw_spin_lock(&rq->lock);
+
 	rq->curr->time_slice /= 2;
 	p->time_slice = rq->curr->time_slice;
 #ifdef CONFIG_SCHED_HRTICK
@@ -2197,9 +2196,10 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 #endif
 
 	if (p->time_slice < RESCHED_NS) {
-		time_slice_expired(p, rq);
+		p->time_slice = sched_timeslice_ns;
 		resched_curr(rq);
 	}
+	sched_task_fork(p, rq);
 	raw_spin_unlock(&rq->lock);
 
 	rseq_migrate(p);
@@ -2795,25 +2795,29 @@ unsigned long nr_iowait(void)
 void sched_exec(void)
 {
 	struct task_struct *p = current;
+	unsigned long flags;
 	int dest_cpu;
+	struct rq *rq;
 
-	if (task_rq(p)->nr_running < 2)
-		return;
+	raw_spin_lock_irqsave(&p->pi_lock, flags);
+	rq = this_rq();
 
-	dest_cpu = cpumask_any_and(p->cpus_ptr, &sched_rq_watermark[IDLE_WM]);
-	if ( dest_cpu < nr_cpu_ids) {
-#ifdef CONFIG_SCHED_SMT
-		int smt = cpumask_any_and(p->cpus_ptr, &sched_sg_idle_mask);
-		if (smt < nr_cpu_ids)
-			dest_cpu = smt;
-#endif
-		if (likely(cpu_active(dest_cpu))) {
-			struct migration_arg arg = { p, dest_cpu };
+	if (rq != task_rq(p) || rq->nr_running < 2)
+		goto unlock;
 
-			stop_one_cpu(task_cpu(p), migration_cpu_stop, &arg);
-			return;
-		}
+	dest_cpu = select_task_rq(p, task_rq(p));
+	if (dest_cpu == smp_processor_id())
+		goto unlock;
+
+	if (likely(cpu_active(dest_cpu))) {
+		struct migration_arg arg = { p, dest_cpu };
+
+		raw_spin_unlock_irqrestore(&p->pi_lock, flags);
+		stop_one_cpu(task_cpu(p), migration_cpu_stop, &arg);
+		return;
 	}
+unlock:
+	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
 }
 
 #endif
@@ -3314,6 +3318,23 @@ static inline void schedule_debug(struct task_struct *prev, bool preempt)
 	schedstat_inc(this_rq()->sched_count);
 }
 
+/*
+ * Compile time debug macro
+ * #define ALT_SCHED_DEBUG
+ */
+
+#ifdef ALT_SCHED_DEBUG
+void alt_sched_debug(void)
+{
+	printk(KERN_INFO "sched: pending: 0x%04lx, idle: 0x%04lx, sg_idle: 0x%04lx\n",
+	       sched_rq_pending_mask.bits[0],
+	       sched_rq_watermark[IDLE_WM].bits[0],
+	       sched_sg_idle_mask.bits[0]);
+}
+#else
+inline void alt_sched_debug(void) {}
+#endif
+
 #ifdef	CONFIG_SMP
 
 #define SCHED_RQ_NR_MIGRATION (32UL)
@@ -5153,6 +5174,8 @@ static int sched_rr_get_interval(pid_t pid, struct timespec64 *t)
 	struct task_struct *p;
 	int retval;
 
+	alt_sched_debug();
+
 	if (pid < 0)
 		return -EINVAL;
 
diff --git a/kernel/sched/alt_debug.c b/kernel/sched/alt_debug.c
index 835e6bb98dda..1212a031700e 100644
--- a/kernel/sched/alt_debug.c
+++ b/kernel/sched/alt_debug.c
@@ -1,7 +1,7 @@
 /*
  * kernel/sched/alt_debug.c
  *
- * Print the BMQ debugging details
+ * Print the alt scheduler debugging details
  *
  * Author: Alfred Chen
  * Date  : 2020
diff --git a/kernel/sched/bmq_imp.h b/kernel/sched/bmq_imp.h
index c9f0c708dd61..0e67e00a6020 100644
--- a/kernel/sched/bmq_imp.h
+++ b/kernel/sched/bmq_imp.h
@@ -146,7 +146,7 @@ static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
 	return (task_sched_prio(p, rq) != p->bmq_idx);
 }
 
-static void sched_task_fork(struct task_struct *p)
+static void sched_task_fork(struct task_struct *p, struct rq *rq)
 {
 	p->boost_prio = (p->boost_prio < 0) ?
 		p->boost_prio + MAX_PRIORITY_ADJ : MAX_PRIORITY_ADJ;
diff --git a/kernel/sched/pds.h b/kernel/sched/pds.h
index 9b9addc205a9..7fdeace7e8a5 100644
--- a/kernel/sched/pds.h
+++ b/kernel/sched/pds.h
@@ -3,7 +3,7 @@
 
 /* bits:
  * RT(0-99), (Low prio adj range, nice width, high prio adj range) / 2, cpu idle task */
-#define SCHED_BITS	(MAX_RT_PRIO + MAX_PRIORITY_ADJ * 2 + 8 + 1)
+#define SCHED_BITS	(MAX_RT_PRIO + 20 + 1)
 #define IDLE_TASK_SCHED_PRIO	(SCHED_BITS - 1)
 
 static inline int task_running_nice(struct task_struct *p)
diff --git a/kernel/sched/pds_imp.h b/kernel/sched/pds_imp.h
index aa7e933f08b8..4a2fc8993229 100644
--- a/kernel/sched/pds_imp.h
+++ b/kernel/sched/pds_imp.h
@@ -1,31 +1,46 @@
 #define ALT_SCHED_VERSION_MSG "sched/bmq: PDS CPU Scheduler 5.8-r0 by Alfred Chen.\n"
 
 static const u64 user_prio2deadline[NICE_WIDTH] = {
-/* -20 */	  6291456,   6920601,   7612661,   8373927,   9211319,
-/* -15 */	 10132450,  11145695,  12260264,  13486290,  14834919,
-/* -10 */	 16318410,  17950251,  19745276,  21719803,  23891783,
-/*  -5 */	 26280961,  28909057,  31799962,  34979958,  38477953,
-/*   0 */	 42325748,  46558322,  51214154,  56335569,  61969125,
-/*   5 */	 68166037,  74982640,  82480904,  90728994,  99801893,
-/*  10 */	109782082, 120760290, 132836319, 146119950, 160731945,
-/*  15 */	176805139, 194485652, 213934217, 235327638, 258860401
+/* -20 */	  4194304,   4613734,   5075107,   5582617,   6140878,
+/* -15 */	  6754965,   7430461,   8173507,   8990857,   9889942,
+/* -10 */	 10878936,  11966829,  13163511,  14479862,  15927848,
+/*  -5 */	 17520632,  19272695,  21199964,  23319960,  25651956,
+/*   0 */	 28217151,  31038866,  34142752,  37557027,  41312729,
+/*   5 */	 45444001,  49988401,  54987241,  60485965,  66534561,
+/*  10 */	 73188017,  80506818,  88557499,  97413248, 107154572,
+/*  15 */	117870029, 129657031, 142622734, 156885007, 172573507
 };
 
-static const int dl_level_map[] = {
-/*      0           4           8           12           */
-	0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,
-/*      16          20          24          28           */
-	1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 4, 4, 5, 6, 7
+static const unsigned char dl_level_map[] = {
+/*       0               4               8              12           */
+	19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18,
+/*      16              20              24              28           */
+	18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17,
+/*      32              36              40              44           */
+	17, 17, 17, 17, 16, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15,
+/*      48              52              56              60           */
+	15, 15, 15, 14, 14, 14, 14, 14, 14, 13, 13, 13, 13, 12, 12, 12,
+/*      64              68              72              76           */
+	12, 11, 11, 11, 10, 10, 10,  9,  9,  8,  7,  6,  5,  4,  3,  2,
+/*      80              84              88              92           */
+	 1,  0
 };
 
 static inline int
 task_sched_prio(const struct task_struct *p, const struct rq *rq)
 {
-	u64 delta = (rq->clock + user_prio2deadline[39] - p->deadline) >> 23;
+	size_t delta;
 
+	if (p == rq->idle)
+		return IDLE_TASK_SCHED_PRIO;
+
+	if (p->prio < MAX_RT_PRIO)
+		return p->prio;
+
+	delta = (rq->clock + user_prio2deadline[39] - p->deadline) >> 21;
 	delta = min((size_t)delta, ARRAY_SIZE(dl_level_map) - 1);
 
-	return (p->prio < MAX_RT_PRIO)? p->prio : MAX_RT_PRIO + dl_level_map[delta];
+	return MAX_RT_PRIO + dl_level_map[delta];
 }
 
 static inline void update_task_priodl(struct task_struct *p)
@@ -173,7 +188,12 @@ static inline bool sched_task_need_requeue(struct task_struct *p, struct rq *rq)
 	return false;
 }
 
-static void sched_task_fork(struct task_struct *p) {}
+static void sched_task_fork(struct task_struct *p, struct rq *rq)
+{
+	if (p->prio >= MAX_RT_PRIO)
+		p->deadline = rq->clock + user_prio2deadline[TASK_USER_PRIO(p)];
+	update_task_priodl(p);
+}
 
 /**
  * task_prio - return the priority value of a given task.
-- 
2.28.0.337.ge9b77c84a0


From 09ca37aafa1fc42e06fba17ebad6d873eda8c5ee Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sun, 6 Sep 2020 10:26:05 +0800
Subject: [PATCH 4/6] sched/alt: Documentation and comments updates.

---
 Documentation/admin-guide/kernel-parameters.txt | 2 +-
 Documentation/admin-guide/sysctl/kernel.rst     | 4 ++--
 kernel/trace/trace_selftest.c                   | 2 +-
 3 files changed, 4 insertions(+), 4 deletions(-)

diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt
index 4f058fcd4887..6e3f8233600e 100644
--- a/Documentation/admin-guide/kernel-parameters.txt
+++ b/Documentation/admin-guide/kernel-parameters.txt
@@ -4526,7 +4526,7 @@
 	sbni=		[NET] Granch SBNI12 leased line adapter
 
 	sched_timeslice=
-			[KNL] Time slice in us for BMQ scheduler.
+			[KNL] Time slice in us for BMQ/PDS scheduler.
 			Format: <int> (must be >= 1000)
 			Default: 4000
 			See Documentation/scheduler/sched-BMQ.txt
diff --git a/Documentation/admin-guide/sysctl/kernel.rst b/Documentation/admin-guide/sysctl/kernel.rst
index f8d6535ab22e..313d2124e709 100644
--- a/Documentation/admin-guide/sysctl/kernel.rst
+++ b/Documentation/admin-guide/sysctl/kernel.rst
@@ -1432,8 +1432,8 @@ tunable to zero will disable lockup detection altogether.
 yield_type:
 ===========
 
-BMQ CPU scheduler only. This determines what type of yield calls to
-sched_yield will perform.
+BMQ/PDS CPU scheduler only. This determines what type of yield calls
+to sched_yield will perform.
 
   0 - No yield.
   1 - Deboost and requeue task. (default)
diff --git a/kernel/trace/trace_selftest.c b/kernel/trace/trace_selftest.c
index cfbae0a21cef..65f60c77bc50 100644
--- a/kernel/trace/trace_selftest.c
+++ b/kernel/trace/trace_selftest.c
@@ -1049,7 +1049,7 @@ static int trace_wakeup_test_thread(void *data)
 	/* Make this a -deadline thread */
 	static const struct sched_attr attr = {
 #ifdef CONFIG_SCHED_ALT
-		/* No deadline on BMQ, use RR */
+		/* No deadline on BMQ/PDS, use RR */
 		.sched_policy = SCHED_RR,
 #else
 		.sched_policy = SCHED_DEADLINE,
-- 
2.28.0.337.ge9b77c84a0


From 709b40c902276a9658a9030f0d4c3c4aec0fb9bb Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sun, 6 Sep 2020 10:28:26 +0800
Subject: [PATCH 5/6] sched/alt: Revert NORMAL_PRIO defination for powerpc
 cell.

---
 arch/powerpc/platforms/cell/spufs/sched.c | 5 +++++
 1 file changed, 5 insertions(+)

diff --git a/arch/powerpc/platforms/cell/spufs/sched.c b/arch/powerpc/platforms/cell/spufs/sched.c
index fe489fc01c73..f18d5067cd0f 100644
--- a/arch/powerpc/platforms/cell/spufs/sched.c
+++ b/arch/powerpc/platforms/cell/spufs/sched.c
@@ -51,6 +51,11 @@ static struct task_struct *spusched_task;
 static struct timer_list spusched_timer;
 static struct timer_list spuloadavg_timer;
 
+/*
+ * Priority of a normal, non-rt, non-niced'd process (aka nice level 0).
+ */
+#define NORMAL_PRIO		120
+
 /*
  * Frequency of the spu scheduler tick.  By default we do one SPU scheduler
  * tick for every 10 CPU scheduler ticks.
-- 
2.28.0.337.ge9b77c84a0


From 3141695f1e27565a6fa6fef5f6792e8f050fb746 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Mon, 31 Aug 2020 15:40:14 +0800
Subject: [PATCH 6/6] sched/alt: Add ALT_SCHED_VERSION micro.

---
 kernel/sched/alt_core.c | 2 ++
 kernel/sched/bmq_imp.h  | 2 +-
 kernel/sched/pds_imp.h  | 2 +-
 3 files changed, 4 insertions(+), 2 deletions(-)

diff --git a/kernel/sched/alt_core.c b/kernel/sched/alt_core.c
index 091f6919195c..76f72292e28a 100644
--- a/kernel/sched/alt_core.c
+++ b/kernel/sched/alt_core.c
@@ -45,6 +45,8 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/sched.h>
 
+#define ALT_SCHED_VERSION "v5.8-r2"
+
 /* rt_prio(prio) defined in include/linux/sched/rt.h */
 #define rt_task(p)		rt_prio((p)->prio)
 #define rt_policy(policy)	((policy) == SCHED_FIFO || (policy) == SCHED_RR)
diff --git a/kernel/sched/bmq_imp.h b/kernel/sched/bmq_imp.h
index 0e67e00a6020..ad9a7c448da7 100644
--- a/kernel/sched/bmq_imp.h
+++ b/kernel/sched/bmq_imp.h
@@ -1,4 +1,4 @@
-#define ALT_SCHED_VERSION_MSG "sched/bmq: BMQ CPU Scheduler 5.8-r1 by Alfred Chen.\n"
+#define ALT_SCHED_VERSION_MSG "sched/bmq: BMQ CPU Scheduler "ALT_SCHED_VERSION" by Alfred Chen.\n"
 
 /*
  * BMQ only routines
diff --git a/kernel/sched/pds_imp.h b/kernel/sched/pds_imp.h
index 4a2fc8993229..041827b92910 100644
--- a/kernel/sched/pds_imp.h
+++ b/kernel/sched/pds_imp.h
@@ -1,4 +1,4 @@
-#define ALT_SCHED_VERSION_MSG "sched/bmq: PDS CPU Scheduler 5.8-r0 by Alfred Chen.\n"
+#define ALT_SCHED_VERSION_MSG "sched/bmq: PDS CPU Scheduler "ALT_SCHED_VERSION" by Alfred Chen.\n"
 
 static const u64 user_prio2deadline[NICE_WIDTH] = {
 /* -20 */	  4194304,   4613734,   5075107,   5582617,   6140878,
-- 
2.28.0.337.ge9b77c84a0

