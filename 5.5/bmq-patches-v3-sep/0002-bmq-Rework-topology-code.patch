From 3f263393d078fc0e598d7398299d2ebe6bf6062c Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sun, 9 Feb 2020 20:24:40 +0800
Subject: [PATCH 2/6] bmq: Rework topology code.

---
 kernel/sched/Makefile    |   4 +-
 kernel/sched/bmq.c       | 112 +--------------------------------------
 kernel/sched/bmq_sched.h |  29 ++++++++++
 kernel/sched/topology.c  |  19 +++++++
 4 files changed, 51 insertions(+), 113 deletions(-)

diff --git a/kernel/sched/Makefile b/kernel/sched/Makefile
index 3ae94fd24fc2..ac31239aa51a 100644
--- a/kernel/sched/Makefile
+++ b/kernel/sched/Makefile
@@ -21,14 +21,14 @@ obj-y += bmq.o bmq_debug.o
 else
 obj-y += core.o
 obj-y += fair.o rt.o deadline.o
-obj-$(CONFIG_SMP) += cpudeadline.o topology.o stop_task.o
+obj-$(CONFIG_SMP) += cpudeadline.o stop_task.o
 obj-$(CONFIG_SCHED_AUTOGROUP) += autogroup.o
 obj-$(CONFIG_SCHED_DEBUG) += debug.o
 endif
 obj-y += loadavg.o clock.o cputime.o
 obj-y += idle.o
 obj-y += wait.o wait_bit.o swait.o completion.o
-obj-$(CONFIG_SMP) += cpupri.o pelt.o
+obj-$(CONFIG_SMP) += cpupri.o pelt.o topology.o
 obj-$(CONFIG_SCHEDSTATS) += stats.o
 obj-$(CONFIG_CGROUP_CPUACCT) += cpuacct.o
 obj-$(CONFIG_CPU_FREQ) += cpufreq.o
diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 2dde29ba6398..ac7bb9219bf7 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -114,17 +114,6 @@ static inline void deboost_task(struct task_struct *p)
 #ifdef CONFIG_SMP
 static cpumask_t sched_rq_pending_mask ____cacheline_aligned_in_smp;
 
-enum {
-	BASE_CPU_AFFINITY_CHK_LEVEL = 1,
-#ifdef CONFIG_SCHED_SMT
-	SMT_CPU_AFFINITY_CHK_LEVEL_SPACE_HOLDER,
-#endif
-#ifdef CONFIG_SCHED_MC
-	MC_CPU_AFFINITY_CHK_LEVEL_SPACE_HOLDER,
-#endif
-	NR_CPU_AFFINITY_CHK_LEVEL
-};
-
 DEFINE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_CHK_LEVEL], sched_cpu_affinity_masks);
 DEFINE_PER_CPU(cpumask_t *, sched_cpu_affinity_end_mask);
 
@@ -1328,19 +1317,6 @@ static int select_fallback_rq(int cpu, struct task_struct *p)
 	return dest_cpu;
 }
 
-static inline int __best_mask_cpu(int cpu, const cpumask_t *cpumask)
-{
-	cpumask_t *mask = &(per_cpu(sched_cpu_affinity_masks, cpu)[0]);
-	while ((cpu = cpumask_any_and(cpumask, mask)) >= nr_cpu_ids)
-		mask++;
-	return cpu;
-}
-
-static inline int best_mask_cpu(int cpu, const cpumask_t *cpumask)
-{
-	return cpumask_test_cpu(cpu, cpumask)? cpu:__best_mask_cpu(cpu, cpumask);
-}
-
 static inline int select_task_rq(struct task_struct *p)
 {
 	cpumask_t chk_mask, tmp;
@@ -5105,7 +5081,7 @@ int task_can_attach(struct task_struct *p,
 	return ret;
 }
 
-static bool sched_smp_initialized __read_mostly;
+bool sched_smp_initialized __read_mostly;
 
 #ifdef CONFIG_NO_HZ_COMMON
 void nohz_balance_enter_idle(int cpu)
@@ -5325,97 +5301,11 @@ bool cpus_share_cache(int this_cpu, int that_cpu)
 }
 #endif /* CONFIG_SMP */
 
-/*
- * Topology list, bottom-up.
- */
-static struct sched_domain_topology_level default_topology[] = {
-#ifdef CONFIG_SCHED_SMT
-	{ cpu_smt_mask, cpu_smt_flags, SD_INIT_NAME(SMT) },
-#endif
-#ifdef CONFIG_SCHED_MC
-	{ cpu_coregroup_mask, cpu_core_flags, SD_INIT_NAME(MC) },
-#endif
-	{ cpu_cpu_mask, SD_INIT_NAME(DIE) },
-	{ NULL, },
-};
-
-static struct sched_domain_topology_level *sched_domain_topology =
-	default_topology;
-
-#define for_each_sd_topology(tl)			\
-	for (tl = sched_domain_topology; tl->mask; tl++)
-
-void set_sched_topology(struct sched_domain_topology_level *tl)
-{
-	if (WARN_ON_ONCE(sched_smp_initialized))
-		return;
-
-	sched_domain_topology = tl;
-}
-
-/*
- * Initializers for schedule domains
- * Non-inlined to reduce accumulated stack pressure in build_sched_domains()
- */
-
-int sched_domain_level_max;
-
-/*
- * Partition sched domains as specified by the 'ndoms_new'
- * cpumasks in the array doms_new[] of cpumasks. This compares
- * doms_new[] to the current sched domain partitioning, doms_cur[].
- * It destroys each deleted domain and builds each new domain.
- *
- * 'doms_new' is an array of cpumask_var_t's of length 'ndoms_new'.
- * The masks don't intersect (don't overlap.) We should setup one
- * sched domain for each mask. CPUs not in any of the cpumasks will
- * not be load balanced. If the same cpumask appears both in the
- * current 'doms_cur' domains and in the new 'doms_new', we can leave
- * it as it is.
- *
- * The passed in 'doms_new' should be allocated using
- * alloc_sched_domains.  This routine takes ownership of it and will
- * free_sched_domains it when done with it. If the caller failed the
- * alloc call, then it can pass in doms_new == NULL && ndoms_new == 1,
- * and partition_sched_domains() will fallback to the single partition
- * 'fallback_doms', it also forces the domains to be rebuilt.
- *
- * If doms_new == NULL it will be replaced with cpu_online_mask.
- * ndoms_new == 0 is a special case for destroying existing domains,
- * and it will not create the default domain.
- *
- * Call with hotplug lock held
- */
-void partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],
-			     struct sched_domain_attr *dattr_new)
-{
-	/**
-	 * BMQ doesn't depend on sched domains, but just keep this api
-	 */
-}
-
 /*
  * used to mark begin/end of suspend/resume:
  */
 static int num_cpus_frozen;
 
-#ifdef CONFIG_NUMA
-int __read_mostly		node_reclaim_distance = RECLAIM_DISTANCE;
-
-/*
- * sched_numa_find_closest() - given the NUMA topology, find the cpu
- *                             closest to @cpu from @cpumask.
- * cpumask: cpumask to find a cpu from
- * cpu: cpu to be close to
- *
- * returns: cpu, or nr_cpu_ids when nothing found.
- */
-int sched_numa_find_closest(const struct cpumask *cpus, int cpu)
-{
-	return best_mask_cpu(cpu, cpus);
-}
-#endif /* CONFIG_NUMA */
-
 /*
  * Update cpusets according to cpu_active mask.  If cpusets are
  * disabled, cpuset_update_active_cpus() becomes a simple wrapper
diff --git a/kernel/sched/bmq_sched.h b/kernel/sched/bmq_sched.h
index c25bbce01716..f1c57e28cf60 100644
--- a/kernel/sched/bmq_sched.h
+++ b/kernel/sched/bmq_sched.h
@@ -192,6 +192,35 @@ static inline void unregister_sched_domain_sysctl(void)
 {
 }
 #endif
+
+extern bool sched_smp_initialized;
+
+enum {
+	BASE_CPU_AFFINITY_CHK_LEVEL = 1,
+#ifdef CONFIG_SCHED_SMT
+	SMT_CPU_AFFINITY_CHK_LEVEL_SPACE_HOLDER,
+#endif
+#ifdef CONFIG_SCHED_MC
+	MC_CPU_AFFINITY_CHK_LEVEL_SPACE_HOLDER,
+#endif
+	NR_CPU_AFFINITY_CHK_LEVEL
+};
+
+DECLARE_PER_CPU(cpumask_t [NR_CPU_AFFINITY_CHK_LEVEL], sched_cpu_affinity_masks);
+
+static inline int __best_mask_cpu(int cpu, const cpumask_t *cpumask)
+{
+	cpumask_t *mask = &(per_cpu(sched_cpu_affinity_masks, cpu)[0]);
+	while ((cpu = cpumask_any_and(cpumask, mask)) >= nr_cpu_ids)
+		mask++;
+	return cpu;
+}
+
+static inline int best_mask_cpu(int cpu, const cpumask_t *cpumask)
+{
+	return cpumask_test_cpu(cpu, cpumask)? cpu:__best_mask_cpu(cpu, cpumask);
+}
+
 #endif /* CONFIG_SMP */
 
 #ifndef arch_scale_freq_capacity
diff --git a/kernel/sched/topology.c b/kernel/sched/topology.c
index 6ec1e595b1d4..f02bbaf837b3 100644
--- a/kernel/sched/topology.c
+++ b/kernel/sched/topology.c
@@ -4,6 +4,7 @@
  */
 #include "sched.h"
 
+#ifndef CONFIG_SCHED_BMQ
 DEFINE_MUTEX(sched_domains_mutex);
 
 /* Protected by sched_domains_mutex: */
@@ -1182,8 +1183,10 @@ static void init_sched_groups_capacity(int cpu, struct sched_domain *sd)
  */
 
 static int default_relax_domain_level = -1;
+#endif /* CONFIG_SCHED_BMQ */
 int sched_domain_level_max;
 
+#ifndef CONFIG_SCHED_BMQ
 static int __init setup_relax_domain_level(char *str)
 {
 	if (kstrtoint(str, 0, &default_relax_domain_level))
@@ -1425,6 +1428,7 @@ sd_init(struct sched_domain_topology_level *tl,
 
 	return sd;
 }
+#endif /* CONFIG_SCHED_BMQ */
 
 /*
  * Topology list, bottom-up.
@@ -1454,6 +1458,7 @@ void set_sched_topology(struct sched_domain_topology_level *tl)
 	sched_domain_topology = tl;
 }
 
+#ifndef CONFIG_SCHED_BMQ
 #ifdef CONFIG_NUMA
 
 static const struct cpumask *sd_numa_mask(int cpu)
@@ -2289,3 +2294,17 @@ void partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],
 	partition_sched_domains_locked(ndoms_new, doms_new, dattr_new);
 	mutex_unlock(&sched_domains_mutex);
 }
+#else /* CONFIG_SCHED_BMQ */
+void partition_sched_domains(int ndoms_new, cpumask_var_t doms_new[],
+			     struct sched_domain_attr *dattr_new)
+{}
+
+#ifdef CONFIG_NUMA
+int __read_mostly		node_reclaim_distance = RECLAIM_DISTANCE;
+
+int sched_numa_find_closest(const struct cpumask *cpus, int cpu)
+{
+	return best_mask_cpu(cpu, cpus);
+}
+#endif /* CONFIG_NUMA */
+#endif
-- 
2.25.1.377.g2d2118b814.dirty

