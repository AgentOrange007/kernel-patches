From 791da295bc8059ed84f97d77aca92c0f761ec075 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Thu, 7 Mar 2019 10:47:33 +0800
Subject: [PATCH 07/64] bmq: Remove unnecessary rq dither.

---
 kernel/sched/bmq.c       | 31 +++----------------------------
 kernel/sched/bmq_sched.h |  1 -
 2 files changed, 3 insertions(+), 29 deletions(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 127a5c5be648..47d9490a3e42 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -53,7 +53,6 @@
  * Some helpers for converting to/from various scales. Use shifts to get
  * approximate multiples of ten for less overhead.
  */
-#define HALF_JIFFY_NS		(1000000000 / HZ / 2)
 #define MS_TO_NS(TIME)		((TIME) << 20)
 #define MS_TO_US(TIME)		((TIME) << 10)
 #define NS_TO_MS(TIME)		((TIME) >> 20)
@@ -871,15 +870,6 @@ static void hrtick_rq_init(struct rq *rq)
 	hrtimer_init(&rq->hrtick_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	rq->hrtick_timer.function = hrtick;
 }
-
-static inline u64 rq_dither(struct rq *rq)
-{
-	if ((rq->clock - rq->last_tick > HALF_JIFFY_NS) || hrtick_enabled(rq))
-		return 0;
-
-	return HALF_JIFFY_NS;
-}
-
 #else	/* CONFIG_SCHED_HRTICK */
 static inline int hrtick_enabled(struct rq *rq)
 {
@@ -893,11 +883,6 @@ static inline void hrtick_clear(struct rq *rq)
 static inline void hrtick_rq_init(struct rq *rq)
 {
 }
-
-static inline u64 rq_dither(struct rq *rq)
-{
-	return (rq->clock - rq->last_tick > HALF_JIFFY_NS)? 0:HALF_JIFFY_NS;
-}
 #endif	/* CONFIG_SCHED_HRTICK */
 
 static inline int normal_prio(struct task_struct *p)
@@ -2564,18 +2549,11 @@ static inline void scheduler_task_tick(struct rq *rq)
 	cpufreq_update_util(rq, 0);
 
 	/*
-	 * Tasks that were scheduled in the first half of a tick are not
-	 * allowed to run into the 2nd half of the next tick if they will
-	 * run out of time slice in the interim. Otherwise, if they have
-	 * less than RESCHED_NS of time slice left they will be rescheduled.
+	 * Tasks have less than RESCHED_NS of time slice left they will be
+	 * rescheduled.
 	 */
-	if (p->time_slice - rq->dither >= RESCHED_NS)
+	if (p->time_slice >= RESCHED_NS)
 		return;
-
-	/**
-	 * p->time_slice < RESCHED_NS. We will modify task_struct under
-	 * rq lock as p is rq->curr
-	 */
 	__set_tsk_resched(p);
 }
 
@@ -3065,8 +3043,6 @@ static inline void set_rq_task(struct rq *rq, struct task_struct *p)
 	if (p != rq->idle)
 		hrtick_start(rq, p->time_slice);
 #endif
-	/* update rq->dither */
-	rq->dither = rq_dither(rq);
 }
 
 /*
@@ -5773,7 +5749,6 @@ void __init sched_init(void)
 		rq->watermark = IDLE_WM;
 
 		raw_spin_lock_init(&rq->lock);
-		rq->dither = 0;
 		rq->nr_running = rq->nr_uninterruptible = 0;
 		rq->calc_load_active = 0;
 		rq->calc_load_update = jiffies + LOAD_FREQ;
diff --git a/kernel/sched/bmq_sched.h b/kernel/sched/bmq_sched.h
index cec8b7b1968d..c1f02ba41e88 100644
--- a/kernel/sched/bmq_sched.h
+++ b/kernel/sched/bmq_sched.h
@@ -121,7 +121,6 @@ struct rq {
 	u64 clock, last_tick;
 	u64 last_ts_switch;
 	u64 clock_task;
-	u64 dither;
 
 	unsigned long nr_running;
 	unsigned long nr_uninterruptible;
-- 
2.22.0.214.g8dca754b1e

