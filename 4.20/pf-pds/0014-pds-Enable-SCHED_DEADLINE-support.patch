From 12203d4cf761e86cabd3aaf789c72551545e7445 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Thu, 18 Oct 2018 15:42:49 +0000
Subject: [PATCH 14/42] pds: Enable SCHED_DEADLINE support.

Enable SCHED_DEADLINE support by squashing into priority 0 SCHED_FIFO
tasks.
---
 include/uapi/linux/sched.h       |  2 --
 kernel/sched/cpufreq_schedutil.c |  6 ------
 kernel/sched/pds.c               | 15 +++++++++++++++
 kernel/sched/pds_sched.h         | 14 ++++++++++++++
 4 files changed, 29 insertions(+), 8 deletions(-)

diff --git a/include/uapi/linux/sched.h b/include/uapi/linux/sched.h
index ebc69c660546..02161dc6a2fc 100644
--- a/include/uapi/linux/sched.h
+++ b/include/uapi/linux/sched.h
@@ -42,9 +42,7 @@
 #define SCHED_ISO		4
 #endif /* CONFIG_SCHED_PDS */
 #define SCHED_IDLE		5
-#ifndef CONFIG_SCHED_PDS
 #define SCHED_DEADLINE		6
-#endif /* !CONFIG_SCHED_PDS */
 
 /* Can be ORed in to make sure the process is reverted back to SCHED_NORMAL on fork */
 #define SCHED_RESET_ON_FORK     0x40000000
diff --git a/kernel/sched/cpufreq_schedutil.c b/kernel/sched/cpufreq_schedutil.c
index 369ecdca735b..5a521ee6596e 100644
--- a/kernel/sched/cpufreq_schedutil.c
+++ b/kernel/sched/cpufreq_schedutil.c
@@ -633,16 +633,10 @@ static int sugov_kthread_create(struct sugov_policy *sg_policy)
 	struct task_struct *thread;
 	struct sched_attr attr = {
 		.size		= sizeof(struct sched_attr),
-#ifdef CONFIG_SCHED_PDS
-		.sched_policy	= SCHED_FIFO,
-		.sched_nice	= 0,
-		.sched_priority	= 1,
-#else
 		.sched_policy	= SCHED_DEADLINE,
 		.sched_flags	= SCHED_FLAG_SUGOV,
 		.sched_nice	= 0,
 		.sched_priority	= 0,
-#endif
 		/*
 		 * Fake (unused) bandwidth; workaround to "fix"
 		 * priority inheritance.
diff --git a/kernel/sched/pds.c b/kernel/sched/pds.c
index 5ccdd8048646..98b52add292e 100644
--- a/kernel/sched/pds.c
+++ b/kernel/sched/pds.c
@@ -4402,6 +4402,12 @@ static int
 __sched_setscheduler(struct task_struct *p,
 		     const struct sched_attr *attr, bool user, bool pi)
 {
+	const struct sched_attr dl_squash_attr = {
+		.size		= sizeof(struct sched_attr),
+		.sched_policy	= SCHED_FIFO,
+		.sched_nice	= 0,
+		.sched_priority = 99,
+	};
 	int newprio = MAX_RT_PRIO - 1 - attr->sched_priority;
 	int retval, oldpolicy = -1;
 	int policy = attr->sched_policy;
@@ -4412,6 +4418,15 @@ __sched_setscheduler(struct task_struct *p,
 
 	/* The pi code expects interrupts enabled */
 	BUG_ON(pi && in_interrupt());
+
+	/*
+	 * PDS supports SCHED_DEADLINE by squash it as prio 0 SCHED_FIFO
+	 */
+	if (unlikely(SCHED_DEADLINE == policy)) {
+		attr = &dl_squash_attr;
+		policy = attr->sched_policy;
+		newprio = MAX_RT_PRIO - 1 - attr->sched_priority;
+	}
 recheck:
 	/* Double check policy once rq lock held */
 	if (policy < 0) {
diff --git a/kernel/sched/pds_sched.h b/kernel/sched/pds_sched.h
index b18fe343171a..96445b43f102 100644
--- a/kernel/sched/pds_sched.h
+++ b/kernel/sched/pds_sched.h
@@ -362,4 +362,18 @@ unsigned long arch_scale_cpu_capacity(struct sched_domain *sd, int cpu)
 
 extern void schedule_idle(void);
 
+/*
+ * !! For sched_setattr_nocheck() (kernel) only !!
+ *
+ * This is actually gross. :(
+ *
+ * It is used to make schedutil kworker(s) higher priority than SCHED_DEADLINE
+ * tasks, but still be able to sleep. We need this on platforms that cannot
+ * atomically change clock frequency. Remove once fast switching will be
+ * available on such platforms.
+ *
+ * SUGOV stands for SchedUtil GOVernor.
+ */
+#define SCHED_FLAG_SUGOV	0x10000000
+
 #endif /* PDS_SCHED_H */
-- 
2.20.1.2.gb21ebb671b

