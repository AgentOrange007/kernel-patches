From bc1e59cda61a7d1faaf26b8b74d43164acdbcfaa Mon Sep 17 00:00:00 2001
From: Oleksandr Natalenko <oleksandr@natalenko.name>
Date: Fri, 23 Nov 2018 18:34:34 +0100
Subject: [PATCH] pds-4.19: merge v0.99e release

Signed-off-by: Oleksandr Natalenko <oleksandr@natalenko.name>
---
 kernel/sched/pds.c | 44 +++++++++++++++++++++-----------------------
 1 file changed, 21 insertions(+), 23 deletions(-)

diff --git a/kernel/sched/pds.c b/kernel/sched/pds.c
index 12b15a0a2d55..35a88c457238 100644
--- a/kernel/sched/pds.c
+++ b/kernel/sched/pds.c
@@ -84,7 +84,7 @@ enum {
 
 static inline void print_scheduler_version(void)
 {
-	printk(KERN_INFO "pds: PDS-mq CPU Scheduler 0.99d by Alfred Chen.\n");
+	printk(KERN_INFO "pds: PDS-mq CPU Scheduler 0.99e by Alfred Chen.\n");
 }
 
 /* task_struct::on_rq states: */
@@ -1561,9 +1561,6 @@ static inline int best_mask_cpu(const int cpu, cpumask_t *cpumask)
 {
 	cpumask_t tmp, *mask;
 
-	if (cpumask_weight(cpumask) == 1)
-		return cpumask_first(cpumask);
-
 	if (cpumask_test_cpu(cpu, cpumask))
 		return cpu;
 
@@ -2065,6 +2062,7 @@ int sched_fork(unsigned long __maybe_unused clone_flags, struct task_struct *p)
 {
 	unsigned long flags;
 	int cpu = get_cpu();
+	struct rq *rq = this_rq();
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
 	INIT_HLIST_HEAD(&p->preempt_notifiers);
@@ -2116,25 +2114,20 @@ int sched_fork(unsigned long __maybe_unused clone_flags, struct task_struct *p)
 	 * total amount of pending timeslices in the system doesn't change,
 	 * resulting in more scheduling fairness.
 	 */
-	if (task_has_rt_policy(p)) {
-		struct rq *rq = this_rq();
-
-		raw_spin_lock_irqsave(&rq->lock, flags);
-		rq->curr->time_slice /= 2;
-		p->time_slice = rq->curr->time_slice;
+	raw_spin_lock_irqsave(&rq->lock, flags);
+	rq->curr->time_slice /= 2;
+	p->time_slice = rq->curr->time_slice;
 #ifdef CONFIG_SCHED_HRTICK
-		hrtick_start(rq, rq->curr->time_slice);
+	hrtick_start(rq, rq->curr->time_slice);
 #endif
 
-		if (p->time_slice < RESCHED_US) {
-			update_rq_clock(rq);
-			time_slice_expired(p, rq);
-			resched_curr(rq);
-		} else
-			update_task_priodl(p);
-		raw_spin_unlock_irqrestore(&rq->lock, flags);
+	if (p->time_slice < RESCHED_US) {
+		update_rq_clock(rq);
+		time_slice_expired(p, rq);
+		resched_curr(rq);
 	} else
 		update_task_priodl(p);
+	raw_spin_unlock_irqrestore(&rq->lock, flags);
 
 	/*
 	 * The child is not yet in the pid-hash so no cgroup attach races,
@@ -3293,8 +3286,12 @@ take_queued_task_cpumask(struct rq *rq, cpumask_t *chk_mask, int filter_prio)
 		int nr_migrated;
 		struct rq *src_rq = cpu_rq(src_cpu);
 
-		if (unlikely(!do_raw_spin_trylock(&src_rq->lock)))
-			continue;
+		if (unlikely(!do_raw_spin_trylock(&src_rq->lock))) {
+			if (PRIO_LIMIT == filter_prio)
+				continue;
+			else
+				return 0;
+		}
 		spin_acquire(&src_rq->lock.dep_map, SINGLE_DEPTH_NESTING, 1, _RET_IP_);
 
 		update_rq_clock(src_rq);
@@ -3334,6 +3331,9 @@ static inline int take_other_rq_task(struct rq *rq, int cpu, int filter_prio)
 	} else
 		cpumask_copy(&chk, &sched_rq_pending_masks[SCHED_RQ_RT]);
 
+	if (cpumask_empty(&chk))
+		return 0;
+
 	affinity_mask = per_cpu(sched_cpu_llc_start_mask, cpu);
 	end = per_cpu(sched_cpu_affinity_chk_end_masks, cpu);
 	do {
@@ -3361,14 +3361,12 @@ choose_next_task(struct rq *rq, int cpu, struct task_struct *prev)
 				return rq_first_queued_task(rq);
 			return rq->idle;
 		}
-		return next;
 	}
 #endif
 
 #ifdef	CONFIG_SMP
 	if (likely(rq->online))
-		if ((next->prio > prev->prio || PRIO_LIMIT == next->prio) &&
-		    take_other_rq_task(rq, cpu, next->prio)) {
+		if (take_other_rq_task(rq, cpu, next->prio)) {
 			resched_curr(rq);
 			return rq_first_queued_task(rq);
 		}
-- 
2.20.0.rc1

