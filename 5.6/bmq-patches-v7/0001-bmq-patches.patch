From a0f5234dafc66a99ca6ab81de417887fd06effb8 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sat, 4 Apr 2020 09:14:17 +0800
Subject: [PATCH 1/4] bmq: Rework take_other_rq_tasks().

---
 kernel/sched/bmq.c | 58 ++++++++++++++++++++++++++++++++++------------
 1 file changed, 43 insertions(+), 15 deletions(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index ad0d073666ae..331ab35962e3 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -480,6 +480,16 @@ static inline void sched_update_tick_dependency(struct rq *rq) { }
  * Add/Remove/Requeue task to/from the runqueue routines
  * Context: rq->lock
  */
+static inline void __dequeue_task(struct task_struct *p, struct rq *rq, int flags)
+{
+	psi_dequeue(p, flags & DEQUEUE_SLEEP);
+	sched_info_dequeued(rq, p);
+
+	list_del(&p->bmq_node);
+	if (list_empty(&rq->queue.heads[p->bmq_idx]))
+		clear_bit(p->bmq_idx, rq->queue.bitmap);
+}
+
 static inline void dequeue_task(struct task_struct *p, struct rq *rq, int flags)
 {
 	lockdep_assert_held(&rq->lock);
@@ -487,6 +497,9 @@ static inline void dequeue_task(struct task_struct *p, struct rq *rq, int flags)
 	WARN_ONCE(task_rq(p) != rq, "bmq: dequeue task reside on cpu%d from cpu%d\n",
 		  task_cpu(p), cpu_of(rq));
 
+	psi_dequeue(p, flags & DEQUEUE_SLEEP);
+	sched_info_dequeued(rq, p);
+
 	list_del(&p->bmq_node);
 	if (list_empty(&rq->queue.heads[p->bmq_idx])) {
 		clear_bit(p->bmq_idx, rq->queue.bitmap);
@@ -499,9 +512,16 @@ static inline void dequeue_task(struct task_struct *p, struct rq *rq, int flags)
 #endif
 
 	sched_update_tick_dependency(rq);
-	psi_dequeue(p, flags & DEQUEUE_SLEEP);
+}
 
-	sched_info_dequeued(rq, p);
+static inline void __enqueue_task(struct task_struct *p, struct rq *rq, int flags)
+{
+	sched_info_queued(rq, p);
+	psi_enqueue(p, flags);
+
+	p->bmq_idx = task_sched_prio(p);
+	list_add_tail(&p->bmq_node, &rq->queue.heads[p->bmq_idx]);
+	set_bit(p->bmq_idx, rq->queue.bitmap);
 }
 
 static inline void enqueue_task(struct task_struct *p, struct rq *rq, int flags)
@@ -511,9 +531,7 @@ static inline void enqueue_task(struct task_struct *p, struct rq *rq, int flags)
 	WARN_ONCE(task_rq(p) != rq, "bmq: enqueue task reside on cpu%d to cpu%d\n",
 		  task_cpu(p), cpu_of(rq));
 
-	p->bmq_idx = task_sched_prio(p);
-	list_add_tail(&p->bmq_node, &rq->queue.heads[p->bmq_idx]);
-	set_bit(p->bmq_idx, rq->queue.bitmap);
+	__enqueue_task(p, rq, flags);
 	update_sched_rq_watermark(rq);
 	++rq->nr_running;
 #ifdef CONFIG_SMP
@@ -523,9 +541,6 @@ static inline void enqueue_task(struct task_struct *p, struct rq *rq, int flags)
 
 	sched_update_tick_dependency(rq);
 
-	sched_info_queued(rq, p);
-	psi_enqueue(p, flags);
-
 	/*
 	 * If in_iowait is set, the code below may not trigger any cpufreq
 	 * utilization updates, so do it here explicitly with the IOWAIT flag
@@ -3215,9 +3230,9 @@ migrate_pending_tasks(struct rq *rq, struct rq *dest_rq, const int dest_cpu)
 	       (p = rq_next_bmq_task(skip, rq)) != rq->idle) {
 		skip = rq_next_bmq_task(p, rq);
 		if (cpumask_test_cpu(dest_cpu, p->cpus_ptr)) {
-			dequeue_task(p, rq, 0);
+			__dequeue_task(p, rq, 0);
 			set_task_cpu(p, dest_cpu);
-			enqueue_task(p, dest_rq, 0);
+			__enqueue_task(p, dest_rq, 0);
 			nr_migrated++;
 		}
 		nr_tries--;
@@ -3250,15 +3265,28 @@ static inline int take_other_rq_tasks(struct rq *rq, int cpu)
 			spin_acquire(&src_rq->lock.dep_map,
 				     SINGLE_DEPTH_NESTING, 1, _RET_IP_);
 
-			nr_migrated = migrate_pending_tasks(src_rq, rq, cpu);
+			if ((nr_migrated = migrate_pending_tasks(src_rq, rq, cpu))) {
+				src_rq->nr_running -= nr_migrated;
+#ifdef CONFIG_SMP
+				if (src_rq->nr_running < 2)
+					cpumask_clear_cpu(i, &sched_rq_pending_mask);
+#endif
+				rq->nr_running += nr_migrated;
+#ifdef CONFIG_SMP
+				if (rq->nr_running > 1)
+					cpumask_set_cpu(cpu, &sched_rq_pending_mask);
+#endif
+				update_sched_rq_watermark(rq);
+				cpufreq_update_util(rq, 0);
 
-			spin_release(&src_rq->lock.dep_map, _RET_IP_);
-			do_raw_spin_unlock(&src_rq->lock);
+				spin_release(&src_rq->lock.dep_map, _RET_IP_);
+				do_raw_spin_unlock(&src_rq->lock);
 
-			if (nr_migrated) {
-				cpufreq_update_util(rq, 0);
 				return 1;
 			}
+
+			spin_release(&src_rq->lock.dep_map, _RET_IP_);
+			do_raw_spin_unlock(&src_rq->lock);
 		}
 	} while (++affinity_mask < end_mask);
 
-- 
2.26.2.533.gb34789c0b0


From 3d94180407b70912bdb7a42b1d2abd0e00b692a1 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sun, 12 Apr 2020 10:36:34 +0800
Subject: [PATCH 2/4] bmq: Rework choose_next_task().

---
 kernel/sched/bmq.c | 34 ++++++++++++++++++++++++----------
 1 file changed, 24 insertions(+), 10 deletions(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 331ab35962e3..0db417604dda 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -3322,18 +3322,39 @@ choose_next_task(struct rq *rq, int cpu, struct task_struct *prev)
 
 	if (unlikely(rq->skip)) {
 		next = rq_runnable_task(rq);
+		if (next == rq->idle) {
 #ifdef	CONFIG_SMP
-		if (next == rq->idle && take_other_rq_tasks(rq, cpu))
+			if (!take_other_rq_tasks(rq, cpu)) {
+#endif
+				rq->skip = NULL;
+				schedstat_inc(rq->sched_goidle);
+				return next;
+#ifdef	CONFIG_SMP
+			}
 			next = rq_runnable_task(rq);
 #endif
+		}
 		rq->skip = NULL;
+#ifdef CONFIG_HIGH_RES_TIMERS
+		hrtick_start(rq, next->time_slice);
+#endif
 		return next;
 	}
 
 	next = rq_first_bmq_task(rq);
+	if (next == rq->idle) {
+#ifdef	CONFIG_SMP
+		if (!take_other_rq_tasks(rq, cpu)) {
+#endif
+			schedstat_inc(rq->sched_goidle);
+			return next;
 #ifdef	CONFIG_SMP
-	if (next == rq->idle && take_other_rq_tasks(rq, cpu))
-		return rq_first_bmq_task(rq);
+		}
+		next = rq_first_bmq_task(rq);
+#endif
+	}
+#ifdef CONFIG_HIGH_RES_TIMERS
+	hrtick_start(rq, next->time_slice);
 #endif
 	return next;
 }
@@ -3433,13 +3454,6 @@ static void __sched notrace __schedule(bool preempt)
 
 	next = choose_next_task(rq, cpu, prev);
 
-	if (next == rq->idle)
-		schedstat_inc(rq->sched_goidle);
-#ifdef CONFIG_HIGH_RES_TIMERS
-	else
-		hrtick_start(rq, next->time_slice);
-#endif
-
 	if (likely(prev != next)) {
 		next->last_ran = rq->clock_task;
 		rq->last_ts_switch = rq->clock;
-- 
2.26.2.533.gb34789c0b0


From 5f84083b0f4a41e45adb6b1b0deadb13d1422984 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 28 Apr 2020 14:41:33 +0800
Subject: [PATCH 3/4] bmq: Set time slice for idle tasks.

---
 kernel/sched/bmq.c | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 0db417604dda..0f449a8e6d69 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -5545,6 +5545,9 @@ static void sched_init_topology_cpumask(void)
 	cpumask_t *chk;
 
 	for_each_online_cpu(cpu) {
+		/* take chance to reset time slice for idle tasks */
+		cpu_rq(cpu)->idle->time_slice = sched_timeslice_ns;
+
 		chk = &(per_cpu(sched_cpu_affinity_masks, cpu)[0]);
 
 		cpumask_complement(chk, cpumask_of(cpu));
@@ -5581,6 +5584,7 @@ void __init sched_init_smp(void)
 #else
 void __init sched_init_smp(void)
 {
+	cpu_rq(0)->idle->time_slice = sched_timeslice_ns;
 }
 #endif /* CONFIG_SMP */
 
-- 
2.26.2.533.gb34789c0b0


From 2a3ec8da60f425a2aec4529ad678e8cfeda9284f Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Mon, 20 Apr 2020 18:06:30 +0800
Subject: [PATCH 4/4] BMQ v5.6-r4

---
 kernel/sched/bmq.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 0f449a8e6d69..10560f7720e2 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -70,7 +70,7 @@ early_param("bmq.timeslice", sched_timeslice);
 
 static inline void print_scheduler_version(void)
 {
-	printk(KERN_INFO "bmq: BMQ CPU Scheduler 5.6-r3 by Alfred Chen.\n");
+	printk(KERN_INFO "bmq: BMQ CPU Scheduler 5.6-r4 by Alfred Chen.\n");
 }
 
 /**
-- 
2.26.2.533.gb34789c0b0

