From 7ef62f5f5bc7fd9e197b64e3fd27f918812c7413 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 18 Feb 2020 22:56:43 +0800
Subject: [PATCH 73/81] bmq: Implement sched_exec().

---
 include/linux/sched/task.h |  2 +-
 kernel/sched/bmq.c         | 33 +++++++++++++++++++++++++++++++++
 2 files changed, 34 insertions(+), 1 deletion(-)

diff --git a/include/linux/sched/task.h b/include/linux/sched/task.h
index 0e961efabeb2..f1879884238e 100644
--- a/include/linux/sched/task.h
+++ b/include/linux/sched/task.h
@@ -102,7 +102,7 @@ extern long kernel_wait4(pid_t, int __user *, int, struct rusage *);
 extern void free_task(struct task_struct *tsk);
 
 /* sched_exec is called by processes performing an exec */
-#if defined(CONFIG_SMP) && !defined(CONFIG_SCHED_BMQ)
+#ifdef CONFIG_SMP
 extern void sched_exec(void);
 #else
 #define sched_exec()   {}
diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 0439a2b97ed1..33a7007e7a47 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -2699,6 +2699,39 @@ unsigned long nr_iowait(void)
 	return sum;
 }
 
+#ifdef CONFIG_SMP
+
+/*
+ * sched_exec - execve() is a valuable balancing opportunity, because at
+ * this point the task has the smallest effective memory and cache
+ * footprint.
+ */
+void sched_exec(void)
+{
+	struct task_struct *p = current;
+	int dest_cpu;
+
+	if (task_rq(p)->nr_running < 2)
+		return;
+
+	dest_cpu = cpumask_any_and(p->cpus_ptr, &sched_rq_watermark[IDLE_WM]);
+	if ( dest_cpu < nr_cpu_ids) {
+#ifdef CONFIG_SCHED_SMT
+		int smt = cpumask_any_and(p->cpus_ptr, &sched_sg_idle_mask);
+		if (smt < nr_cpu_ids)
+			dest_cpu = smt;
+#endif
+		if (likely(cpu_active(dest_cpu))) {
+			struct migration_arg arg = { p, dest_cpu };
+
+			stop_one_cpu(task_cpu(p), migration_cpu_stop, &arg);
+			return;
+		}
+	}
+}
+
+#endif
+
 DEFINE_PER_CPU(struct kernel_stat, kstat);
 DEFINE_PER_CPU(struct kernel_cpustat, kernel_cpustat);
 
-- 
2.26.0.51.ga7d14a4428

