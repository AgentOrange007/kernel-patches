From 1fbd65f2cd27a7f347097dec400a5c76b24423d1 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Sun, 22 Mar 2020 08:46:21 +0800
Subject: [PATCH 1/5] bmq: Remove set_rq_task().

---
 kernel/sched/bmq.c | 26 +++++++++-----------------
 1 file changed, 9 insertions(+), 17 deletions(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index b37608bbc23a..3b9f9b109c9d 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -3311,21 +3311,6 @@ choose_next_task(struct rq *rq, int cpu, struct task_struct *prev)
 	return next;
 }
 
-static inline void set_rq_task(struct rq *rq, struct task_struct *p)
-{
-	p->last_ran = rq->clock_task;
-
-	if (unlikely(sched_timeslice_ns == p->time_slice))
-		rq->last_ts_switch = rq->clock;
-
-	if (p == rq->idle)
-		schedstat_inc(rq->sched_goidle);
-#ifdef CONFIG_HIGH_RES_TIMERS
-	else
-		hrtick_start(rq, p->time_slice);
-#endif
-}
-
 /*
  * schedule() is the main scheduler function.
  *
@@ -3421,9 +3406,17 @@ static void __sched notrace __schedule(bool preempt)
 
 	next = choose_next_task(rq, cpu, prev);
 
-	set_rq_task(rq, next);
+	if (next == rq->idle)
+		schedstat_inc(rq->sched_goidle);
+#ifdef CONFIG_HIGH_RES_TIMERS
+	else
+		hrtick_start(rq, next->time_slice);
+#endif
 
 	if (prev != next) {
+		next->last_ran = rq->clock_task;
+		rq->last_ts_switch = rq->clock;
+
 		rq->nr_switches++;
 		/*
 		 * RCU users of rcu_dereference(rq->curr) may not see
@@ -3445,7 +3438,6 @@ static void __sched notrace __schedule(bool preempt)
 		 *   is a RELEASE barrier),
 		 */
 		++*switch_count;
-		rq->last_ts_switch = rq->clock;
 
 		trace_sched_switch(preempt, prev, next);
 
-- 
2.26.1.107.gefe3874640


From 6094e8070e2c7a64a329c0af1f8beb0d59d7416d Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Tue, 24 Mar 2020 14:50:43 +0800
Subject: [PATCH 2/5] bmq: Adjust boost_task/deboost_task chagnes.

---
 kernel/sched/bmq.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 3b9f9b109c9d..6efb8778ada7 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -1956,7 +1956,7 @@ static int try_to_wake_up(struct task_struct *p, unsigned int state,
 		atomic_dec(&task_rq(p)->nr_iowait);
 	}
 
-	if(cpu_rq(smp_processor_id())->clock - p->last_ran > sched_timeslice_ns)
+	if(this_rq()->clock_task - p->last_ran > sched_timeslice_ns)
 		boost_task(p);
 
 	cpu = select_task_rq(p);
-- 
2.26.1.107.gefe3874640


From 3c4af0f94c804a408bf2b0bc991e57a4d850c143 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Wed, 25 Mar 2020 11:35:33 +0800
Subject: [PATCH 3/5] bmq: Rework __schedule().

---
 kernel/sched/bmq.c | 11 ++++++-----
 1 file changed, 6 insertions(+), 5 deletions(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 6efb8778ada7..1dd61cf08fc5 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -3273,7 +3273,7 @@ static inline int take_other_rq_tasks(struct rq *rq, int cpu)
  */
 static inline void check_curr(struct task_struct *p, struct rq *rq)
 {
-	if (rq->idle == p)
+	if (unlikely(rq->idle == p))
 		return;
 
 	update_curr(rq, p);
@@ -3413,7 +3413,7 @@ static void __sched notrace __schedule(bool preempt)
 		hrtick_start(rq, next->time_slice);
 #endif
 
-	if (prev != next) {
+	if (likely(prev != next)) {
 		next->last_ran = rq->clock_task;
 		rq->last_ts_switch = rq->clock;
 
@@ -3443,11 +3443,12 @@ static void __sched notrace __schedule(bool preempt)
 
 		/* Also unlocks the rq: */
 		rq = context_switch(rq, prev, next);
-#ifdef CONFIG_SCHED_SMT
-		sg_balance_check(rq);
-#endif
 	} else
 		raw_spin_unlock_irq(&rq->lock);
+
+#ifdef CONFIG_SCHED_SMT
+	sg_balance_check(rq);
+#endif
 }
 
 void __noreturn do_task_dead(void)
-- 
2.26.1.107.gefe3874640


From 9e83e6fffe8a073620b215b116313937b247216f Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Wed, 25 Mar 2020 13:50:09 +0800
Subject: [PATCH 4/5] bmq: Rework sched_fork().

---
 kernel/sched/bmq.c | 27 +++++++++++++--------------
 1 file changed, 13 insertions(+), 14 deletions(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index 1dd61cf08fc5..e545cf6460e4 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -2035,8 +2035,7 @@ static inline void __sched_fork(unsigned long clone_flags, struct task_struct *p
 int sched_fork(unsigned long clone_flags, struct task_struct *p)
 {
 	unsigned long flags;
-	int cpu = get_cpu();
-	struct rq *rq = this_rq();
+	struct rq *rq;
 
 	__sched_fork(clone_flags, p);
 	/*
@@ -2073,12 +2072,21 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 
 	p->boost_prio = (p->boost_prio < 0) ?
 		p->boost_prio + MAX_PRIORITY_ADJ : MAX_PRIORITY_ADJ;
+	/*
+	 * The child is not yet in the pid-hash so no cgroup attach races,
+	 * and the cgroup is pinned to this child due to cgroup_fork()
+	 * is ran before sched_fork().
+	 *
+	 * Silence PROVE_RCU.
+	 */
+	raw_spin_lock_irqsave(&p->pi_lock, flags);
 	/*
 	 * Share the timeslice between parent and child, thus the
 	 * total amount of pending timeslices in the system doesn't change,
 	 * resulting in more scheduling fairness.
 	 */
-	raw_spin_lock_irqsave(&rq->lock, flags);
+	rq = this_rq();
+	raw_spin_lock(&rq->lock);
 	rq->curr->time_slice /= 2;
 	p->time_slice = rq->curr->time_slice;
 #ifdef CONFIG_SCHED_HRTICK
@@ -2089,21 +2097,13 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 		p->time_slice = sched_timeslice_ns;
 		resched_curr(rq);
 	}
-	raw_spin_unlock_irqrestore(&rq->lock, flags);
+	raw_spin_unlock(&rq->lock);
 
-	/*
-	 * The child is not yet in the pid-hash so no cgroup attach races,
-	 * and the cgroup is pinned to this child due to cgroup_fork()
-	 * is ran before sched_fork().
-	 *
-	 * Silence PROVE_RCU.
-	 */
-	raw_spin_lock_irqsave(&p->pi_lock, flags);
 	/*
 	 * We're setting the CPU for the first time, we don't migrate,
 	 * so use __set_task_cpu().
 	 */
-	__set_task_cpu(p, cpu);
+	__set_task_cpu(p, cpu_of(rq));
 	raw_spin_unlock_irqrestore(&p->pi_lock, flags);
 
 #ifdef CONFIG_SCHED_INFO
@@ -2112,7 +2112,6 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 #endif
 	init_task_preempt_count(p);
 
-	put_cpu();
 	return 0;
 }
 
-- 
2.26.1.107.gefe3874640


From abcd478eb4510ef79a610d05375547e29538ba93 Mon Sep 17 00:00:00 2001
From: Alfred Chen <cchalpha@gmail.com>
Date: Thu, 9 Apr 2020 15:21:44 +0800
Subject: [PATCH 5/5] BMQ v5.6-r3

---
 kernel/sched/bmq.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

diff --git a/kernel/sched/bmq.c b/kernel/sched/bmq.c
index e545cf6460e4..ad0d073666ae 100644
--- a/kernel/sched/bmq.c
+++ b/kernel/sched/bmq.c
@@ -70,7 +70,7 @@ early_param("bmq.timeslice", sched_timeslice);
 
 static inline void print_scheduler_version(void)
 {
-	printk(KERN_INFO "bmq: BMQ CPU Scheduler 5.6-r2 by Alfred Chen.\n");
+	printk(KERN_INFO "bmq: BMQ CPU Scheduler 5.6-r3 by Alfred Chen.\n");
 }
 
 /**
-- 
2.26.1.107.gefe3874640

